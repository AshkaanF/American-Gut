{
 "metadata": {
  "name": "module2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This IPython notebook assumes precomputed BIOM tables and metadata are readily available. The purpose of this notebook is to output a summary PDF, per sample, present in the American Gut dataset. This requires the full American Gut, the Human Microbiome Project, Global Gut and Personal Genome Project tables. Please see XXX for a discussion on how these tables were created. This notebook assumes there is a PBS/Torque-based compute cluster in which to submit jobs to, and that QIIME 1.7 is available in the path."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, lets get our environment up and running. The script utils.ipy contains some helper methods for submitting jobs to Torque-based clusters. In addition, lets create a new directory for us to work under."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from random import choice\n",
      "%run utils.ipy\n",
      "\n",
      "# get the current absolute path\n",
      "_basedir = os.path.abspath('.')\n",
      "if not _basedir.endswith('American-Gut/ipynb'):\n",
      "    raise ValueError, \"The notebook must be run from the American-Gut/ipynb directory!\"\n",
      "    \n",
      "# create a working directory\n",
      "alpha = 'abcdefghijklmnopqrstuvwxyz'; alpha += alpha.upper(); alpha += '0123456789'\n",
      "prj_name = 'module2_%s' % ''.join([choice(alpha) for i in range(4)])\n",
      "working_dir = os.path.join(_basedir, prj_name)\n",
      "try:\n",
      "    os.mkdir(working_dir)\n",
      "except OSError:\n",
      "    print \"WARNING: %s directory already exists!\" % prj_name\n",
      "    \n",
      "# submission wrapper\n",
      "submit = lambda cmd: submit_qsub(cmd, job_name=prj_name, queue='memroute', extra_args='-l pvmem=8gb')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### test job submission\n",
      "# ids = [submit('hostname; sleep %d' % (i + 10)) for i in range(10)]\n",
      "# wait_on(ids)\n",
      "# job_run_details(*ids[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, lets setup the paths and some helper methods. The first PCoA plot is a combination of the American Gut, Human Microbiome Project, Personal Genome Project and Global Gut datasets. These projects all used three different sequencing technologies however, and in order to combine them, we need to use the BIOM tables derived from sequence data all trimmed to the same length. See XXX for a more detailed discussion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file paths for tables and metadata. \n",
      "# ag -> American Gut\n",
      "# pgp -> Personal Genome Project\n",
      "# hmp -> Human Microbiome Project\n",
      "# gg -> Global Gut\n",
      "#\n",
      "# _t_ -> table\n",
      "# _m_ -> mapping file\n",
      "#\n",
      "# 100nt -> trimmed to the first 100 nucleotides \n",
      "ag_100nt_t_fp = os.path.join(_basedir, '../data/AG/AG_100nt.biom')\n",
      "ag_100nt_m_fp = os.path.join(_basedir, '../data/AG/AG_100nt.txt')\n",
      "\n",
      "pgp_100nt_t_fp = os.path.join(_basedir, '../data/PGP/PGP_100nt.biom')\n",
      "pgp_100nt_m_fp = os.path.join(_basedir, '../data/PGP/PGP_100nt.txt')\n",
      "\n",
      "hmp_100nt_t_fp = os.path.join(_basedir, '../data/HMP/HMPv35_100nt.biom')\n",
      "hmp_100nt_m_fp = os.path.join(_basedir, '../data/HMP/HMPv35_100nt.txt')\n",
      "\n",
      "gg_100nt_t_fp = os.path.join(_basedir, '../data/GG/GG_100nt.biom')\n",
      "gg_100nt_m_fp = os.path.join(_basedir, '../data/GG/GG_100nt.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not all QIIME scripts support gzip'd BIOM tables, so lets uncompress if necessary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for f in [ag_100nt_t_fp, pgp_100nt_t_fp, hmp_100nt_t_fp, gg_100nt_t_fp]:\n",
      "    if not os.path.exists(f) and os.path.exists(f + '.gz'):\n",
      "        decom = f + '.gz'\n",
      "        !gunzip $decom"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To improve the readability of the code here, let's also define what the QIIME commands we're going to use will look like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qiime_scripts = {\n",
      "    'Merge OTU Tables':'merge_otu_tables.py -i %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Single Rarifaction':'single_rarifaction.py -i %(input)s -o %(output)s -d %(depth)s',\n",
      "    'Parallel Beta Diversity':'parallel_beta_diversity.py -i %(input)s -o %(output)s -X %(job_prefix)s -O %(num_jobs)s -m unweighted_unifrac -t %(gg97_tree)s',\n",
      "    'Principle Coordinates':'principle_coordinates.py -i %(input)s -o %(output)s',\n",
      "    'Make Emperor':'make_emperor.py -i %(input)s -o %(output)s -m %(mapping)s',\n",
      "    'Merge Mapping Files':'merge_mapping_files.py -m %(input)s -o %(output)s',\n",
      "    'Filter Samples':'filter_samples_from_otu_table.py -i %(input)s -o %(output)s -m %(mapping)s -s %(valid_states)s'\n",
      "    'Summarize Taxonomy':'summarize_taxa.py \n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the first figure, we want to place the American Gut population in the context of other large studies. To do so, we need to first merge the tables together. Since there are 4 tables to merge, we need to use two merge calls. (It is also feasible to use QIIME's parallel_merge_otu_tables.py here as well)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting paths\n",
      "hmp_gg_t_fp = os.path.join(working_dir, \"HMP_GG_100nt.biom\")\n",
      "ag_pgp_t_fp = os.path.join(working_dir, \"AG_PGP_100nt.biom\")\n",
      "hmp_gg_ag_pgp_t_fp = os.path.join(working_dir, \"HMP_GG_AG_PGP_100nt.biom\")\n",
      "\n",
      "# setup the command arguments for each call\n",
      "hmp_gg_cmd_args = {'input_a':hmp_100nt_t_fp, \n",
      "                   'input_b':gg_100nt_t_fp,\n",
      "                   'output':hmp_gg_t_fp}\n",
      "ag_pgp_cmd_args = {'input_a':ag_100nt_t_fp,\n",
      "                   'input_b':pgp_100nt_t_fp,\n",
      "                   'output':ag_pgp_t_fp}\n",
      "hmp_gg_ag_pgp_cmd_args = {'input_a':ag_pgp_t_fp,\n",
      "                          'input_b':hmp_gg_t_fp,\n",
      "                          'output':hmp_gg_ag_pgp_t_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_gg_cmd_args)\n",
      "ag_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % ag_pgp_cmd_args)\n",
      "jobs = wait_on([hmp_gg_job, ag_pgp_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_ag_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_gg_ag_pgp_cmd_args)\n",
      "jobs = wait_on(hmp_gg_ag_pgp_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 / 2 jobs still running, approximately  260 seconds elapsed\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-27-d724aad846be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mhmp_gg_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqiime_scripts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Merge OTU Tables'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mhmp_gg_cmd_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mag_pgp_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqiime_scripts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Merge OTU Tables'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mag_pgp_cmd_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mwait_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhmp_gg_job\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_pgp_job\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# merge and block until completion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-18-ccc6f0cc57e7>\u001b[0m in \u001b[0;36mwait_on\u001b[1;34m(jobs_to_monitor, additional_prefix)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mrunning_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_qstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mjobs_to_monitor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPOLL_INTERVAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0melapsed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mPOLL_INTERVAL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the data are combined, we need to rarify them in order to normalize for sequencing effort. The Human Microbiome Project has the shallowest coverage per sample, and when operating with the HMP dataset, a rarifaction depth of 1000 is a reasonable balance between effort and retaining sufficient numbers of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting path\n",
      "hmp_gg_ag_pgp_t_1k_fp = os.path.join(working_dir, \"HMP_GG_AG_PGP_100nt_even1k.biom\")\n",
      "\n",
      "# setup the command arguments\n",
      "hmp_gg_ag_pgp_t_1k_cmd_args = {'input':hmp_gg_ag_pgp_t_fp,\n",
      "                               'output':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                               'depth':'1000'}\n",
      "\n",
      "# rarifiy and block until completion\n",
      "hmp_gg_ag_pgp_t_1k_job = submit(qiime_scripts['Single Rarifaction'] % hmp_gg_ag_pgp_t_1k_cmd_args)\n",
      "wait_on(hmp_gg_ag_pgp_t_1k_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unhashable type: 'dict'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-be3b878152aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[('module2', '491121'), ('module2', '491122'), ('module2', '491123'), ('module2', '491124'), ('module2', '491125'), ('module2', '491126'), ('module2', '491127'), ('module2', '491128'), ('module2', '491129'), ('module2', '491130'), ('module2', '491131'), ('module2', '491132'), ('module2', '491133'), ('module2', '491134'), ('module2', '491135'), ('module2', '491136'), ('module2', '491137'), ('module2', '491138'), ('module2', '491139'), ('module2', '491140'), ('module2', '491141'), ('module2', '491142'), ('module2', '491143'), ('module2', '491144'), ('module2', '491145'), ('module2', '491146'), ('module2', '491147'), ('module2', '491148'), ('module2', '491149'), ('module2', '491150')]]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to massage the metadata to improve our ability to compare samples. Specifically, were going to: \n",
      "\n",
      "* simplify body sites into their major categories (e.g., transform \"forehead\" and \"skin of hand\" to just \"skin\")\n",
      "* simplify country codes (e.g., GAZ:Venezuela to Venezuela)\n",
      "* simplify the experiment title (e.g., American Gut Project to AGP)\n",
      "* create a hybrid field combining the experiment title with the body site"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a body site mapping:\n",
      "simple_matter_map = {\n",
      "        'feces':'FECAL',\n",
      "        'sebum':'SKIN',\n",
      "        'tongue':'ORAL',\n",
      "        'skin':'SKIN',\n",
      "        'mouth':'ORAL',\n",
      "        'gingiva':'ORAL',\n",
      "        'gingival epithelium':'ORAL',\n",
      "        'nares':'SKIN',\n",
      "        'skin of hand':'SKIN',\n",
      "        'hand':'SKIN',\n",
      "        'skin of head':'SKIN',\n",
      "        'hand skin':'SKIN',\n",
      "        'throat':'ORAL',\n",
      "        'auricular region zone of skin':'SKIN',\n",
      "        'mucosa of tongue':'ORAL',\n",
      "        'palatine tonsil':'ORAL',\n",
      "        'hard palate':'ORAL',\n",
      "        'saliva':'ORAL',\n",
      "        'stool':'FECAL',\n",
      "        'vagina':'SKIN',\n",
      "        'fossa':'SKIN',\n",
      "        'buccal mucosa':'ORAL',\n",
      "        'vaginal fornix':'SKIN',\n",
      "        }\n",
      "\n",
      "def massage_mapping(in_fp, out_fp, body_site_column_name, exp_acronym):\n",
      "    \"\"\"Simplify the mapping file for use in figures\n",
      "\n",
      "    body_site_column_name : specify the column name for body\n",
      "    exp_acronym : short name for the study\n",
      "\n",
      "    Returns False on failure, True on success\n",
      "    \"\"\"\n",
      "    mapping_lines = [l.strip().split('\\t') for l in open(in_fp)]\n",
      "    \n",
      "    header = mapping_lines[0]\n",
      "    header_low = map(lambda x: x.lower(), header)\n",
      "        \n",
      "    try:\n",
      "        bodysite_idx = header_low.index(body_site_column_name.lower())\n",
      "    except IndexError:\n",
      "        print \"Could not find %s in the mapping file header!\" % body_site_column_name\n",
      "        return False\n",
      "    \n",
      "    try:\n",
      "        country_idx = header_low.index('country')\n",
      "    except IndexError:\n",
      "        print \"Could not find the country in the mapping file header!\"\n",
      "        return False\n",
      "\n",
      "    new_mapping_lines = [header[:]]\n",
      "    new_mapping_lines[0].append('SIMPLE_BODY_SITE')\n",
      "    new_mapping_lines[0].append('TITLE_ACRONYM')\n",
      "    new_mapping_lines[0].append('TITLE_BODY_SITE')\n",
      "\n",
      "    for l in mapping_lines[1:]:\n",
      "        new_line = l[:]\n",
      "        body_site = new_line[bodysite_idx]\n",
      "        country = new_line[country_idx]\n",
      "        \n",
      "        # grab the body site\n",
      "        if body_site.startswith('UBERON_'):\n",
      "            body_site = body_site.split('_',1)[-1].replace(\"_\",\" \")\n",
      "        elif body_site.startswith('UBERON:'):\n",
      "            body_site = body_site.split(':',1)[-1]\n",
      "        elif body_site in ['NA', 'unknown']:\n",
      "            # controls, environmental, etc\n",
      "            continue\n",
      "        else:\n",
      "            print \"Unknown body site value: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            return False\n",
      "        \n",
      "        # remap the body site\n",
      "        if body_site.lower() not in simple_matter_map:\n",
      "            print \"Could not remap body site: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            return False\n",
      "        else:\n",
      "            body_site = simple_matter_map[body_site.lower()]\n",
      "         \n",
      "        # simplify the country    \n",
      "        if country.startswith('GAZ:'):\n",
      "            country = country.split(':',1)[-1]\n",
      "        else:\n",
      "            print \"Could not parse country value: %s\" % country\n",
      "            return False\n",
      "        \n",
      "        new_line.append(body_site)\n",
      "        new_line.append(exp_acronym)\n",
      "        new_line.append(\"%s-%s\" % (exp_acronym, body_site))\n",
      "        new_line[country_idx] = country\n",
      "        new_mapping_lines.append(new_line)\n",
      "    \n",
      "    output = open(out_fp,'w')\n",
      "    output.write('\\n'.join(['\\t'.join(l) for l in new_mapping_lines]))\n",
      "    output.write('\\n')\n",
      "    output.close()\n",
      "    \n",
      "    return True\n",
      "        \n",
      "# new file paths\n",
      "ag_100nt_m_massaged_fp = os.path.join(working_dir, 'AG_100nt_massaged.txt')\n",
      "gg_100nt_m_massaged_fp = os.path.join(working_dir, 'GG_100nt_massaged.txt')\n",
      "pgp_100nt_m_massaged_fp = os.path.join(working_dir, 'PGP_100nt_massaged.txt')\n",
      "hmp_100nt_m_massaged_fp = os.path.join(working_dir, 'HMP_100nt_massaged.txt')\n",
      "\n",
      "# massage\n",
      "assert massage_mapping(ag_100nt_m_fp, ag_100nt_m_massaged_fp, 'body_site', 'AGP')\n",
      "assert massage_mapping(gg_100nt_m_fp, gg_100nt_m_massaged_fp, 'body_site', 'GG')\n",
      "assert massage_mapping(pgp_100nt_m_fp, pgp_100nt_m_massaged_fp, 'body_site', 'PGP')\n",
      "assert massage_mapping(hmp_100nt_m_fp, hmp_100nt_m_massaged_fp, 'bodysite', 'HMP')\n",
      "\n",
      "# merge the mapping files\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
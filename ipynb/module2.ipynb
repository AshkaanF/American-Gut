{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The purpose of this notebook is to output a summary PDF, per fecal sample, present in the American Gut dataset. This requires the American Gut, the [Human Microbiome Project](http://www.ncbi.nlm.nih.gov/pubmed/22699609), [Global Gut](http://www.ncbi.nlm.nih.gov/pubmed/22699611) and the unpublished [Personal Genome Project](http://personalgenomes.org/) microbiome data. More information about the American Gut Project can be found [here](http://www.americangut.org). \n",
      "\n",
      "The American Gut Project is part of the [Earth Microbiome Project](http://www.earthmicrobiome.org) and follows the same protocols and data release standards put forth by the EMP. The project is run primarily through the [Knight Lab](http://knightlab.colorado.edu) in the [BioFrontiers Institute](http://biofrontiers.colorado.edu) at the [University of Colorado in Boulder](http://colorado.edu). \n",
      "\n",
      "The effort that is summarized in this Notebook is thanks to the over [50 people](https://github.com/biocore/American-Gut/blob/master/CREDITS.md) who have volunteered their time to this project. From sending thousands of samples to thousands of participants all over the world, to developing complex web and database infrastructure, to developing new analysis techniques and procedures, to responding to the questions from our participants, this project could not have made it anywhere without the immense support from so many dedicated people.\n",
      "\n",
      "This Notebook assumes the following packages are installed:\n",
      "\n",
      "* a PBS/Torque-based compute cluster in which to submit jobs to\n",
      "* [QIIME](http://www.qiime.org)\n",
      "* A custom Emperor [branch](https://github.com/eldeveloper/emperor/tree/faces_plus_lines) is available in the path\n",
      "* TexLive 2013\n",
      "* [scikit-bio](https://github.com/biocore/scikit-bio)\n",
      "* [The American Gut repository](https://github.com/biocore/American-Gut)\n",
      "* [IPython](http://ipython.org)\n",
      "\n",
      "The goal of this notebook is to, from publically accessible data, produce framable results for every American Gut fecal sample. Specifically, for each sample, we will be producing the following figures:\n",
      "\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, the [Human Microbiome Project](http://www.ncbi.nlm.nih.gov/pubmed/22699609), the [Global Gut](http://www.ncbi.nlm.nih.gov/pubmed/22699611) and the [Personal Genome Project](http://personalgenomes.org/) microbiome datasets.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, Global Gut project, highlighting the variation in host age has on the sample.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project.\n",
      "* Taxonomy summary of the sample, and for comparison, a few collapsed American Gut groups and Michael Pollan's pre and post antibiotics sample.\n",
      "* Significantly differentiated operational taxonomic units in the sample compared to other American Gut samples.\n",
      "\n",
      "As an outline, the major steps this notebook will take are:\n",
      "\n",
      "1. Download the American Gut data and metadata from the European Bioinformatics Institute ([EBI](http://www.ebi.ac.uk/)), which is part of International Nucleotide Sequence Database Collaboration ([INSDC](http://www.insdc.org/))\n",
      "2. Trim the sequences to 100 nucleotides as to avoid a sequence length bias with the studies we are combining with (note, other studies are provided pretrimmed)\n",
      "3. Resolve and filter the gammaproteobacteria that appear to have artificially bloomed\n",
      "4. Determine observational taxonomic units (OTUs) at 97% sequence identity (approximately species) against [Greengenes](http://greengenes.secondgenome.com/) version 13_5\n",
      "5. Merge the American Gut OTUs with the OTUs from the [Human Microbiome Project](http://www.nature.com/nature/journal/v486/n7402/full/nature11234.html), the [Global Gut](http://www.nature.com/nature/journal/v486/n7402/abs/nature11053.html) and the [Personal Genome Project](http://www.personalgenomes.org/) microbiome data\n",
      "6. Merge the mapping files, or the study variables that describe each sample\n",
      "7. Rarify the samples to 1000 sequences per sample (due to the relative low coverage in the Human Microbiome Project) to normalize for differential sequencing effort\n",
      "8. Compute beta diversity using unweighted [UniFrac](http://www.ncbi.nlm.nih.gov/pubmed/16332807)\n",
      "9. Compute principal coordinates, and visualize using [Emperor](http://www.gigasciencejournal.com/content/2/1/16/)\n",
      "10. Build phylum level taxonomy summaries comparing each sample to similar groups (e.g., age, BMI, etc), and to [Michael Pollan](http://michaelpollan.com/)\n",
      "11. Determine abundant and enriched microbes within each sample relative to the American Gut population\n",
      "12. Produce the full taxonomy summary for each sample (unrarified)\n",
      "13. Populate the results template (additional information about the results can be found here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's first setup a few settings for the data in the Notebook and a few parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# process a small subset of the data if True\n",
      "debug = False\n",
      "\n",
      "# EBI accesions to process\n",
      "accessions = ['ERP003819', 'ERP003822', 'ERP003820', 'ERP003821',\n",
      "              'ERP005367', 'ERP005366', 'ERP005361', 'ERP005362']\n",
      "\n",
      "# If a single sequence composes at least this fraction of a gammaproteobacterial\n",
      "# OTU that is identified to be overrepresentend, consider it to be the result of\n",
      "# a bloom.\n",
      "bloom_abundance_threshold = 0.85\n",
      "# Set to False to leave sequences for full length\n",
      "trim_seqs = False\n",
      "\n",
      "# The value to pass for the -l option of select_gamma.py;\n",
      "# determines what threshold to use when determining what gamma OTUs are over-represented\n",
      "select_gamma_threshold = 0.03\n",
      "\n",
      "# set the path to the Greengenes 13_5 97% OTU tree\n",
      "reference_rep_set = '/shared/gg_13_5/gg_13_5/rep_set/97_otus.fasta' # e.g., path to 97_otus.fasta from Greengenes\n",
      "reference_taxonomy = '/shared/gg_13_5/gg_13_5/taxonomy/97_otu_taxonomy.txt' # e.g., path to 97_otu_taxonomy.txt from Grengenes\n",
      "reference_tree = '/shared/gg_13_5/gg_13_5/trees/97_otus.tree' # e.g., path to 97_otus.tree from Grengenes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, lets get our environment up and running. The script ``cluster_utils.ipy`` contains some helper methods for submitting jobs to Torque-based clusters, and we also have quite a few methods from Python and from the American Gut repository to import. In addition, lets create a new directory for us to work under, setup some helper functions and some paths."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run cluster_utils.ipy\n",
      "import os\n",
      "from functools import partial\n",
      "from tempfile import mktemp\n",
      "from collections import defaultdict\n",
      "from cPickle import loads\n",
      "from itertools import izip\n",
      "from ftplib import FTP\n",
      "from tarfile import open as tar_open\n",
      "from gzip import open as gz_open\n",
      "from glob import glob\n",
      "\n",
      "from IPython.lib.display import FileLink\n",
      "\n",
      "from americangut.util import fetch_study, trim_fasta, concatenate_files\n",
      "from americangut.results_utils import (check_file, get_path, get_repository_dir, stage_static_files, \n",
      "                                       parse_identifying_data, filter_mapping_file, massage_mapping,\n",
      "                                       parse_previously_printed, bootstrap_result, MissingFigure,\n",
      "                                       construct_svg_smash_commands, construct_phyla_plots_cmds,\n",
      "                                       per_sample_taxa_summaries, construct_bootstrap_and_latex_commands,\n",
      "                                       count_unique_sequences_per_otu, write_bloom_fasta,\n",
      "                                       harvest)\n",
      "\n",
      "# get the current absolute path\n",
      "current_dir = os.path.abspath('.')\n",
      "\n",
      "# get the path to the American Gut repository \n",
      "repo_dir = get_repository_dir()\n",
      "\n",
      "# create a place to do work\n",
      "prj_name = \"americangut_results\"\n",
      "working_dir = os.path.join(current_dir, prj_name)\n",
      "os.makedirs(prj_name)\n",
      "\n",
      "# path wrappers\n",
      "get_relative_new_path = lambda x: os.path.join(working_dir, x)\n",
      "get_relative_existing_path = partial(get_path, working_dir)\n",
      "\n",
      "# set the number of processors parallel tasks will use\n",
      "NUM_PROCS = 100\n",
      "\n",
      "# bootstrap the submit method\n",
      "submit = partial(submit, prj_name)\n",
      "\n",
      "# make sure our reference files exist\n",
      "check_file(reference_rep_set)\n",
      "check_file(reference_taxonomy)\n",
      "check_file(reference_tree)\n",
      "\n",
      "if debug:\n",
      "    sequence_files = [os.path.join(repo_dir, 'data', 'AG_debug', 'test_seqs.fna'),\n",
      "                      os.path.join(repo_dir, 'data', 'AG_debug', 'test_seqs_2.fna')]\n",
      "    mapping_files = [os.path.join(repo_dir, 'data', 'AG_debug', 'test_mapping.txt')]\n",
      "else:\n",
      "    sequence_files = [get_relative_new_path(acc + '.fna') for acc in accessions]\n",
      "    mapping_files = [get_relative_new_path(acc + '.txt') for acc in accessions]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The various computational steps in this workflow require using quite a few external scripts. What we're doing here is constructing a Python ``dict``, or a lookup structure, that lets us easily refer to commands that we want to run."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scripts = {\n",
      "    'Merge OTU Tables':'merge_otu_tables.py -i %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Single Rarifaction':'single_rarefaction.py -i %(input)s -o %(output)s -d %(depth)s',\n",
      "    'Parallel Beta Diversity':'parallel_beta_diversity.py -i %(input)s -o %(output)s -X %(job_prefix)s -O %(num_jobs)s -m unweighted_unifrac -t %(gg97_tree)s',\n",
      "    'Principal Coordinates':'principal_coordinates.py -i %(input)s -o %(output)s',\n",
      "    'Merge Mapping Files':'merge_mapping_files.py -m %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Filter Samples':'filter_samples_from_otu_table.py -i %(input)s -o %(output)s --sample_id_fp=%(sample_id_fp)s',\n",
      "    'Summarize OTU by Category':'summarize_otu_by_cat.py -m %(mapping)s -o %(output)s -n -i %(otu_table)s -c %(category)s',\n",
      "    'Filter Distance Matrix':'filter_distance_matrix.py -i %(input)s -o %(output)s --sample_id_fp=%(sample_ids)s',\n",
      "    'Summarize Taxa':'summarize_taxa.py -i %(input)s -o %(output)s -L %(level)s',\n",
      "    'Summarize Taxa Mapping':'summarize_taxa.py -i %(input)s -o %(output)s -L %(level)s -m %(mapping)s',\n",
      "    'Taxonomy Comparison':'taxonomy_comparison.py -i %(input)s -m %(mapping)s -l %(level)s -o %(output)s -c %(list_of_categories)s',\n",
      "    'Make Emperor':'make_emperor.py -i %(input)s -o %(output)s -m %(mapping)s',\n",
      "    'SVG Smash':'replace_svg_object.py -i %(input)s -o %(output)s --prefix %(prefix)s --sample_id=%(sample_id)s',\n",
      "    'Make Phyla Plots':\"make_phyla_plots_AGP.py -i %(input)s -m %(mapping)s -o %(output)s -c '%(categories)s' -s %(samples)s %(debug)s\",\n",
      "    'OTU Significance':\"generate_otu_signifigance_tables_AGP.py -i %(input)s -o %(output)s\",\n",
      "    'Create Titles':'create_titles.py -m %(mapping)s -f',\n",
      "    'Format Template':'format_file.py -i %(template)s -k %(keys_for_replace)s -v %(values_for_replace)s -K %(keys_for_insert)s -V %(values_for_insert)s -o %(output)s',\n",
      "    'To PDF':'module load texlive_2013; cd %(path)s; lualatex %(input)s',\n",
      "    'PDF Smash':'gs -r150 -q -sPAPERSIZE=ledger -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -dFIXEDMEDIA -dPDFFitPage -dCompatibilityLevel=1.4 -sOutputFile=%(output)s -c 100000000 setvmthreshold -f %(pdfs)s',\n",
      "    'gunzip': 'gunzip -f %(input)s',\n",
      "    'Pick Closed Reference OTUs': 'pick_closed_reference_otus.py -i %(input)s -o %(output)s -r %(reference)s -t %(taxonomy)s -aO %(num_procs)s',\n",
      "    'Pick Closed Reference OTUs Without Taxonomy': 'pick_closed_reference_otus.py -i %(input)s -o %(output)s -r %(reference)s -aO %(num_procs)s',\n",
      "    'Filter Sequences to Fecal Only': 'filter_fasta.py -f %(input)s -o %(output)s --mapping_fp=%(mapping_file)s --valid_states=\"BODY_SITE:UBERON:feces\"',\n",
      "    'Filter Sequences': 'filter_fasta.py -f %(input)s -m %(otu_map)s -o %(output)s -n',\n",
      "    'Select Gamma': 'select_gamma.py -i %(input)s -o %(output)s -l %(level)f'\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the contaminant filtering, we need to setup a few paths. Lets get that taken care of."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Merged sequence files\n",
      "merged_sequences_full_length_fp = get_relative_new_path('merged_sequences_full_length.fna')\n",
      "merged_sequences_trimmed_100_fp = get_relative_new_path('merged_sequences_trimmed_100.fna')\n",
      "merged_sequences_full_length_fecal_only_fp = get_relative_new_path('merged_sequences_fecal_only_full_length.fna')\n",
      "merged_sequences_trimmed_100_fecal_only_fp = get_relative_new_path('merged_sequences_fecal_only_trimmed_100.fna')\n",
      "\n",
      "if trim_seqs:\n",
      "    merged_seqs_fp = merged_sequences_trimmed_100_fp\n",
      "    merged_seqs_fecal_only_fp = merged_sequences_trimmed_100_fecal_only_fp\n",
      "else:\n",
      "    merged_seqs_fp = merged_sequences_full_length_fp\n",
      "    merged_seqs_fecal_only_fp = merged_sequences_full_length_fecal_only_fp\n",
      "\n",
      "# File paths for the initial round of OTU picking, which is used to determine the most abundant proteobacterial OTUs\n",
      "initial_pick_otus_output_dir = get_relative_new_path('initial_otus')\n",
      "initial_otu_table = os.path.join(initial_pick_otus_output_dir, 'otu_table.biom')\n",
      "initial_otu_map = os.path.join(initial_pick_otus_output_dir, 'uclust_ref_picked_otus',\n",
      "                               os.path.splitext(os.path.split(merged_seqs_fecal_only_fp)[-1])[0]+'_otus.txt')\n",
      "\n",
      "# Files for select_gamma.py, which calculates cumulative abundances\n",
      "gammaproteo_cumulative_abundances_fp = get_relative_new_path('gammaproteo_cumul_abundances.txt')\n",
      "\n",
      "# File paths for the second round of OTU picking, where the full set of sequences are clustered using the most\n",
      "# abundant proteobacterial OTUs as a reference\n",
      "bloom_sequences_fp = get_relative_new_path('bloom_sequences_%f.fna' % bloom_abundance_threshold)\n",
      "bloom_picking_dir = get_relative_new_path('bloom_otus_%f' % bloom_abundance_threshold)\n",
      "\n",
      "# The final set of output sequences (after bloom sequence filtering)\n",
      "filtered_sequences_fp = get_relative_new_path('filtered_sequences_%f.fna' % bloom_abundance_threshold)\n",
      "\n",
      "# The final round of OTU picking, using the filtered_sequences_fp above\n",
      "filtered_otus_dir = get_relative_new_path('filtered_otus_%f' % bloom_abundance_threshold)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, lets fetch the American Gut data from EBI (if necessary)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "to_obtain = []\n",
      "for acc, seqs, map_ in zip(accessions, sequence_files, mapping_files):\n",
      "    if not os.path.exists(seqs) or not os.path.exists(map_):\n",
      "        to_obtain.append((acc, map_, seqs))\n",
      "\n",
      "if not debug:\n",
      "    for acc, map_, seqs in to_obtain:\n",
      "        print \"Fetching %s\" % acc\n",
      "        fetch_study(acc, map_, seqs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the fetch from EBI can result in multiple individual mapping files, lets go ahead and merge them into a single file for ease of use downstream."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if len(mapping_files) == 1:\n",
      "    mapping_fp = mapping_files[0]\n",
      "else:\n",
      "    mapping_fp = get_relative_new_path('combined_mapping.txt')\n",
      "    \n",
      "    merge_args = {'input_a': mapping_files[0], 'input_b': mapping_files[1], 'output': mapping_fp}\n",
      "    res = wait_on(submit(scripts['Merge Mapping Files'] % merge_args))\n",
      "    \n",
      "    for m in mapping_files[2:]:\n",
      "        merge_args = {'input_a': mapping_fp, 'input_b': m, 'output': mapping_fp}\n",
      "        res = wait_on(submit(scripts['Merge Mapping Files'] % merge_args))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And lets do the same for the sequence files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# concatenate all sequence files into one merged sequence file. This can take a while!\n",
      "with open(merged_sequences_full_length_fp, 'w') as merged_seqs:\n",
      "    concatenate_files([open(f, 'U') for f in sequence_files], merged_seqs)\n",
      "\n",
      "check_file(merged_sequences_full_length_fp)\n",
      "\n",
      "if trim_seqs:\n",
      "    with open(merged_sequences_full_length_fp, 'U') as merged_seqs, open(merged_sequences_trimmed_100_fp, 'w') as merged_seqs_trimmed:\n",
      "        trim_fasta(merged_seqs, merged_seqs_trimmed, 100)\n",
      "    \n",
      "    check_file(merged_sequences_trimmed_100_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When finding Gammaproteobacterial contaminants, we want to consider only fecal samples as those were the only sample types we observed the contamination with, so we will filter our sequences to contain only those that are associated with fecal samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filter_fecal_only_args = {\n",
      "    'input': merged_seqs_fp,\n",
      "    'output': merged_seqs_fecal_only_fp,\n",
      "    'mapping_file': mapping_fp\n",
      "}\n",
      "filter_fecal_only_job = submit_qsub(scripts['Filter Sequences to Fecal Only'] % filter_fecal_only_args, prj_name, queue='memroute', extra_args='-l pvmem=32gb')\n",
      "jobs = wait_on([filter_fecal_only_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The subsequent filtering procedure will proceed according to the following steps:\n",
      "\n",
      "1. Identify the most abundant Gammaproteobacteria and select the representative sequences from the most abundant, such that the cumulative abundance of remaining Gammaproteobacteria is no greater than 3%\n",
      "2. Find all sequences in the American Gut sequences that map to these OTUs at 97PI\n",
      "3. Of these sequences, if 90% of them are a single unique sequence, treat this sequence as a contaminant sequence\n",
      "4. Create reference database of these contaminant sequences\n",
      "5. Cluster (at 97PI) the full set of American Gut sequence against this reference set of contaminant sequences\n",
      "6. Remove all that map to those contaminant sequences\n",
      "7. Pick OTUs against the standard reference database (Greengenes 13_5 in this case)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_1_args = {\n",
      "    'input': merged_seqs_fecal_only_fp,\n",
      "    'output': initial_pick_otus_output_dir,\n",
      "    'reference': reference_rep_set,\n",
      "    'taxonomy': reference_taxonomy,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "closed_ref_1_job = submit(scripts['Pick Closed Reference OTUs'] % closed_ref_1_args)\n",
      "jobs = wait_on([closed_ref_1_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, identify gammaproteobacterial OTUs that are over-represented in our data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "select_gamma_args = {\n",
      "    'input': initial_otu_table,\n",
      "    'output': gammaproteo_cumulative_abundances_fp,\n",
      "    'level': select_gamma_threshold,\n",
      "}\n",
      "select_gamma_job = submit(scripts['Select Gamma'] % select_gamma_args)\n",
      "jobs = wait_on([select_gamma_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to grab all of our sequences that mapped to each of these OTUs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The IDs generated in the previous step by select_gamma.py\n",
      "gammaproteo_ids = {x.strip().split('\\t')[0] for x in open(gammaproteo_cumulative_abundances_fp, 'U').readlines()}\n",
      "\n",
      "print \"Counting unique sequences\"\n",
      "with open(initial_otu_map, 'U') as otu_map, open(merged_seqs_fp, 'U') as input_seqs:\n",
      "    unique_counts = count_unique_sequences_per_otu(gammaproteo_ids, otu_map, input_seqs)\n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then write out the sequences that are over the abundance threshold"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Writing bloom sequences...\"\n",
      "with open(bloom_sequences_fp, 'w') as bloom_f:\n",
      "    write_bloom_fasta(unique_counts, bloom_f, bloom_abundance_threshold)\n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now pick OTUs against this FASTA file, which represents the set of contaminant sequences, to identify all sequences in our original merged FASTA file are likely the result of contamination. We will discard these sequences before picking OTUs against our standard reference set.\n",
      "\n",
      "N.B.: If your contaminants.fna file is empty, then no contaminants were detected, and you do not need to proceed any further!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cluster (at 97PI) the full set of American Gut sequence against this reference set of contaminant sequences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_2_args = {\n",
      "    'input': merged_seqs_fp,\n",
      "    'output': bloom_picking_dir,\n",
      "    'reference': bloom_sequences_fp,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "closed_ref_2_job = submit(scripts['Pick Closed Reference OTUs Without Taxonomy'] % closed_ref_2_args)\n",
      "jobs = wait_on([closed_ref_2_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remove all that map to those contaminant sequences\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bloom_picking_otu_map = os.path.join(bloom_picking_dir, 'uclust_ref_picked_otus',\n",
      "                                     os.path.splitext(os.path.split(merged_seqs_fp)[-1])[0]+'_otus.txt')\n",
      "filter_fasta_args = {\n",
      "    'input': merged_seqs_fp,\n",
      "    'output':filtered_sequences_fp,\n",
      "    'otu_map': bloom_picking_otu_map\n",
      "}\n",
      "filter_fasta_job = submit(scripts['Filter Sequences'] % filter_fasta_args)\n",
      "jobs = wait_on([filter_fasta_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pick OTUs against the standard reference database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_3_args = {\n",
      "    'input': filtered_sequences_fp,\n",
      "    'output': filtered_otus_dir,\n",
      "    'reference': reference_rep_set,\n",
      "    'taxonomy': reference_taxonomy,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "closed_ref_3_job = submit(scripts['Pick Closed Reference OTUs'] % closed_ref_3_args)\n",
      "jobs = wait_on([closed_ref_3_job])\n",
      "\n",
      "print \"Your filtering is complete!  Your sequences are available at\", filtered_otus_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, lets setup the paths for the subsequent processing to create the figures. The first PCoA plot is a combination of the American Gut, Human Microbiome Project, Personal Genome Project and Global Gut datasets. These projects all used three different sequencing technologies however, and in order to combine them, we need to use the BIOM tables derived from sequence data all trimmed to the same length. See [here](http://www.ncbi.nlm.nih.gov/pubmed/23861384) for a more detailed discussion about meta-analyses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# working directory file paths for tables and metadata. \n",
      "# ag -> American Gut\n",
      "# pgp -> Personal Genome Project\n",
      "# hmp -> Human Microbiome Project\n",
      "# gg -> Global Gut\n",
      "#\n",
      "# _t_ -> table\n",
      "# _m_ -> mapping file\n",
      "#\n",
      "# 100nt -> trimmed to the first 100 nucleotides \n",
      "    \n",
      "#stage_static_files('fecal', working_dir, debug=debug)\n",
      "\n",
      "jobs = []\n",
      "for f in glob(os.path.join(working_dir, \"*.biom.gz\")):\n",
      "     jobs.append(submit(scripts['gunzip'] % {'input': f}))\n",
      "res = wait_on(jobs)\n",
      "\n",
      "ag_100nt_t_fp = os.path.join(filtered_otus_dir, 'otu_table.biom')\n",
      "ag_100nt_m_fp = mapping_fp\n",
      "check_file(ag_100nt_t_fp)\n",
      "check_file(ag_100nt_m_fp)\n",
      "\n",
      "pgp_100nt_t_fp = get_relative_existing_path('PGP_100nt.biom')\n",
      "pgp_100nt_m_fp = get_relative_existing_path('PGP_100nt.txt')\n",
      "hmp_100nt_t_fp = get_relative_existing_path('HMPv35_100nt.biom')\n",
      "hmp_100nt_m_fp = get_relative_existing_path('HMPv35_100nt.txt')\n",
      "gg_100nt_t_fp  = get_relative_existing_path('GG_100nt.biom')\n",
      "gg_100nt_m_fp  = get_relative_existing_path('GG_100nt.txt')\n",
      "\n",
      "template       = get_relative_existing_path('template_gut.tex')\n",
      "aglogo         = get_relative_existing_path('logoshape.pdf')\n",
      "fig1_legend    = get_relative_existing_path('figure1_legend.pdf')\n",
      "fig2_legend    = get_relative_existing_path('figure2_legend.pdf')\n",
      "fig2_2ndlegend = get_relative_existing_path('figure2_country_legend.pdf')\n",
      "fig3_legend    = get_relative_existing_path('figure3_legend.pdf')\n",
      "fig1_ovals     = get_relative_existing_path('figure1_ovals.png')\n",
      "fig2_ovals     = get_relative_existing_path('figure2_ovals.png')\n",
      "fig4_overlay   = get_relative_existing_path('figure4_overlay.pdf')\n",
      "ball_legend    = get_relative_existing_path('ball_legend.pdf')\n",
      "title          = get_relative_existing_path('youramericangutsampletext.pdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Paths to identified data if available, password for the data and the path to any previously printed results (by barcode)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path_to_participant_data = None\n",
      "passwd_for_participant_data = None\n",
      "path_to_previously_printed = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Manage identified participant data if provided. **Note: these data are _not_ provided for privacy reasons.** Processing can proceed without identified data, however."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "participants = parse_identifying_data(path_to_participant_data, passwd_for_participant_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prev_printed = parse_previously_printed(path_to_previously_printed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to massage the metadata to improve our ability to compare samples. Specifically, were going to: \n",
      "\n",
      "* simplify body sites into their major categories (e.g., transform \"forehead\" and \"skin of hand\" to just \"skin\")\n",
      "* simplify country codes (e.g., GAZ:Venezuela to Venezuela)\n",
      "* simplify the experiment title (e.g., American Gut Project to AGP)\n",
      "* create a hybrid field combining the experiment title with the body site"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# new file paths\n",
      "ag_100nt_m_massaged_fp = get_relative_new_path('AG_100nt_massaged.txt')\n",
      "gg_100nt_m_massaged_fp = get_relative_new_path('GG_100nt_massaged.txt')\n",
      "pgp_100nt_m_massaged_fp = get_relative_new_path('PGP_100nt_massaged.txt')\n",
      "hmp_100nt_m_massaged_fp = get_relative_new_path('HMP_100nt_massaged.txt')\n",
      "\n",
      "# massage\n",
      "massage_mapping(open(ag_100nt_m_fp, 'U'), open(ag_100nt_m_massaged_fp, 'w'), 'body_site', 'AGP')\n",
      "massage_mapping(open(gg_100nt_m_fp, 'U'), open(gg_100nt_m_massaged_fp, 'w'), 'body_site', 'GG')\n",
      "massage_mapping(open(pgp_100nt_m_fp, 'U'), open(pgp_100nt_m_massaged_fp, 'w'), 'body_site', 'PGP')\n",
      "massage_mapping(open(hmp_100nt_m_fp, 'U'), open(hmp_100nt_m_massaged_fp, 'w'), 'bodysite', 'HMP')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've massaged the metadata, we need to merge the mapping files from all the analyses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup output paths, (mm -> massaged mapping)\n",
      "hmp_pgp_mm_fp = get_relative_new_path('HMP_PGP_100nt_massaged.txt')\n",
      "ag_gg_mm_fp = get_relative_new_path('AG_GG_100nt_massaged.txt')\n",
      "hmp_pgp_ag_gg_mm_fp = get_relative_new_path('HMP_GG_AG_PGP_100nt_massaged.txt')\n",
      "\n",
      "hmp_pgp_cmd_args = {'input_a':hmp_100nt_m_massaged_fp,\n",
      "                  'input_b':pgp_100nt_m_massaged_fp,\n",
      "                  'output':hmp_pgp_mm_fp}\n",
      "\n",
      "ag_gg_cmd_args = {'input_a':ag_100nt_m_massaged_fp,\n",
      "                  'input_b':gg_100nt_m_massaged_fp,\n",
      "                  'output':ag_gg_mm_fp}\n",
      "\n",
      "hmp_pgp_ag_gg_cmd_args = {'input_a':hmp_pgp_mm_fp,\n",
      "                          'input_b':ag_gg_mm_fp,\n",
      "                          'output':hmp_pgp_ag_gg_mm_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_job = submit(scripts['Merge Mapping Files'] % hmp_pgp_cmd_args)\n",
      "ag_gg_job = submit(scripts['Merge Mapping Files'] % ag_gg_cmd_args)\n",
      "jobs = wait_on([hmp_pgp_job, ag_gg_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_ag_gg_job = submit(scripts['Merge Mapping Files'] % hmp_pgp_ag_gg_cmd_args)\n",
      "jobs = wait_on(hmp_pgp_ag_gg_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to simplify compute on the downstream processes, we're going to filter out the metadata columns that we aren't interested in."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig1_m_fp = get_relative_new_path('HMP_GG_AG_PGP_figure1.txt')\n",
      "fig2_m_fp = get_relative_new_path('AG_GG_fecal_figure2.txt')\n",
      "fig3_m_fp = get_relative_new_path('AG_fecal_figure3.txt')\n",
      "fig4_m_fp = get_relative_new_path('AG_fecal_figure4.txt')\n",
      "\n",
      "# for the first PCoA, keep only these 4 columns regardless of value\n",
      "fig1_filter_criteria = {'TITLE_ACRONYM':None, \n",
      "                        'SIMPLE_BODY_SITE':None,\n",
      "                        'TITLE_BODY_SITE':None, \n",
      "                        'HMP_SITE':None}\n",
      "\n",
      "# for the second PCoA, we want only the same columns but only the fecal samples\n",
      "fig2_filter_criteria = {'TITLE_ACRONYM':None, \n",
      "                        'AGE':None, \n",
      "                        'SIMPLE_BODY_SITE':lambda x: x == 'FECAL', \n",
      "                        'COUNTRY':None}\n",
      "\n",
      "# for the third PCoA, we want only two columns and only fecal samples\n",
      "fig3_filter_criteria = {'TITLE_ACRONYM':None, \n",
      "                        'SIMPLE_BODY_SITE':lambda x: x == 'FECAL'}\n",
      "\n",
      "# for the taxonomy figure, we want to retain fecal samples and a few additional categories\n",
      "fig4_filter_criteria = {'TITLE_ACRONYM':None,\n",
      "                        'AGE_CATEGORY':None,\n",
      "                        'SEX':None,\n",
      "                        'BMI_CATEGORY':None,\n",
      "                        'DIET_TYPE':None,\n",
      "                        'SIMPLE_BODY_SITE':lambda x: x == 'FECAL'}\n",
      "\n",
      "filter_mapping_file(open(hmp_pgp_ag_gg_mm_fp, 'U'),    open(fig1_m_fp, 'w'), fig1_filter_criteria)\n",
      "filter_mapping_file(open(ag_gg_mm_fp, 'U'),            open(fig2_m_fp, 'w'), fig2_filter_criteria)\n",
      "filter_mapping_file(open(ag_100nt_m_massaged_fp, 'U'), open(fig3_m_fp, 'w'), fig3_filter_criteria)\n",
      "filter_mapping_file(open(ag_100nt_m_massaged_fp, 'U'), open(fig4_m_fp, 'w'), fig4_filter_criteria)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the figures, we'll need to merge and filter the tables in a few ways. For figure 1, we want to place the American Gut population in the context of other large studies. To do so, we need to first merge the tables together. Since there are 4 tables to merge, we need to use two merge calls. (It is also feasible to use QIIME's ``parallel_merge_otu_tables.py`` here as well). Figure 2 is a combination of Global Gut and the American Gut, but only fecal samples, as is figure 3. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting paths\n",
      "hmp_pgp_t_fp = get_relative_new_path(\"HMP_PGP_100nt.biom\")\n",
      "ag_gg_t_fp = get_relative_new_path(\"AG_GG_100nt.biom\")\n",
      "hmp_gg_ag_pgp_t_fp = get_relative_new_path(\"HMP_GG_AG_PGP_100nt.biom\")\n",
      "\n",
      "# setup the command arguments for each call\n",
      "hmp_pgp_cmd_args = {'input_a':hmp_100nt_t_fp, \n",
      "                   'input_b':pgp_100nt_t_fp,\n",
      "                   'output':hmp_pgp_t_fp}\n",
      "ag_gg_cmd_args = {'input_a':ag_100nt_t_fp,\n",
      "                   'input_b':gg_100nt_t_fp,\n",
      "                   'output':ag_gg_t_fp}\n",
      "hmp_gg_ag_pgp_cmd_args = {'input_a':ag_gg_t_fp,\n",
      "                          'input_b':hmp_pgp_t_fp,\n",
      "                          'output':hmp_gg_ag_pgp_t_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_job = submit(scripts['Merge OTU Tables'] % hmp_pgp_cmd_args)\n",
      "ag_gg_job = submit(scripts['Merge OTU Tables'] % ag_gg_cmd_args)\n",
      "jobs = wait_on([hmp_pgp_job, ag_gg_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_ag_pgp_job = submit(scripts['Merge OTU Tables'] % hmp_gg_ag_pgp_cmd_args)\n",
      "jobs = wait_on(hmp_gg_ag_pgp_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the data are combined, we need to rarify them in order to normalize for sequencing effort. The Human Microbiome Project has the shallowest coverage per sample, and when operating with the HMP dataset, a rarifaction depth of 1,000 is a reasonable balance between effort and retaining sufficient numbers of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting path\n",
      "hmp_gg_ag_pgp_t_1k_fp = get_relative_new_path(\"HMP_GG_AG_PGP_100nt_even1k.biom\")\n",
      "\n",
      "# setup the command arguments\n",
      "hmp_gg_ag_pgp_t_1k_cmd_args = {'input':hmp_gg_ag_pgp_t_fp,\n",
      "                               'output':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                               'depth':'1000'}\n",
      "\n",
      "# rarifiy and block until completion\n",
      "hmp_gg_ag_pgp_t_1k_job = submit(scripts['Single Rarifaction'] % hmp_gg_ag_pgp_t_1k_cmd_args)\n",
      "res = wait_on(hmp_gg_ag_pgp_t_1k_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our rarified table, that combines the AG, PGP, HMP, and GG datasets, and a merged mapping file, we can now compute the beta diversity of this OTU table. This step is computationally demanding, and will run for a few hours on a 100 processors. **Note, the final merge job for parallel beta diversity requires > 8GB of RAM**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup output directory\n",
      "bdiv_dir = lambda x: 'bdiv_' + os.path.basename(x).split('.',1)[0]\n",
      "hmp_gg_ag_pgp_1k_unweighted_unifrac_d = get_relative_new_path(bdiv_dir(hmp_gg_ag_pgp_t_1k_fp))\n",
      " \n",
      "# setup beta diversity arguments\n",
      "prefix = 'ag2_bdiv_'\n",
      "hmp_gg_ag_pgp_cmd_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                          'output':hmp_gg_ag_pgp_1k_unweighted_unifrac_d,\n",
      "                          'job_prefix':prefix,\n",
      "                          'num_jobs':NUM_PROCS,\n",
      "                          'gg97_tree':reference_tree}\n",
      "\n",
      "# submit and wait\n",
      "hmp_gg_ag_pgp_bdiv_job = submit_qsub(scripts['Parallel Beta Diversity'] % hmp_gg_ag_pgp_cmd_args, prj_name, queue='memroute', extra_args='-l pvmem=16gb')\n",
      "res = wait_on(hmp_gg_ag_pgp_bdiv_job, additional_prefix=prefix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have calculated beta diversity for all of the samples, we need to filter the table down to the subsets that we're interested in. For figure 1 we want to use all of the samples. But, for figure 2 and 3, we only want to use subsets of the full distance matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bdiv_path = get_relative_existing_path(os.path.join(hmp_gg_ag_pgp_1k_unweighted_unifrac_d, \n",
      "                                                    'unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k.txt')) \n",
      "\n",
      "full_bdiv = bdiv_path\n",
      "fig_path = lambda x: full_bdiv.rsplit('.txt',1)[0] + '-' + x + '.txt'\n",
      "fig1_bdiv = fig_path('fig1')\n",
      "fig2_bdiv = fig_path('fig2')\n",
      "fig3_bdiv = fig_path('fig3')\n",
      "\n",
      "fig1_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig1_bdiv,\n",
      "                 'sample_ids':fig1_m_fp}\n",
      "fig2_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig2_bdiv,\n",
      "                 'sample_ids':fig2_m_fp}\n",
      "fig3_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig3_bdiv,\n",
      "                 'sample_ids':fig3_m_fp}\n",
      "\n",
      "jobs = []\n",
      "jobs.append(submit(scripts['Filter Distance Matrix'] % fig1_cmd_args))\n",
      "jobs.append(submit(scripts['Filter Distance Matrix'] % fig2_cmd_args))\n",
      "jobs.append(submit(scripts['Filter Distance Matrix'] % fig3_cmd_args))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our Unifrac distance matrices, need to transform them into principal coordinates space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pc_path = lambda x: x.rsplit('.txt',1)[0] + '_pc.txt'\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(fig1_bdiv)\n",
      "check_file(fig2_bdiv)\n",
      "check_file(fig3_bdiv)\n",
      "\n",
      "fig1_pc = pc_path(fig1_bdiv)\n",
      "fig2_pc = pc_path(fig2_bdiv)\n",
      "fig3_pc = pc_path(fig3_bdiv)\n",
      "\n",
      "# setup our arguments\n",
      "fig1_cmd_args = {'input':fig1_bdiv,\n",
      "                 'output':fig1_pc}\n",
      "fig2_cmd_args = {'input':fig2_bdiv,\n",
      "                 'output':fig2_pc}\n",
      "fig3_cmd_args = {'input':fig3_bdiv,\n",
      "                 'output':fig3_pc}\n",
      "\n",
      "# submit the jobs\n",
      "jobs = []\n",
      "jobs.append(submit(scripts['Principal Coordinates'] % fig1_cmd_args))\n",
      "jobs.append(submit(scripts['Principal Coordinates'] % fig2_cmd_args))\n",
      "jobs.append(submit(scripts['Principal Coordinates'] % fig3_cmd_args))\n",
      "job_results = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now to actually make figures 1, 2, and 3 using [Emperor](http://qiime.org/emperor/), a WebGL-based Principal Coordinates viewer. While Emperor is the only tool that we're aware of that can effectively scale to these size datasets for 3D visualization and painting of arbitrary metadata, the tie to WebGL makes its use here a little bit of a challenge. Specifically, we'll be able to generate the plots, but we cannot automatically generate the images from the Notebook. First, lets get Emperor up and running, in the following cell, we'll describe how what needs to happen to produce the images."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# quick little helper method\n",
      "emp_path = lambda x: x.rsplit('.txt',1)[0] + '-emp'\n",
      "\n",
      "# verify expected files are present\n",
      "check_file(fig1_pc)\n",
      "check_file(fig2_pc)\n",
      "check_file(fig3_pc)\n",
      "\n",
      "# setup output paths\n",
      "fig1_emp = emp_path(fig1_pc)\n",
      "fig2_emp = emp_path(fig2_pc)\n",
      "fig3_emp = emp_path(fig3_pc)\n",
      "fig3_filter = get_relative_new_path(\"figure3.biom\")\n",
      "fig3_taxa = get_relative_new_path(\"figure3_taxa\")\n",
      "fig3_taxa_mapping_fp = os.path.join(fig3_taxa, os.path.splitext(os.path.basename(fig3_m_fp))[0] + '_L2.txt')\n",
      "\n",
      "# setup arguments\n",
      "fig3_filter_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                    'output':fig3_filter,\n",
      "                    'sample_id_fp':fig3_m_fp}\n",
      "fig3_summarize_args = {'input':fig3_filter,\n",
      "                       'output':fig3_taxa,\n",
      "                       'level':'2',\n",
      "                       'mapping':fig3_m_fp}\n",
      "\n",
      "fig1_cmd_args = {'input':fig1_pc, \n",
      "                 'output':fig1_emp, \n",
      "                 'mapping':fig1_m_fp}\n",
      "fig2_cmd_args = {'input':fig2_pc, \n",
      "                 'output':fig2_emp, \n",
      "                 'mapping':fig2_m_fp}\n",
      "fig3_cmd_args = {'input':fig3_pc, \n",
      "                 'output':fig3_emp, \n",
      "                 'mapping':fig3_taxa_mapping_fp}\n",
      "\n",
      "# filter the table down to just the AG fecal samples\n",
      "filter_job = submit(scripts['Filter Samples'] % fig3_filter_args)\n",
      "res = wait_on(filter_job)\n",
      "\n",
      "taxa_job = submit(scripts['Summarize Taxa Mapping'] % fig3_summarize_args)\n",
      "res = wait_on(taxa_job)\n",
      "\n",
      "check_file(fig3_taxa_mapping_fp)\n",
      "\n",
      "jobs = []\n",
      "jobs.append(submit(scripts['Make Emperor'] % fig1_cmd_args))\n",
      "jobs.append(submit(scripts['Make Emperor'] % fig2_cmd_args))\n",
      "jobs.append(submit(scripts['Make Emperor'] % fig3_cmd_args))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's the required manual intervention:\n",
      "\n",
      "1. Click on the first URL link in the resulting pane\n",
      "2. Rotate the view to your a nice perspective\n",
      "3. Click the \"Options\" tab on the right side, set a filename prefix (e.g., figure1) and then click on the \"Multishot\" button. **NOTE: this will produce a lot of files on to your LOCAL computer!**\n",
      "4. Wait until all of the images have downloaded\n",
      "5. Pack up the local image files:\n",
      "  - Change directories on your LOCAL computer to your downloads folder\n",
      "  - Execute (make sure to replace **prefix** with the prefix specified in 3): tar czf **prefix**.tar.gz **prefix**.*\n",
      "      - If to many files, you can do: \"find . -name \"Figure_1.*\" -type f -print0 | tar czf Figure_1.tar.gz -T - --null\"\n",
      "      - (adapted from: http://stackoverflow.com/questions/5891866/find-files-and-tar-them-with-spaces)\n",
      "6. Copy the **prefix**.tar.gz file back to your compute resource, and place it in your home directory (~/ or $HOME)\n",
      "7. repeat for each figure\n",
      "\n",
      "If the figures are redone, then you may need to clear your web-browsers cache befone the HTML links below will work correct. Optionally, you could hit refresh a few times after clicking the links as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emp_index = lambda x: os.path.join(x, 'index.html')\n",
      "\n",
      "# form the expected paths for Emperor\n",
      "fig1 = emp_index(fig1_emp)\n",
      "fig2 = emp_index(fig2_emp)\n",
      "fig3 = emp_index(fig3_emp)\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(fig1)\n",
      "check_file(fig2)\n",
      "check_file(fig3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig1, result_html_prefix='<p>Figure 1:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Figure 2 is slightly different: we need to produce a global shot with the uninformative ages (None/NA/0.0) removed using the mockup functionality, then we need to unselect mockup, change the uninformative ages to grey, and do a full image dump. The motivation is that the background image will not contain the samples missing the ages, but we can still display them for each populated template. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig2, result_html_prefix='<p>Figure 2:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig3, result_html_prefix='<p>Figure 3:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the images are created, we need to copy them to our working area. It is possible file paths will be different, in which case, you may need to change the DOWNLOAD_DIRECTORY variable in the next cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DOWNLOAD_DIRECTORY = os.path.expandvars('$HOME')\n",
      "FIGURE1_EXPECTED_FILENAME = 'Figure_1.tar.gz'\n",
      "FIGURE2_EXPECTED_FILENAME = 'Figure_2.tar.gz'\n",
      "FIGURE3_EXPECTED_FILENAME = 'Figure_3.tar.gz'\n",
      "\n",
      "# helper function for creating wildcard paths\n",
      "source_path = lambda x,y: os.path.join(x,y)\n",
      "\n",
      "# setup the destination paths\n",
      "emperor_images = get_relative_new_path('emperor_images_svg')\n",
      "\n",
      "if not os.path.exists(emperor_images):\n",
      "    os.mkdir(emperor_images)\n",
      "\n",
      "# setup the source paths\n",
      "figure1_src = source_path(DOWNLOAD_DIRECTORY, FIGURE1_EXPECTED_FILENAME)\n",
      "figure2_src = source_path(DOWNLOAD_DIRECTORY, FIGURE2_EXPECTED_FILENAME)\n",
      "figure3_src = source_path(DOWNLOAD_DIRECTORY, FIGURE3_EXPECTED_FILENAME)\n",
      "\n",
      "check_file(figure1_src)\n",
      "check_file(figure2_src)\n",
      "check_file(figure3_src)\n",
      "\n",
      "# unpack the tarballs\n",
      "jobs = []\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure1_src, emperor_images)))\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure2_src, emperor_images)))\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure3_src, emperor_images)))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to produce the actual PDF files of the PCoA plots that will go into the individualized results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_ag_IDs = set([l.strip().split('\\t')[0] for l in open(ag_100nt_m_fp) if not l.startswith('#')])\n",
      "all_emp_SVGs = os.listdir(emperor_images)\n",
      "template_files = get_relative_new_path('template_files')\n",
      "\n",
      "if not os.path.exists(template_files):\n",
      "    os.mkdir(template_files)\n",
      "\n",
      "svg_smash_args = {'input':emperor_images, \n",
      "                  'output':template_files, \n",
      "                  'prefix':None, \n",
      "                  'sample_id':None}\n",
      "\n",
      "commands = construct_svg_smash_commands(all_emp_SVGs, all_ag_IDs, scripts['SVG Smash'], svg_smash_args)\n",
      "res = farm_commands(commands, 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets create the phylum level taxonomy summary plots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig4_sex_fp = get_relative_new_path('fig4_sex.biom')\n",
      "fig4_age_fp = get_relative_new_path('fig4_age_category.biom')\n",
      "fig4_diet_fp = get_relative_new_path('fig4_diet.biom')\n",
      "fig4_bmi_fp = get_relative_new_path('fig4_bmi_category.biom')\n",
      "ag_fecal_t_1k_fp = get_relative_new_path('ag_fecal_even1k.biom')\n",
      "template_files = get_relative_existing_path('template_files')\n",
      "\n",
      "check_file(hmp_gg_ag_pgp_t_1k_fp)\n",
      "check_file(fig4_m_fp)\n",
      "\n",
      "filter_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "               'output':ag_fecal_t_1k_fp,\n",
      "               'sample_id_fp':fig4_m_fp}\n",
      "\n",
      "otu_by_cat_sex_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_sex_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'SEX'}\n",
      "otu_by_cat_age_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_age_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'AGE_CATEGORY'}\n",
      "otu_by_cat_diet_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_diet_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'DIET_TYPE'}\n",
      "otu_by_cat_bmi_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_bmi_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'BMI_CATEGORY'}\n",
      "\n",
      "phyla_plot_args = {'input':ag_fecal_t_1k_fp,\n",
      "                   'output':template_files,\n",
      "                   'mapping':fig4_m_fp,\n",
      "                   'debug':\"-d\" if debug else \"\",\n",
      "                   'categories':'SEX:%s, AGE_CATEGORY:%s, DIET_TYPE:%s, BMI_CATEGORY:%s' % (fig4_sex_fp, fig4_age_fp, fig4_diet_fp, fig4_bmi_fp)}\n",
      "                \n",
      "# filter the ag table down to just the fecal samples for fig 4\n",
      "filter_job = submit(scripts['Filter Samples'] % filter_args)\n",
      "res = wait_on(filter_job)\n",
      "\n",
      "# get the summarized tables\n",
      "jobs = []\n",
      "jobs.append(submit(scripts['Summarize OTU by Category'] % otu_by_cat_sex_args))\n",
      "jobs.append(submit(scripts['Summarize OTU by Category'] % otu_by_cat_age_args))\n",
      "jobs.append(submit(scripts['Summarize OTU by Category'] % otu_by_cat_diet_args))\n",
      "jobs.append(submit(scripts['Summarize OTU by Category'] % otu_by_cat_bmi_args))\n",
      "res = wait_on(jobs)\n",
      "\n",
      "# farm out the phyla plots\n",
      "sample_ids = [l.split('\\t')[0] for l in open(fig4_m_fp) if not l.startswith('#')]\n",
      "commands = construct_phyla_plots_cmds(sample_ids, scripts['Make Phyla Plots'], phyla_plot_args)\n",
      "res = farm_commands(commands, 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And for the last figure, lets create the over represented taxonomy table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ag_fecal_t_norare_fp = get_relative_new_path('ag_fecal_norare_filtered.biom')\n",
      "fig5_tax_sum = get_relative_new_path('ag_fecal_norare_taxasum')\n",
      "template_files = get_relative_existing_path('template_files')\n",
      "\n",
      "check_file(ag_fecal_t_1k_fp)\n",
      "\n",
      "filter_args = {'input':ag_100nt_t_fp,\n",
      "               'output':ag_fecal_t_norare_fp,\n",
      "               'sample_id_fp':fig4_m_fp}\n",
      "\n",
      "sum_args = {'input':ag_fecal_t_norare_fp,\n",
      "            'output':fig5_tax_sum,\n",
      "            'level':\"2,3,6\"}\n",
      "\n",
      "filter_job = submit(scripts['Filter Samples'] % filter_args)\n",
      "res = wait_on(filter_job)\n",
      "\n",
      "sum_job = submit(scripts['Summarize Taxa'] % sum_args)\n",
      "res = wait_on(sum_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "template_files = get_relative_existing_path('template_files')\n",
      "fig5_lvl6 = get_relative_existing_path('ag_fecal_norare_taxasum/ag_fecal_norare_filtered_L6.biom')\n",
      "\n",
      "sig_args = {'input':fig5_lvl6,\n",
      "            'output':template_files}\n",
      "\n",
      "sig_job = submit(scripts['OTU Significance'] % sig_args) \n",
      "res = wait_on(sig_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets also dump out the per sample taxonomy information that we can distribute on the participant website."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "per_sample_taxa_summaries(open(fig5_lvl6), get_relative_new_path('template_files/Figure_6_%s.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we construct a method that will return the necessary commands to populate a template per sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "static_paths = {'template': template,\n",
      "                'aglogo':aglogo,\n",
      "                'fig1_legend': fig1_legend,\n",
      "                'fig2_legend': fig2_legend,\n",
      "                'fig2_2ndlegend': fig2_2ndlegend,\n",
      "                'fig3_legend': fig3_legend,\n",
      "                'fig4_overlay': fig4_overlay,\n",
      "                'fig1_ovals': fig1_ovals,\n",
      "                'fig2_ovals': fig2_ovals,\n",
      "                'ball_legend': ball_legend,\n",
      "                'title': title,\n",
      "                'working_dir': working_dir}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets build up the list of commands that will populate the templates per sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_file(ag_100nt_m_massaged_fp)\n",
      "check_file(template)\n",
      "\n",
      "if not os.path.exists(get_relative_new_path('unidentified')):\n",
      "    os.mkdir(get_relative_new_path('unidentified'))\n",
      "if participants is not None and os.path.exists(get_relative_new_path('identified')):\n",
      "    os.mkdir(get_relative_new_path('identified'))\n",
      "             \n",
      "base_setup_cmd = 'cd %s; %s'\n",
      "\n",
      "indiv_cmds, latex_cmds, missing = construct_bootstrap_and_latex_commands(all_ag_IDs, participants, \n",
      "                                                                         get_relative_existing_path,\n",
      "                                                                         static_paths, base_setup_cmd, \n",
      "                                                                         scripts['To PDF'])\n",
      "_ = farm_commands(indiv_cmds, 25)\n",
      "_ = farm_commands(latex_cmds, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These last few cells \"harvest\" the populated templates, which places them in a single folder. We can then \"smash\" or combine multiple templates together to simplify the results printing process."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if participants is not None:\n",
      "    res = farm_commands(harvest(get_relative_existing_path('identified')), 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = farm_commands(harvest(get_relative_existing_path('unidentified')), 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We only do this part if we have identifying information on hand."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if participants is not None:\n",
      "    harvest_path = get_path('identified/harvested')\n",
      "    identified_smash = pdf_smash(harvest_path, 'identified', previously_printed=prev_printed)\n",
      "    res = farm_commands(identified_smash, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And we're done! You can view all of the intermediate files in the working directory which will be displayed below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print working_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
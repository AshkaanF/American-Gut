{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This IPython notebook assumes precomputed BIOM tables and metadata are readily available. The purpose of this notebook is to output a summary PDF, per sample, present in the American Gut dataset. This requires the full American Gut, the [Human Microbiome Project](http://www.ncbi.nlm.nih.gov/pubmed/22699609), [Global Gut](http://www.ncbi.nlm.nih.gov/pubmed/22699611) and unpublished [Personal Genome Project](http://personalgenomes.org/) microbiome samples. Please see XXX for a discussion on how these tables were created. This notebook assumes the following: \n",
      "\n",
      "* a PBS/Torque-based compute cluster in which to submit jobs to\n",
      "* QIIME 1.7 is available in the path\n",
      "* A custom Emperor [branch](https://github.com/eldeveloper/emperor/tree/faces_plus_lines) is available in the path\n",
      "* TexLive"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this notebook is to produce a framable results PDF for every American Gut fecal sample. Specifically, for each sample, we will be producing the following figures:\n",
      "\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, the Human Microbiome Project, the Global Gut and the Personal Genome Project datasets.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, Global Gut project, highlighting the variation in host age has on the sample.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project.\n",
      "* Taxonomy summary of the sample, and for comparison, a few collapsed American Gut groups and Michael Pollan's pre and post antibiotics sample.\n",
      "* Significantly differentiated operational taxonomic units in the sample compared to other American Gut samples "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, lets get our environment up and running. The script utils.ipy contains some helper methods for submitting jobs to Torque-based clusters. In addition, lets create a new directory for us to work under, setup some helper functions and some paths. Please note, you'll need to specify the path to the 97% Greengenes tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run cluster_utils.ipy\n",
      "\n",
      "# get the current absolute path\n",
      "_basedir = os.path.abspath('.')\n",
      "\n",
      "### to toss existing saved environment\n",
      "# drop_env()\n",
      "\n",
      "if recover():\n",
      "    recover_env()\n",
      "    print \"Recovered %s\" % prj_name\n",
      "else:    \n",
      "    create_env(\"ag_mod2\")\n",
      "    print \"Working on: %s\" % prj_name\n",
      "\n",
      "if not working_dir.rsplit('/', 1)[0].endswith('American-Gut/ipynb'):\n",
      "    raise ValueError, \"The notebook must be run from the American-Gut/ipynb directory!\"\n",
      "\n",
      "# submission wrapper\n",
      "submit = lambda cmd: submit_qsub(cmd, job_name=prj_name, queue='memroute', extra_args='-l pvmem=8gb')\n",
      "    \n",
      "# path wrapper\n",
      "get_path = lambda x: os.path.join(working_dir, x)\n",
      "    \n",
      "# set the number of processors parallel tasks will use\n",
      "NUM_PROCS = 100\n",
      "\n",
      "# set the path to the Greengenes 13_5 97% OTU tree\n",
      "#greengenes135_97_tree_fp = os.path.expandvars('/ABSOLUTE/PATH/MUST/BE/SPECIFIED')\n",
      "greengenes135_97_tree_fp = os.path.expandvars('$HOME/ResearchWork/gg_13_5_otus/trees/97_otus.tree')\n",
      "if not os.path.exists(greengenes135_97_tree_fp):\n",
      "    raise ValueError(\"Greengenes tree not found, make sure to set the path in the variable 'greengenes135_97_tree_fp'\")\n",
      "    \n",
      "def check_file(f):\n",
      "    if not os.path.exists(f):\n",
      "        raise ValueError(\"Cannot continue! The file %s does not exist!\" % f)\n",
      "\n",
      "def cp_raw(src,dst):\n",
      "    \"\"\"make copies of the original data\"\"\"\n",
      "    !cp $src $dst\n",
      "\n",
      "qiime_scripts = {\n",
      "    'Merge OTU Tables':'merge_otu_tables.py -i %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Single Rarifaction':'single_rarefaction.py -i %(input)s -o %(output)s -d %(depth)s',\n",
      "    'Parallel Beta Diversity':'parallel_beta_diversity.py -i %(input)s -o %(output)s -X %(job_prefix)s -O %(num_jobs)s -m unweighted_unifrac -t %(gg97_tree)s',\n",
      "    'Principal Coordinates':'principal_coordinates.py -i %(input)s -o %(output)s',\n",
      "    'Merge Mapping Files':'merge_mapping_files.py -m %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Filter Samples':'filter_samples_from_otu_table.py -i %(input)s -o %(output)s --sample_id_fp=%(sample_id_fp)s',\n",
      "    'Summarize OTU by Category':'summarize_otu_by_cat.py -i %(mapping)s -o %(output)s -n -c %(otu_table)s -m %(category)s',\n",
      "    'Filter Distance Matrix':'filter_distance_matrix.py -i %(input)s -o %(output)s --sample_id_fp=%(sample_ids)s',\n",
      "    'Summarize Taxa':'summarize_taxa.py -i %(input)s -o %(output)s -L %(level)s'\n",
      "    }\n",
      "other_scripts = {\n",
      "    'Signifigant OTU Table':'otu_significance.py -i %(input)s -o %(output)s -l %(level)s -t adasdsdasd', # input -> biomtable, output->directory,\n",
      "    'Taxonomy Comparison':'taxonomy_comparison.py -i %(input)s -m %(mapping)s -l %(level)s -o %(output)s -c %(list_of_categories)s',\n",
      "    'Make Emperor':'make_emperor.py -i %(input)s -o %(output)s -m %(mapping)s',\n",
      "    'SVG Smash':'replace_svg_object.py -i %(input)s -o %(output)s --prefix %(prefix)s --sample_id=%(sample_id)s',\n",
      "    'Make Phyla Plots':\"make_phyla_plots_AGP.py -i %(input)s -m %(mapping)s -o %(output)s -c '%(categories)s' -s %(samples)s\",\n",
      "    'OTU Significance':\"generate_otu_signifigance_tables_AGP.py -i %(input)s -o %(output)s\",\n",
      "    'Create Titles':'create_titles.py -m %(mapping)s -f',\n",
      "    'Format Template':'format_file.py -i %(template)s -k %(keys_for_replace)s -v %(values_for_replace)s -K %(keys_for_insert)s -V %(values_for_insert)s -o %(output)s',\n",
      "    'To PDF':'pdflatex %(input)s'\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Recovered ag_mod2_l2E\n"
       ]
      }
     ],
     "prompt_number": 247
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, lets setup the paths and some helper methods. The first PCoA plot is a combination of the American Gut, Human Microbiome Project, Personal Genome Project and Global Gut datasets. These projects all used three different sequencing technologies however, and in order to combine them, we need to use the BIOM tables derived from sequence data all trimmed to the same length. See XXX for a more detailed discussion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# working directory file paths for tables and metadata. \n",
      "# ag -> American Gut\n",
      "# pgp -> Personal Genome Project\n",
      "# hmp -> Human Microbiome Project\n",
      "# gg -> Global Gut\n",
      "#\n",
      "# _t_ -> table\n",
      "# _m_ -> mapping file\n",
      "#\n",
      "# 100nt -> trimmed to the first 100 nucleotides \n",
      "# original file paths\n",
      "get_raw_support_filepath = lambda x: os.path.join('../template_files', x)\n",
      "get_raw_data_filepath = lambda x, y: os.path.join('../data', x, y)\n",
      "\n",
      "# setup raw data sources\n",
      "raw_ag_100nt_t_fp = get_raw_data_filepath('AG', 'AG_100nt.biom.gz')\n",
      "raw_ag_100nt_m_fp = get_raw_data_filepath('AG', 'AG_100nt.txt')\n",
      "raw_ag_t_fp = get_raw_data_filepath('AG', 'AG.biom.gz')\n",
      "raw_ag_m_fp = get_raw_data_filepath('AG', 'AG.txt')\n",
      "\n",
      "raw_pgp_100nt_t_fp = get_raw_data_filepath('PGP', 'PGP_100nt.biom.gz')\n",
      "raw_pgp_100nt_m_fp = get_raw_data_filepath('PGP', 'PGP_100nt.txt')\n",
      "\n",
      "raw_hmp_100nt_t_fp = get_raw_data_filepath('HMP', 'HMPv35_100nt.biom.gz')\n",
      "raw_hmp_100nt_m_fp = get_raw_data_filepath('HMP', 'HMPv35_100nt.txt')\n",
      "\n",
      "raw_gg_100nt_t_fp = get_raw_data_filepath('GG', 'GG_100nt.biom.gz')\n",
      "raw_gg_100nt_m_fp = get_raw_data_filepath('GG', 'GG_100nt.txt')\n",
      "\n",
      "raw_latex_template = get_raw_support_filepath('module_2_template.tex')\n",
      "raw_latex_aglogo = get_raw_support_filepath('aglogo.pdf')\n",
      "raw_latex_fig1_legend = get_raw_support_filepath('figure_1_labels_two_rows.pdf')\n",
      "raw_latex_fig2_legend = get_raw_support_filepath('colormap.pdf')\n",
      "# raw_latex_fig3_legend = get_raw_support_filepath('...')\n",
      "\n",
      "# verify the data sources exist\n",
      "check_file(raw_ag_100nt_t_fp)\n",
      "check_file(raw_ag_100nt_m_fp)\n",
      "check_file(raw_ag_t_fp)\n",
      "check_file(raw_ag_m_fp)\n",
      "check_file(raw_pgp_100nt_t_fp)\n",
      "check_file(raw_pgp_100nt_m_fp)\n",
      "check_file(raw_hmp_100nt_t_fp)\n",
      "check_file(raw_hmp_100nt_m_fp)\n",
      "check_file(raw_gg_100nt_t_fp)\n",
      "check_file(raw_gg_100nt_t_fp)\n",
      "check_file(raw_latex_template)\n",
      "check_file(raw_latex_aglogo)\n",
      "check_file(raw_latex_fig1_legend)\n",
      "check_file(raw_latex_fig2_legend)\n",
      "# check_file(raw_latex_fig3_legend)\n",
      "\n",
      "# copy raw to working directory\n",
      "cp_raw(raw_ag_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_ag_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_ag_t_fp, working_dir)\n",
      "cp_raw(raw_ag_m_fp, working_dir)\n",
      "cp_raw(raw_pgp_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_pgp_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_hmp_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_hmp_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_gg_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_gg_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_latex_template, working_dir)\n",
      "cp_raw(raw_latex_aglogo, working_dir)\n",
      "cp_raw(raw_latex_fig1_legend, working_dir)\n",
      "cp_raw(raw_latex_fig2_legend, working_dir)\n",
      "# cp_raw(raw_latex_fig3_legend, working_dir)\n",
      "\n",
      "# setup working file paths (note, .biom files decompressed in the following cell\n",
      "ag_100nt_t_fp = get_path('AG_100nt.biom')\n",
      "ag_100nt_m_fp = get_path('AG_100nt.txt')\n",
      "ag_t_fp = get_path('AG.biom')\n",
      "ag_m_fp = get_path('AG.txt')\n",
      "\n",
      "pgp_100nt_t_fp = get_path('PGP_100nt.biom')\n",
      "pgp_100nt_m_fp = get_path('PGP_100nt.txt')\n",
      "\n",
      "hmp_100nt_t_fp = get_path('HMPv35_100nt.biom')\n",
      "hmp_100nt_m_fp = get_path('HMPv35_100nt.txt')\n",
      "\n",
      "gg_100nt_t_fp = get_path('GG_100nt.biom')\n",
      "gg_100nt_m_fp = get_path('GG_100nt.txt')\n",
      "\n",
      "template = get_path('module_2_template.tex')\n",
      "aglogo = get_path('aglogo.pdf')\n",
      "fig1_legend = get_path('figure_1_labels_two_rows.pdf')\n",
      "fig2_legned = get_path('colormap.pdf')\n",
      "# fig3_legend = get_path('...')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not all QIIME scripts support gzip'd BIOM tables, so lets uncompress if necessary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files = !ls $working_dir\n",
      "for f in files.grep(\"biom.gz$\"):\n",
      "    f = get_path(f)\n",
      "    !gunzip $f"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to massage the metadata to improve our ability to compare samples. Specifically, were going to: \n",
      "\n",
      "* simplify body sites into their major categories (e.g., transform \"forehead\" and \"skin of hand\" to just \"skin\")\n",
      "* simplify country codes (e.g., GAZ:Venezuela to Venezuela)\n",
      "* simplify the experiment title (e.g., American Gut Project to AGP)\n",
      "* create a hybrid field combining the experiment title with the body site"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a body site mapping:\n",
      "simple_matter_map = {\n",
      "        'feces':'FECAL',\n",
      "        'sebum':'SKIN',\n",
      "        'tongue':'ORAL',\n",
      "        'skin':'SKIN',\n",
      "        'mouth':'ORAL',\n",
      "        'gingiva':'ORAL',\n",
      "        'gingival epithelium':'ORAL',\n",
      "        'nares':'SKIN',\n",
      "        'skin of hand':'SKIN',\n",
      "        'hand':'SKIN',\n",
      "        'skin of head':'SKIN',\n",
      "        'hand skin':'SKIN',\n",
      "        'throat':'ORAL',\n",
      "        'auricular region zone of skin':'SKIN',\n",
      "        'mucosa of tongue':'ORAL',\n",
      "        'palatine tonsil':'ORAL',\n",
      "        'hard palate':'ORAL',\n",
      "        'saliva':'ORAL',\n",
      "        'stool':'FECAL',\n",
      "        'vagina':'SKIN',\n",
      "        'fossa':'SKIN',\n",
      "        'buccal mucosa':'ORAL',\n",
      "        'vaginal fornix':'SKIN',\n",
      "        }\n",
      "\n",
      "def massage_mapping(in_fp, out_fp, body_site_column_name, exp_acronym):\n",
      "    \"\"\"Simplify the mapping file for use in figures\n",
      "\n",
      "    body_site_column_name : specify the column name for body\n",
      "    exp_acronym : short name for the study\n",
      "\n",
      "    Returns False on failure, True on success\n",
      "    \"\"\"\n",
      "    age_cat_map = [(0,2,'Baby'),\n",
      "                   (2,13,'Child'),\n",
      "                   (13,20,'Teen'),\n",
      "                   (20,30,'20s'),\n",
      "                   (30,40,'30s'),\n",
      "                   (40,50,'40s'),\n",
      "                   (50,60,'50s'),\n",
      "                   (60,70,'60s'),\n",
      "                   (70,80,'70s'),\n",
      "                   (80,99999,'Older than 80')]\n",
      "    bmi_cat_map = [(0, 18.5,'Underweight'),\n",
      "                   (18.5, 25,'Normal'),\n",
      "                   (25, 30,'Overweight'),\n",
      "                   (30, 35,'Moderately obese'),\n",
      "                   (35, 40,'Severely obese'),\n",
      "                   (40, 99999,'Very severely obese')]\n",
      "    \n",
      "    mapping_lines = [l.strip().split('\\t') for l in open(in_fp)]\n",
      "    \n",
      "    header = mapping_lines[0]\n",
      "    header_low = map(lambda x: x.lower(), header)\n",
      "        \n",
      "    try:\n",
      "        bodysite_idx = header_low.index(body_site_column_name.lower())\n",
      "    except ValueError:\n",
      "        print \"Could not find '%s' in the mapping file header!\" % body_site_column_name\n",
      "        return False\n",
      "    \n",
      "    try:\n",
      "        country_idx = header_low.index('country')\n",
      "    except ValueError:\n",
      "        print \"Could not find the country in the mapping file header!\"\n",
      "        return False\n",
      "\n",
      "    # see if there is an age category\n",
      "    try:\n",
      "        age_idx = header_low.index('age')\n",
      "    except ValueError:\n",
      "        age_idx = None\n",
      "        \n",
      "    # see if there is a bmi category    \n",
      "    try:\n",
      "        bmi_idx = header_low.index('bmi')\n",
      "    except ValueError:\n",
      "        bmi_idx = None\n",
      "    \n",
      "    new_mapping_lines = [header[:]]\n",
      "    new_mapping_lines[0].append('SIMPLE_BODY_SITE')\n",
      "    new_mapping_lines[0].append('TITLE_ACRONYM')\n",
      "    new_mapping_lines[0].append('TITLE_BODY_SITE')\n",
      "    new_mapping_lines[0].append('HMP_SITE')\n",
      "\n",
      "    if age_idx is not None:\n",
      "        new_mapping_lines[0].append('AGE_CATEGORY')\n",
      "    if bmi_idx is not None:\n",
      "        new_mapping_lines[0].append('BMI_CATEGORY')\n",
      "    \n",
      "    for l in mapping_lines[1:]:\n",
      "        new_line = l[:]\n",
      "        body_site = new_line[bodysite_idx]\n",
      "        country = new_line[country_idx]\n",
      "        \n",
      "        # grab the body site\n",
      "        if body_site.startswith('UBERON_'):\n",
      "            body_site = body_site.split('_',1)[-1].replace(\"_\",\" \")\n",
      "        elif body_site.startswith('UBERON:'):\n",
      "            body_site = body_site.split(':',1)[-1]\n",
      "        elif body_site in ['NA', 'unknown']:\n",
      "            # controls, environmental, etc\n",
      "            continue\n",
      "        else:\n",
      "            print \"Unknown body site value: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            return False\n",
      "        \n",
      "        # remap the body site\n",
      "        if body_site.lower() not in simple_matter_map:\n",
      "            print \"Could not remap body site: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            return False\n",
      "        else:\n",
      "            body_site = simple_matter_map[body_site.lower()]\n",
      " \n",
      "        if exp_acronym == 'HMP':\n",
      "            hmp_site = 'HMP-%s' % body_site\n",
      "        else:\n",
      "            hmp_site = body_site            \n",
      "            \n",
      "        # simplify the country    \n",
      "        if country.startswith('GAZ:'):\n",
      "            country = country.split(':',1)[-1]\n",
      "        else:\n",
      "            print \"Could not parse country value: %s\" % country\n",
      "            return False\n",
      "        \n",
      "        if age_idx is not None:\n",
      "            age_cat = None\n",
      "            if new_line[age_idx] in ['NA','None']:\n",
      "                age_cat = 'Unknown'\n",
      "            else:\n",
      "                try:\n",
      "                    # PGP is currently in age ranges, ignoring those for now\n",
      "                    age = float(new_line[age_idx])\n",
      "                except ValueError:\n",
      "                    age_cat = 'Unknown'\n",
      "                    \n",
      "            if age_cat is not 'Unknown':    \n",
      "                for low,high,cat in age_cat_map:\n",
      "                    if low <= age < high:\n",
      "                        age_cat = cat\n",
      "                        break\n",
      "                if age_cat is None:\n",
      "                    print \"Count not map %f to an age category!\" % age\n",
      "                    return False\n",
      "            \n",
      "        if bmi_idx is not None:\n",
      "            if new_line[bmi_idx] in ['NA','']:\n",
      "                bmi_cat = 'Unknown'\n",
      "            else:\n",
      "                bmi = float(new_line[bmi_idx])\n",
      "                bmi_cat = None\n",
      "                for low,high,cat in bmi_cat_map:\n",
      "                    if low <= bmi < high:\n",
      "                        bmi_cat = cat\n",
      "                        break\n",
      "                if bmi_cat is None:\n",
      "                    print \"Count not map %f to a BMI category!\" % bmi\n",
      "                \n",
      "        new_line.append(body_site)\n",
      "        new_line.append(exp_acronym)\n",
      "        new_line.append(\"%s-%s\" % (exp_acronym, body_site))\n",
      "        new_line[country_idx] = country\n",
      "        new_line.append(hmp_site)\n",
      "        \n",
      "        if age_idx is not None:\n",
      "            new_line.append(age_cat)\n",
      "        \n",
      "        if bmi_idx is not None:\n",
      "            new_line.append(bmi_cat)\n",
      "            \n",
      "        new_mapping_lines.append(new_line)\n",
      "    \n",
      "    output = open(out_fp,'w')\n",
      "    output.write('\\n'.join(['\\t'.join(l) for l in new_mapping_lines]))\n",
      "    output.write('\\n')\n",
      "    output.close()\n",
      "    \n",
      "    return True\n",
      "\n",
      "def test_massage_mapping():\n",
      "    \"\"\"Exercise the massage mapping code, verify expected results\"\"\"\n",
      "    # output test file\n",
      "    ag_100nt_m_TEST_fp = get_path('AG_100nt_TEST.txt')\n",
      "    \n",
      "    # make sure massage_mapping fails with a bad column header\n",
      "    assert not massage_mapping(ag_100nt_m_fp, ag_100nt_m_TEST_fp, 'does not exist', 'AGP')\n",
      "    \n",
      "    # make sure massage mapping works with a good header\n",
      "    assert massage_mapping(ag_100nt_m_fp, ag_100nt_m_TEST_fp, 'body_site', 'AGP')\n",
      "    \n",
      "    # verify the resulting header structure\n",
      "    test_mapping = [l.strip().split('\\t') for l in open(ag_100nt_m_TEST_fp)]\n",
      "    test_header = test_mapping[0]\n",
      "    test_header_length = len(test_header)\n",
      "    assert test_header[-6:] == ['SIMPLE_BODY_SITE', 'TITLE_ACRONYM', 'TITLE_BODY_SITE', 'HMP_SITE', 'AGE_CATEGORY','BMI_CATEGORY']\n",
      "    \n",
      "    # verify each line in the test file\n",
      "    for l in test_mapping[1:]:\n",
      "        assert l[-6] in ['FECAL','SKIN','ORAL']\n",
      "        assert l[-5] == 'AGP'\n",
      "        acro, site = l[-4].split('-')\n",
      "        assert acro == 'AGP'\n",
      "        assert site in ['FECAL','SKIN','ORAL']\n",
      "        assert l[-3] in ['FECAL','SKIN','ORAL']\n",
      "        assert len(l) == test_header_length\n",
      "    \n",
      "    # missing explicit check on age category and bmi category. visual inspection:\n",
      "    # !head AG_100nt_TEST.txt | cut -f 11,53,168,169\n",
      "    # ...passes, sufficient for right now. \n",
      "\n",
      "### uncomment the next line to test the massage_mapping function\n",
      "#test_massage_mapping()\n",
      "\n",
      "# new file paths\n",
      "ag_100nt_m_massaged_fp = get_path('AG_100nt_massaged.txt')\n",
      "gg_100nt_m_massaged_fp = get_path('GG_100nt_massaged.txt')\n",
      "pgp_100nt_m_massaged_fp = get_path('PGP_100nt_massaged.txt')\n",
      "hmp_100nt_m_massaged_fp = get_path('HMP_100nt_massaged.txt')\n",
      "\n",
      "# massage\n",
      "assert massage_mapping(ag_100nt_m_fp, ag_100nt_m_massaged_fp, 'body_site', 'AGP')\n",
      "assert massage_mapping(gg_100nt_m_fp, gg_100nt_m_massaged_fp, 'body_site', 'GG')\n",
      "assert massage_mapping(pgp_100nt_m_fp, pgp_100nt_m_massaged_fp, 'body_site', 'PGP')\n",
      "assert massage_mapping(hmp_100nt_m_fp, hmp_100nt_m_massaged_fp, 'bodysite', 'HMP')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 255
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've massaged the metadata, we need to merge the mapping files from all the analyses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup output paths, (mm -> massaged mapping)\n",
      "hmp_pgp_mm_fp = get_path('HMP_PGP_100nt_massaged.txt')\n",
      "ag_gg_mm_fp = get_path('AG_GG_100nt_massaged.txt')\n",
      "hmp_pgp_ag_gg_mm_fp = get_path('HMP_GG_AG_PGP_100nt_massaged.txt')\n",
      "\n",
      "hmp_pgp_cmd_args = {'input_a':hmp_100nt_m_massaged_fp,\n",
      "                  'input_b':pgp_100nt_m_massaged_fp,\n",
      "                  'output':hmp_pgp_mm_fp}\n",
      "\n",
      "ag_gg_cmd_args = {'input_a':ag_100nt_m_massaged_fp,\n",
      "                  'input_b':gg_100nt_m_massaged_fp,\n",
      "                  'output':ag_gg_mm_fp}\n",
      "\n",
      "hmp_pgp_ag_gg_cmd_args = {'input_a':hmp_pgp_mm_fp,\n",
      "                          'input_b':ag_gg_mm_fp,\n",
      "                          'output':hmp_pgp_ag_gg_mm_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_job = submit(qiime_scripts['Merge Mapping Files'] % hmp_pgp_cmd_args)\n",
      "ag_gg_job = submit(qiime_scripts['Merge Mapping Files'] % ag_gg_cmd_args)\n",
      "jobs = wait_on([hmp_pgp_job, ag_gg_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_ag_gg_job = submit(qiime_scripts['Merge Mapping Files'] % hmp_pgp_ag_gg_cmd_args)\n",
      "jobs = wait_on(hmp_pgp_ag_gg_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  10 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 256
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to simplify compute on the downstream processes, we're going to filter out the metadata columns that we aren't interested in."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_mapping_file(in_fp, out_fp, columns_to_keep):\n",
      "    \"\"\"Filter out columns in a mapping file\n",
      "\n",
      "    in_fp : the input file path\n",
      "    out_fp : the output file path\n",
      "    columns_to_keep : a dict of the columns to keep, valued by specific category value\n",
      "        if desired to filter out samples that don't meet a given criteria\n",
      "    \"\"\"\n",
      "    lines = [l.strip().split('\\t') for l in open(in_fp)]\n",
      "    header = lines[0][:]\n",
      "    header_lower = map(lambda x: x.lower(), header)\n",
      "\n",
      "    # ensure SampleID is always first\n",
      "    new_header = [\"#SampleID\"]\n",
      "    indices = [0] # always keep SampleID\n",
      "    for c in columns_to_keep:\n",
      "        if c.lower() not in header_lower:\n",
      "            raise ValueError(\"Cannot find %s!\" % c)\n",
      "        \n",
      "        indices.append(header_lower.index(c.lower()))\n",
      "        new_header.append(c)\n",
      "    columns_to_keep['#SampleID'] = None # add for consistency\n",
      "    \n",
      "    new_lines = [new_header]\n",
      "    for l in lines[1:]:\n",
      "        new_line = []\n",
      "        \n",
      "        keep = True\n",
      "        # fetch values from specific columns\n",
      "        for column, index in zip(new_header, indices):\n",
      "            value = l[index] \n",
      "            if columns_to_keep[column] is None:\n",
      "                new_line.append(value)  \n",
      "            elif not columns_to_keep[column](value):\n",
      "                keep = False\n",
      "                break\n",
      "            else:\n",
      "                new_line.append(value)\n",
      "        \n",
      "        if keep:\n",
      "            new_lines.append(new_line)\n",
      "        \n",
      "    out = open(out_fp, 'w')\n",
      "    out.write('\\n'.join(['\\t'.join(l) for l in new_lines]))\n",
      "    out.write('\\n')\n",
      "    out.close()\n",
      "\n",
      "def test_filter_mapping_file():\n",
      "    # test output file\n",
      "    fmf_TEST_fp = get_path('AG_GG_fmf_TEST.txt')\n",
      "    \n",
      "    # filter to just fecal samples, keep the age and title_acronym columns\n",
      "    filter_mapping_file(ag_gg_mm_fp, fmf_TEST_fp, {'SIMPLE_BODY_SITE':lambda x: x == 'FECAL', 'AGE':None, 'TITLE_ACRONYM':None})\n",
      "    \n",
      "    # parse output\n",
      "    test_mapping = [l.strip().split('\\t') for l in open(fmf_TEST_fp)]\n",
      "    \n",
      "    # fish header, verify sanity of it\n",
      "    test_header = test_mapping[0]\n",
      "    assert len(test_header) == 4\n",
      "    assert test_header[0] == '#SampleID'\n",
      "    assert sorted(test_header) == sorted(['#SampleID','SIMPLE_BODY_SITE','AGE','TITLE_ACRONYM'])\n",
      "    \n",
      "    # check each record\n",
      "    test_sbs = test_header.index('SIMPLE_BODY_SITE')\n",
      "    test_acro = test_header.index('TITLE_ACRONYM')\n",
      "    for l in test_mapping[1:]:\n",
      "        assert len(l) == 4\n",
      "        assert l[test_sbs] == 'FECAL'\n",
      "        assert l[test_acro] in ['AGP','GG']   \n",
      "\n",
      "# uncomment the next line to test filter_mapping_file\n",
      "test_filter_mapping_file()\n",
      "fig1_m_fp = get_path('HMP_GG_AG_PGP_figure1.txt')\n",
      "fig2_m_fp = get_path('AG_GG_fecal_figure2.txt')\n",
      "fig3_m_fp = get_path('AG_fecal_figure3.txt')\n",
      "fig4_m_fp = get_path('AG_fecal_figure4.txt')\n",
      "\n",
      "filter_mapping_file(hmp_gg_ag_pgp_mm_fp, fig1_m_fp, {'TITLE_ACRONYM':None, 'SIMPLE_BODY_SITE':None, 'TITLE_BODY_SITE':None, 'HMP_SITE':None})\n",
      "filter_mapping_file(ag_gg_mm_fp, fig2_m_fp, {'TITLE_ACRONYM':None, 'AGE':lambda x: x not in ['None','NA','0.0'], 'SIMPLE_BODY_SITE':lambda x: x == 'FECAL'})\n",
      "filter_mapping_file(ag_100nt_m_massaged_fp, fig3_m_fp, {'TITLE_ACRONYM':None, 'SIMPLE_BODY_SITE':lambda x: x == 'FECAL'})\n",
      "filter_mapping_file(ag_100nt_m_massaged_fp, fig4_m_fp, {'TITLE_ACRONYM':None, 'AGE_CATEGORY':None, 'SEX':None, 'BMI_CATEGORY':None, 'DIET_TYPE':None, 'SIMPLE_BODY_SITE':lambda x: x == 'FECAL'})\n",
      "\n",
      "# what columns here?\n",
      "#filter_mapping_file(ag_m_fp, fig3_m_fp, {'SIMPLE_BODY_SITE':'FECAL', 'TITLE_ACRONYM':None, 'TITLE_BODY_SITE':None})\n",
      "### use everything right now for fig 3\n",
      "#raise ValueError, \"NEED TO RESOLVE METADATA FOR FIG 3\"\n",
      "\n",
      "#filter_mapping_file(ag_m_fp, fig5_m_fp, {'AGE':None, 'SIMPLE_BODY_SITE':'FECAL', 'BMI':None, 'DIET_TYPE':None, 'SEX':None})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 295
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the figures, we'll need to merge and filter the tables in a few ways. For figure 1, we want to place the American Gut population in the context of other large studies. To do so, we need to first merge the tables together. Since there are 4 tables to merge, we need to use two merge calls. (It is also feasible to use QIIME's parallel_merge_otu_tables.py here as well). Figure 2 is a combination of Global Gut and the American Gut, but only fecal samples, as is figure 3. Note that for figure 3, we're using the full American Gut table and not the 100 nucleotide version. Since this table is not being combined with the HiSeq data in the Global Gut, we can get away with retaining the full read for added specificity. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting paths\n",
      "hmp_pgp_t_fp = get_path(\"HMP_PGP_100nt.biom\")\n",
      "ag_gg_t_fp = get_path(\"AG_GG_100nt.biom\")\n",
      "hmp_gg_ag_pgp_t_fp = get_path(\"HMP_GG_AG_PGP_100nt.biom\")\n",
      "\n",
      "# setup the command arguments for each call\n",
      "hmp_pgp_cmd_args = {'input_a':hmp_100nt_t_fp, \n",
      "                   'input_b':pgp_100nt_t_fp,\n",
      "                   'output':hmp_pgp_t_fp}\n",
      "ag_gg_cmd_args = {'input_a':ag_100nt_t_fp,\n",
      "                   'input_b':gg_100nt_t_fp,\n",
      "                   'output':ag_gg_t_fp}\n",
      "hmp_gg_ag_pgp_cmd_args = {'input_a':ag_gg_t_fp,\n",
      "                          'input_b':hmp_pgp_t_fp,\n",
      "                          'output':hmp_gg_ag_pgp_t_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_pgp_cmd_args)\n",
      "ag_gg_job = submit(qiime_scripts['Merge OTU Tables'] % ag_gg_cmd_args)\n",
      "jobs = wait_on([hmp_pgp_job, ag_gg_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_ag_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_gg_ag_pgp_cmd_args)\n",
      "jobs = wait_on(hmp_gg_ag_pgp_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 2 jobs still running, approximately  195 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the data are combined, we need to rarify them in order to normalize for sequencing effort. The Human Microbiome Project has the shallowest coverage per sample, and when operating with the HMP dataset, a rarifaction depth of 1,000 is a reasonable balance between effort and retaining sufficient numbers of samples. The Global Gut and American Gut Project have much deeper coverage per sample (particularly in the case of the Global Gut) and as such, we will rarify at 10,000 sequences per sample for those tables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting path\n",
      "hmp_gg_ag_pgp_t_1k_fp = get_path(\"HMP_GG_AG_PGP_100nt_even1k.biom\")\n",
      "\n",
      "# setup the command arguments\n",
      "hmp_gg_ag_pgp_t_1k_cmd_args = {'input':hmp_gg_ag_pgp_t_fp,\n",
      "                               'output':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                               'depth':'1000'}\n",
      "\n",
      "# rarifiy and block until completion\n",
      "hmp_gg_ag_pgp_t_1k_job = submit(qiime_scripts['Single Rarifaction'] % hmp_gg_ag_pgp_t_1k_cmd_args)\n",
      "res = wait_on(hmp_gg_ag_pgp_t_1k_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  1370 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our rarified table, that combines the AG, PGP, HMP, and GG datasets, and a merged mapping file, we can now compute the beta diversity of this OTU table. This step is computationally demanding, and will run for a few hours on a 100 processors. **Note, the final merge job for parallel beta diversity requires > 8GB of RAM**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# verify we have the files we need to operate on\n",
      "check_file(hmp_gg_ag_pgp_t_1k_fp); check_file(fig1_m_fp)\n",
      "    \n",
      "# setup output directory\n",
      "bdiv_dir = lambda x: 'bdiv_' + x.basename(x).split('.',1)[0]\n",
      "hmp_gg_ag_pgp_1k_unweighted_unifrac_d = get_path(bdiv_dir(hmp_gg_ag_pgp_t_1k_fp))\n",
      " \n",
      "# setup beta diversity arguments\n",
      "prefix = 'ag2_bdiv_'\n",
      "hmp_gg_ag_pgp_cmd_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                          'output':hmp_gg_ag_pgp_1k_unweighted_unifrac_d,\n",
      "                          'job_prefix':prefix,\n",
      "                          'num_jobs':NUM_PROCS,\n",
      "                          'gg97_tree':greengenes135_97_tree_fp}\n",
      "\n",
      "# submit and wait\n",
      "hmp_gg_ag_pgp_bdiv_job = submit(qiime_scripts['Parallel Beta Diversity'] % hmp_gg_ag_pgp_cmd_args)\n",
      "res = wait_on(hmp_gg_ag_pgp_bdiv_job, additional_prefix=prefix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 2 jobs still running, approximately  95 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have calculated beta diversity for all of the samples, we need to filter the table down to the subsets that we're interested in. For figure 1 we want to use all of the samples. But, for figure 2 and 3, we only want to use subsets of the full distance matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####bdiv_path = lambda x: os.path.join(x, \"unweighted_unifrac_%s.txt\" % os.path.basename(x).split('_unweighted',1)[0])\n",
      "\n",
      "### actually determine the path here...\n",
      "bdiv_path = get_path('bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k.txt') \n",
      "check_file(bdiv_path)\n",
      "\n",
      "full_bdiv = bdiv_path\n",
      "fig_path = lambda x: full_bdiv.rsplit('.txt',1)[0] + '-' + x + '.txt'\n",
      "fig1_bdiv = fig_path('fig1')\n",
      "fig2_bdiv = fig_path('fig2')\n",
      "fig3_bdiv = fig_path('fig3')\n",
      "\n",
      "fig1_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig1_bdiv,\n",
      "                 'sample_ids':fig1_m_fp}\n",
      "fig2_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig2_bdiv,\n",
      "                 'sample_ids':fig2_m_fp}\n",
      "fig3_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig3_bdiv,\n",
      "                 'sample_ids':fig3_m_fp}\n",
      "\n",
      "jobs = []\n",
      "jobs.append(submit(qiime_scripts['Filter Distance Matrix'] % fig1_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Filter Distance Matrix'] % fig2_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Filter Distance Matrix'] % fig3_cmd_args))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  35 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 187
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our Unifrac distance matrices, need to transform them into principal coordinates space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pc_path = lambda x: x.rsplit('.txt',1)[0] + '_pc.txt'\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(fig1_bdiv)\n",
      "check_file(fig2_bdiv)\n",
      "check_file(fig3_bdiv)\n",
      "\n",
      "fig1_pc = pc_path(fig1_bdiv)\n",
      "fig2_pc = pc_path(fig2_bdiv)\n",
      "fig3_pc = pc_path(fig3_bdiv)\n",
      "\n",
      "# setup our arguments\n",
      "fig1_cmd_args = {'input':fig1_bdiv,\n",
      "                 'output':fig1_pc}\n",
      "fig2_cmd_args = {'input':fig2_bdiv,\n",
      "                 'output':fig2_pc}\n",
      "fig3_cmd_args = {'input':fig3_bdiv,\n",
      "                 'output':fig3_pc}\n",
      "\n",
      "# submit the jobs\n",
      "jobs = []\n",
      "jobs.append(submit(qiime_scripts['Principal Coordinates'] % fig1_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Principal Coordinates'] % fig2_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Principal Coordinates'] % fig3_cmd_args))\n",
      "job_results = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  15 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 188
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now to actually make figures 1, 2, and 3 using [Emperor](http://qiime.org/emperor/), a WebGL-based Principal Coordinates viewer. While Emperor is the only tool that we're aware of that can effectively scale to these size datasets for 3D visualization and painting of arbitrary metadata, the tie to WebGL makes its use here a little bit of a challenge. Specifically, we'll be able to generate the plots, but we cannot automatically generate the images from the notebook. First, lets get Emperor up and running, in the following cell, we'll describe how what needs to happen to produce the images."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# quick little helper method\n",
      "emp_path = lambda x: x.rsplit('.txt',1)[0] + '-emp'\n",
      "\n",
      "# verify expected files are present\n",
      "check_file(fig1_pc)\n",
      "check_file(fig2_pc)\n",
      "check_file(fig3_pc)\n",
      "\n",
      "# setup output paths\n",
      "fig1_emp = emp_path(fig1_pc)\n",
      "fig2_emp = emp_path(fig2_pc)\n",
      "fig3_emp = emp_path(fig3_pc)\n",
      "\n",
      "# setup arguments\n",
      "fig1_cmd_args = {'input':fig1_pc, \n",
      "                 'output':fig1_emp, \n",
      "                 'mapping':fig1_m_fp}\n",
      "fig2_cmd_args = {'input':fig2_pc, \n",
      "                 'output':fig2_emp, \n",
      "                 'mapping':fig2_m_fp}\n",
      "fig3_cmd_args = {'input':fig3_pc, \n",
      "                 'output':fig3_emp, \n",
      "                 'mapping':fig3_m_fp}\n",
      "\n",
      "jobs = []\n",
      "jobs.append(submit(other_scripts['Make Emperor'] % fig1_cmd_args))\n",
      "jobs.append(submit(other_scripts['Make Emperor'] % fig2_cmd_args))\n",
      "jobs.append(submit(other_scripts['Make Emperor'] % fig3_cmd_args))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  65 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 263
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's the required manual intervention:\n",
      "\n",
      "1. Click on the first URL link in the resulting pane\n",
      "2. Rotate the view to your a nice perspective\n",
      "3. Click the \"Options\" tab on the right side, set a filename prefix (e.g., figure1) and then click on the \"Multishot\" button. **NOTE: this will produce a lot of files on to your LOCAL computer!**\n",
      "4. Wait until all of the images have downloaded\n",
      "5. Pack up the local image files:\n",
      "  - Change directories on your LOCAL computer to your downloads folder\n",
      "  - Execute (make sure to replace **prefix** with the prefix specified in 3): tar czf **prefix**.tar.gz **prefix**.*\n",
      "6. Copy the **prefix**.tar.gz file back to your compute resource, and place it in your home directory (~/ or $HOME)\n",
      "7. repeat for each figure\n",
      "\n",
      "If the figures are redone, then you may need to clear your web-browsers cache befone the HTML links below will work correct. Optionally, you could hit refresh a few times after clicking the links as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.lib.display import FileLink\n",
      "\n",
      "emp_index = lambda x: os.path.join(x, 'index.html')\n",
      "\n",
      "# form the expected paths for Emperor\n",
      "fig1 = emp_index(fig1_emp)\n",
      "fig2 = emp_index(fig2_emp)\n",
      "fig3 = emp_index(fig3_emp)\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(fig1)\n",
      "check_file(fig2)\n",
      "check_file(fig3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 264
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig1, result_html_prefix='<p>Figure 1:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p>Figure 1:</p><a href='files//home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig1_pc-emp/index.html' target='_blank'>/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig1_pc-emp/index.html</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 260,
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig1_pc-emp/index.html"
       ]
      }
     ],
     "prompt_number": 260
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig2, result_html_prefix='<p>Figure 2:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p>Figure 2:</p><a href='files//home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig2_pc-emp/index.html' target='_blank'>/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig2_pc-emp/index.html</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 201,
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig2_pc-emp/index.html"
       ]
      }
     ],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig3, result_html_prefix='<p>Figure 3:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p>Figure 3:</p><a href='files//home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig3_pc-emp/index.html' target='_blank'>/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig3_pc-emp/index.html</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 198,
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig3_pc-emp/index.html"
       ]
      }
     ],
     "prompt_number": 198
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the images are created, we need to copy them to our working area. It is possible file paths will be different, in which case, you may need to change the DOWNLOAD_DIRECTORY variable in the next cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DOWNLOAD_DIRECTORY = os.path.expandvars('$HOME')\n",
      "FIGURE1_EXPECTED_FILENAME = 'Figure_1.tar.gz'\n",
      "FIGURE2_EXPECTED_FILENAME = 'Figure_2.tar.gz'\n",
      "FIGURE3_EXPECTED_FILENAME = 'Figure_3.tar.gz'\n",
      "\n",
      "# helper function for creating wildcard paths\n",
      "source_path = lambda x,y: os.path.join(x,y)\n",
      "\n",
      "# setup the destination paths\n",
      "emperor_images = get_path('emperor_images_svg')\n",
      "\n",
      "if not os.path.exists(emperor_images):\n",
      "    os.mkdir(emperor_images)\n",
      "\n",
      "# setup the source paths\n",
      "figure1_src = source_path(DOWNLOAD_DIRECTORY, FIGURE1_EXPECTED_FILENAME)\n",
      "figure2_src = source_path(DOWNLOAD_DIRECTORY, FIGURE2_EXPECTED_FILENAME)\n",
      "figure3_src = source_path(DOWNLOAD_DIRECTORY, FIGURE3_EXPECTED_FILENAME)\n",
      "\n",
      "check_file(figure1_src)\n",
      "check_file(figure2_src)\n",
      "check_file(figure3_src)\n",
      "\n",
      "# unpack the tarballs\n",
      "jobs = []\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure1_src, emperor_images)))\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure2_src, emperor_images)))\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure3_src, emperor_images)))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 3 jobs still running, approximately  700 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 265
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to produce the actual PDF files of the PCoA plots that will go into the individualized results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_ag_IDs = set([l.strip().split('\\t')[0] for l in open(ag_m_fp) if not l.startswith('#')])\n",
      "all_emp_SVGs = !ls $emperor_images\n",
      "template_files = get_path('template_files')\n",
      "\n",
      "if not os.path.exists(template_files):\n",
      "    os.mkdir(template_files)\n",
      "\n",
      "svg_smash_args = {'input':emperor_images, \n",
      "                  'output':template_files, \n",
      "                  'prefix':None, \n",
      "                  'sample_id':None}\n",
      "\n",
      "commands = []\n",
      "for f in all_emp_SVGs:\n",
      "    prefix, remainder = f.split('.',1)\n",
      "    \n",
      "    try:\n",
      "        id_, remainder = remainder.rsplit('_',1)\n",
      "    except:\n",
      "        # GLOBAL SVG for each figure\n",
      "        assert remainder == 'GLOBAL'\n",
      "        continue\n",
      "        \n",
      "    # delete svgs for non-AG points    \n",
      "    if id_ not in all_ag_IDs:\n",
      "        #!rm $f \n",
      "        continue\n",
      "        \n",
      "    args = svg_smash_args.copy()\n",
      "    args['sample_id'] = id_\n",
      "    args['prefix'] = prefix\n",
      "    commands.append(other_scripts['SVG Smash'] % args)\n",
      "\n",
      "# submit SVG->PDF processing in batches of chunk_size\n",
      "chunk_size = 50\n",
      "start = 0\n",
      "jobs = []\n",
      "for end in range(chunk_size, len(commands) + chunk_size, chunk_size):\n",
      "    chunk = commands[start:end]\n",
      "    start = end\n",
      "    jobs.append(submit(chunk)) \n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 48 jobs still running, approximately  1830 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 266,
       "text": [
        "{('ag_mod2_l2E', '500922'),\n",
        " ('ag_mod2_l2E', '500923'),\n",
        " ('ag_mod2_l2E', '500924'),\n",
        " ('ag_mod2_l2E', '500925'),\n",
        " ('ag_mod2_l2E', '500926'),\n",
        " ('ag_mod2_l2E', '500927'),\n",
        " ('ag_mod2_l2E', '500928'),\n",
        " ('ag_mod2_l2E', '500929'),\n",
        " ('ag_mod2_l2E', '500930'),\n",
        " ('ag_mod2_l2E', '500931'),\n",
        " ('ag_mod2_l2E', '500932'),\n",
        " ('ag_mod2_l2E', '500933'),\n",
        " ('ag_mod2_l2E', '500934'),\n",
        " ('ag_mod2_l2E', '500935'),\n",
        " ('ag_mod2_l2E', '500936'),\n",
        " ('ag_mod2_l2E', '500937'),\n",
        " ('ag_mod2_l2E', '500938'),\n",
        " ('ag_mod2_l2E', '500939'),\n",
        " ('ag_mod2_l2E', '500940'),\n",
        " ('ag_mod2_l2E', '500941'),\n",
        " ('ag_mod2_l2E', '500942'),\n",
        " ('ag_mod2_l2E', '500943'),\n",
        " ('ag_mod2_l2E', '500944'),\n",
        " ('ag_mod2_l2E', '500945'),\n",
        " ('ag_mod2_l2E', '500946'),\n",
        " ('ag_mod2_l2E', '500947'),\n",
        " ('ag_mod2_l2E', '500948'),\n",
        " ('ag_mod2_l2E', '500949'),\n",
        " ('ag_mod2_l2E', '500950'),\n",
        " ('ag_mod2_l2E', '500951'),\n",
        " ('ag_mod2_l2E', '500952'),\n",
        " ('ag_mod2_l2E', '500953'),\n",
        " ('ag_mod2_l2E', '500954'),\n",
        " ('ag_mod2_l2E', '500955'),\n",
        " ('ag_mod2_l2E', '500956'),\n",
        " ('ag_mod2_l2E', '500957'),\n",
        " ('ag_mod2_l2E', '500958'),\n",
        " ('ag_mod2_l2E', '500959'),\n",
        " ('ag_mod2_l2E', '500960'),\n",
        " ('ag_mod2_l2E', '500961'),\n",
        " ('ag_mod2_l2E', '500962'),\n",
        " ('ag_mod2_l2E', '500963'),\n",
        " ('ag_mod2_l2E', '500964'),\n",
        " ('ag_mod2_l2E', '500965'),\n",
        " ('ag_mod2_l2E', '500966'),\n",
        " ('ag_mod2_l2E', '500967'),\n",
        " ('ag_mod2_l2E', '500968'),\n",
        " ('ag_mod2_l2E', '500969')}"
       ]
      }
     ],
     "prompt_number": 266
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets create the phylum level taxonomy summary plots!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "###\n",
      "###\n",
      "# sex categories should be \"Males\" and \"Females\"\n",
      "# age categories should be \"Your age group: 40s\"\n",
      "# bmi categories should be \"Your BMI group: normal\"\n",
      "###\n",
      "###\n",
      "###\n",
      "\n",
      "### summarize over the categories provided by justine\n",
      "fig4_sex_fp = get_path('fig4_sex.biom')\n",
      "fig4_age_fp = get_path('fig4_age_category.biom')\n",
      "fig4_diet_fp = get_path('fig4_diet.biom')\n",
      "fig4_bmi_fp = get_path('fig4_bmi_category.biom')\n",
      "ag_fecal_t_1k_fp = get_path('ag_fecal_even1k.biom')\n",
      "template_files = get_path('template_files')\n",
      "\n",
      "check_file(hmp_gg_ag_pgp_t_1k_fp)\n",
      "check_file(fig4_m_fp)\n",
      "\n",
      "filter_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "               'output':ag_fecal_t_1k_fp,\n",
      "               'sample_id_fp':fig4_m_fp}\n",
      "\n",
      "otu_by_cat_sex_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_sex_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'SEX'}\n",
      "otu_by_cat_age_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_age_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'AGE_CATEGORY'}\n",
      "otu_by_cat_diet_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_diet_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'DIET_TYPE'}\n",
      "otu_by_cat_bmi_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_bmi_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'BMI_CATEGORY'}\n",
      "\n",
      "phyla_plot_args = {'input':ag_fecal_t_1k_fp,\n",
      "                   'output':template_files,\n",
      "                   'mapping':fig4_m_fp,\n",
      "                   'categories':'SEX:%s, AGE_CATEGORY:%s, DIET_TYPE:%s, BMI_CATEGORY:%s' % (fig4_sex_fp, fig4_age_fp, fig4_diet_fp, fig4_bmi_fp)}\n",
      "                \n",
      "# filter the ag table down to just the fecal samples for fig 4\n",
      "filter_job = submit(qiime_scripts['Filter Samples'] % filter_args)\n",
      "res = wait_on(filter_job)\n",
      "\n",
      "# get the summarized tables\n",
      "jobs = []\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_sex_args))\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_age_args))\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_diet_args))\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_bmi_args))\n",
      "res = wait_on(jobs)\n",
      "\n",
      "# farm out the phyla plots\n",
      "chunk_size = 25\n",
      "start = 0\n",
      "jobs = []\n",
      "sample_ids = [i.strip().split('\\t')[0] for i in open(fig4_m_fp) if not i.startswith('#')]\n",
      "for end in range(chunk_size, len(sample_ids) + chunk_size, chunk_size):\n",
      "    chunk = sample_ids[start:end]\n",
      "    start = end\n",
      "    args = phyla_plot_args.copy()\n",
      "    args['samples'] = ','.join(chunk)\n",
      "    jobs.append(submit(other_scripts['Make Phyla Plots'] % args)) \n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 33 jobs still running, approximately  100 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 271
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And for the last figure, lets create the over represented taxonomy table!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig5_tax_sum = get_path('ag_fecal_even1k_taxasum_L6')\n",
      "fig5_lvl6 = get_path('ag_fecal_even1k_taxasum_L6/ag_fecal_even1k_L6.txt')\n",
      "template_files = get_path('template_files')\n",
      "\n",
      "check_file(ag_fecal_t_1k_fp)\n",
      "\n",
      "sum_args = {'input':ag_fecal_t_1k_fp,\n",
      "            'output':fig5_tax_sum,\n",
      "            'level':\"6\"}\n",
      "sig_args = {'input':fig5_lvl6,\n",
      "            'output':template_files}\n",
      "\n",
      "sum_job = submit(qiime_scripts['Summarize Taxa'] % sum_args)\n",
      "res = wait_on(sum_job)\n",
      "\n",
      "check_file(fig5_lvl6)\n",
      "\n",
      "sig_job = submit(other_scripts['OTU Significance'] % sig_args) \n",
      "res = wait_on(sig_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  125 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 272
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TEMPLATES!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_file(ag_100nt_m_massaged_fp)\n",
      "check_file(template)\n",
      " \n",
      "create_titles_args = {'mapping':ag_100nt_m_massaged_fp}\n",
      "\n",
      "format_template_args = {'template':template, \n",
      "                        'keys_for_replace':'__PCOA1__,__PCOA2__,__PCOA3__,__TAXA_SUMMARY__',\n",
      "                        'values_for_replace':None,\n",
      "                        'keys_for_insert':'__COUNTRY__,__TAXA_TABLE__,__RARE_TAXA__',\n",
      "                        'values_for_insert':None,\n",
      "                        'output':None}\n",
      "\n",
      "titles_job = submit(other_scripts['Create Titles'] % create_titles_args)\n",
      "res = wait_on(titles_job)\n",
      "\n",
      "template_files_path = lambda x: os.path.join(template_files, x)\n",
      "final_path = lambda x,y: os.path.join(get_path(x), y)\n",
      "\n",
      "format_commands = []\n",
      "latex_commands = []\n",
      "ids = [l.strip().split('\\t')[0] for l in open(ag_100nt_m_massaged_fp) if not l.startswith('#')]\n",
      "for i in ids:\n",
      "    fig1_src = template_files_path(\"Figure_1.%s_huge.pdf\" % i)\n",
      "    fig2_src = template_files_path(\"Figure_2.%s_huge.pdf\" % i)\n",
      "    fig3_src = template_files_path(\"Figure_3.%s_huge.pdf\" % i)\n",
      "    fig4_src = template_files_path(\"Figure_4_%s.pdf\" % i)\n",
      "    table_src = template_files_path(\"Table_%s.txt\" % i)\n",
      "    list_src = template_files_path(\"List_%s.txt\" % i)\n",
      "                  \n",
      "    fig1_dst = final_path(i, \"Figure_1.%s_huge.pdf\" % i)\n",
      "    fig2_dst = final_path(i, \"Figure_2.%s_huge.pdf\" % i)\n",
      "    fig3_dst = final_path(i, \"Figure_3.%s_huge.pdf\" % i)\n",
      "    fig4_dst = final_path(i, \"Figure_4_%s.pdf\" % i)\n",
      "    table_dst = final_path(i, \"Table_%s.txt\" % i)\n",
      "    list_dst = final_path(i, \"List_%s.txt\" % i)\n",
      "    \n",
      "    populated_template = final_path(i, i.split('.')[0] + '.tex')\n",
      "    \n",
      "    if not os.path.exists(fig1_src): \n",
      "        print \"Missing fig1 for %s\" % i\n",
      "        continue\n",
      "    if not os.path.exists(fig2_src): \n",
      "        print \"Missing fig2 for %s\" % i\n",
      "        continue\n",
      "    if not os.path.exists(fig3_src): \n",
      "        print \"Missing fig3 for %s\" % i\n",
      "        continue\n",
      "    if not os.path.exists(fig4_src): \n",
      "        print \"Missing fig4 for %s\" % i\n",
      "        continue\n",
      "    if not os.path.exists(table_src): \n",
      "        print \"Missing table for %s\" % i\n",
      "        continue\n",
      "    if not os.path.exists(list_src): \n",
      "        print \"Missing list for %s\" % i\n",
      "        continue\n",
      "\n",
      "    # verify title.txt\n",
      "    title = final_path(i, \"title.txt\")\n",
      "    if not os.path.exists(title):\n",
      "        print \"Missing title for %s\" % i\n",
      "        continue\n",
      "        \n",
      "    !mv $fig1_src $fig1_dst\n",
      "    !mv $fig2_src $fig2_dst\n",
      "    !mv $fig3_src $fig3_dst\n",
      "    !mv $fig4_src $fig4_dst\n",
      "    !mv $table_src $table_dst\n",
      "    !mv $list_src $list_dst\n",
      "          \n",
      "    ### files are getting moved\n",
      "    ### need to now create the populated templates.... i think that's it\n",
      "    \n",
      "    strip_ext = lambda x: os.path.splitext(x)[0]\n",
      "    \n",
      "    args = format_template_args.copy()\n",
      "    args['values_for_replace'] = ','.join(map(strip_ext, [fig1_dst, fig2_dst, fig3_dst, fig4_dst]))\n",
      "    args['values_for_insert'] = ','.join([title, table_dst, list_dst])\n",
      "    args['output'] = populated_template\n",
      "    \n",
      "    format_commands.append(other_scripts['Format Template'] % args)\n",
      "    latex_commands.append(other_scripts['To PDF'] % {'input':populated_template})  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  45 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001256.1075816\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001548.1075923\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002472.1076062\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001214.1076070\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001432.1076127\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001109.1075852\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002476.1053817\n",
        "Missing fig1 for 000003441.1053724\n",
        "Missing fig2 for 000005429.1053715\n",
        "Missing fig2 for 000005292.1053943\n",
        "Missing fig2 for 000003167.1053875\n",
        "Missing fig2 for 000005578.1053814\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004924.1053834\n",
        "Missing fig2 for 000005420.1053848\n",
        "Missing fig2 for 000003080.1053829\n",
        "Missing fig2 for 000005160.1053828\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001787.1053922\n",
        "Missing fig2 for 000005359.1053955\n",
        "Missing fig2 for 000003298.1053819\n",
        "Missing fig2 for 000003104.1053741\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002872.1053856\n",
        "Missing fig2 for 000002794.1053852\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004689.1053716\n",
        "Missing fig2 for 000002303.1053732\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002304.1075713\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007129.1075665\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007093.1075683\n",
        "Missing fig2 for 000007098.1075695\n",
        "Missing fig2 for 000007134.1075662\n",
        "Missing fig2 for 000007089.1075709\n",
        "Missing fig2 for 000007107.1075698\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002825.1076132\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000005263.1076371\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001400.1076345\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004980.1076442\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004633.1076440\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004780.1076163\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002906.1076335\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001238.1076317\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002237.1075757\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001555.1076056\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001611.1076072\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001303.1075744\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001411.1075848\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001008.1075990\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001770.1075955\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005260.1053749\n",
        "Missing fig2 for 000005262.1053760\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001933.1053837\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005634.1053886\n",
        "Missing fig2 for 000005291.1053887\n",
        "Missing fig2 for 000005289.1053933\n",
        "Missing fig2 for 000005065.1053744\n",
        "Missing fig2 for 000004986.1053911\n",
        "Missing fig2 for 000004984.1053908\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000003031.1053939\n",
        "Missing fig2 for 000005419.1053903\n",
        "Missing fig2 for 000003118.1053727\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004012.1053757\n",
        "Missing fig2 for 000004865.1053870\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003652.1053801\n",
        "Missing fig2 for 000003950.1053802\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000003534.1075829\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001852.1075839\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001795.1076010\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001154.1076055\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007117.1075649\n",
        "Missing fig2 for 000007097.1075666\n",
        "Missing fig2 for 000007090.1075704\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001769.1076133\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001635.1076140\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001631.1076423\n",
        "Missing fig2 for 000004697.1076252\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004711.1076148\n",
        "Missing fig1 for 000003136.1076362\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001593.1076013\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002631.1075741\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002944.1075933\n",
        "Missing fig2 for 000005081.1053770\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002581.1053733\n",
        "Missing fig2 for 000002608.1053896\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005236.1053902\n",
        "Missing fig2 for 000005084.1053897\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003029.1053755\n",
        "Missing fig1 for 000002778.1053899\n",
        "Missing fig2 for 000003601.1053807\n",
        "Missing fig1 for 000003079.1053831\n",
        "Missing fig2 for 000005071.1053849\n",
        "Missing fig2 for 000002383.1053906\n",
        "Missing fig2 for 000005361.1053822\n",
        "Missing fig2 for 000005363.1053937\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003299.1053769\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005572.1053791\n",
        "Missing fig2 for 000005573.1053893\n",
        "Missing fig2 for 000003443.1053752\n",
        "Missing fig2 for 000005221.1053844\n",
        "Missing fig2 for 000005222.1053872\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003367.1053954\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002088.1053900\n",
        "Missing fig2 for 000003421.1053773\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007109.1075688\n",
        "Missing fig2 for 000007113.1075702\n",
        "Missing fig2 for 000007123.1075697\n",
        "Missing fig2 for 000007084.1075710\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007096.1075707\n",
        "Missing fig2 for 000007103.1075699\n",
        "Missing fig2 for 000007091.1075672\n",
        "Missing fig2 for 000007132.1075653\n",
        "Missing fig2 for 000007088.1075655\n",
        "Missing fig2 for 000002777.1076105\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002632.1075849\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002733.1076457\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002301.1076386\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004662.1076254\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004144.1076152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001616.1076171\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004726.1076189\n",
        "Missing fig2 for 000005780.1076196\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001912.1076194\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000005755.1076260\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002109.1076061\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005901.1053781\n",
        "Missing fig2 for 000001970.1053948\n",
        "Missing fig2 for 000005637.1053909\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005636.1053788\n",
        "Missing fig2 for 000005635.1053833\n",
        "Missing fig2 for 000005432.1053792\n",
        "Missing fig2 for 000005237.1053927\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002820.1053783\n",
        "Missing fig2 for 000003320.1053766\n",
        "Missing fig2 for 000004925.1053790\n",
        "Missing fig2 for 000005645.1053736\n",
        "Missing fig2 for 000005646.1053778\n",
        "Missing fig2 for 000005644.1053889\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005813.1053868\n",
        "Missing fig2 for 000005556.1053713\n",
        "Missing fig2 for 000003604.1053944\n",
        "Missing fig2 for 000005421.1053753\n",
        "Missing fig2 for 000002390.1053737\n",
        "Missing fig2 for 000004237.1053855\n",
        "Missing fig2 for 000004239.1053920\n",
        "Missing fig1 for 000006114.1053803\n",
        "Missing fig2 for 000002610.1053895\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002641.1053853\n",
        "Missing fig1 for 000002160.1053942\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003058.1075837\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001060.1076121\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007100.1075656\n",
        "Missing fig2 for 000007128.1075670\n",
        "Missing fig2 for 000007115.1075661\n",
        "Missing fig2 for 000007105.1075650\n",
        "Missing fig2 for 000007078.1075691\n",
        "Missing fig2 for 000007081.1075648\n",
        "Missing fig2 for 000007635.1075669\n",
        "Missing fig2 for 000007082.1075658\n",
        "Missing fig1 for 000002897.1075721\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001265.1075808\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001110.1076116\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001842.1076007\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001590.1076320\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004679.1076461\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004916.1076313\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000003603.1076479\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000003894.1076311\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002443.1075906\n",
        "Missing fig2 for 000001344.1075861\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001354.1075844\n",
        "Missing fig2 for 000002147.1075772\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002315.1075719\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002296.1053926\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003087.1053787\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005656.1053921\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005290.1053869\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005066.1053797\n",
        "Missing fig2 for 000005067.1053894\n",
        "Missing fig2 for 000002504.1053838\n",
        "Missing fig2 for 000005812.1053866\n",
        "Missing fig2 for 000005555.1053864\n",
        "Missing fig2 for 000002212.1053780\n",
        "Missing fig2 for 000005164.1053861\n",
        "Missing fig2 for 000005167.1053923\n",
        "Missing fig2 for 000005070.1053756\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002162.1053707\n",
        "Missing fig2 for 000003638.1053935\n",
        "Missing fig2 for 000005570.1053761\n",
        "Missing fig2 for 000005569.1053771\n",
        "Missing fig2 for 000005652.1053862\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000003557.1053914\n",
        "Missing fig2 for 000003701.1053717\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002316.1053925\n",
        "Missing fig2 for 000002854.1053775\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007131.1075703\n",
        "Missing fig2 for 000007112.1075685\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007122.1075681\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007102.1075673\n",
        "Missing fig2 for 000007079.1075687\n",
        "Missing fig2 for 000007135.1075674\n",
        "Missing fig2 for 000007133.1075671\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002826.1075879\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002859.1075875\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004807.1076183\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001483.1076445\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001808.1075826\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002391.1075924\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002481.1076124\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000006819.1053765\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002438.1053759\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005088.1053710\n",
        "Missing fig2 for 000005654.1053843\n",
        "Missing fig2 for 000003321.1053721\n",
        "Missing fig2 for 000003030.1053876\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000003224.1053847\n",
        "Missing fig2 for 000003223.1053824\n",
        "Missing fig2 for 000005159.1053709\n",
        "Missing fig2 for 000005161.1053863\n",
        "Missing fig2 for 000004236.1053845\n",
        "Missing fig2 for 000005069.1053832\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004846.1053763\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005650.1053881\n",
        "Missing fig2 for 000005651.1053932\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004953.1053883\n",
        "Missing fig2 for 000002395.1053907\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001713.1076135\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007110.1075668\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007124.1075694\n",
        "Missing fig2 for 000007092.1075654\n",
        "Missing fig2 for 000007076.1075693\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001130.1076092\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001545.1076110\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002893.1075782\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001269.1075870\n",
        "Missing fig2 for 000002575.1075824\n",
        "Missing fig1 for 000001582.1076447\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004272.1076219\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002023.1076356\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005840.1076354\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004695.1076294\n",
        "Missing fig2 for 000001725.1076340\n",
        "Missing fig1 for 000004891.1076474\n",
        "Missing fig1 for 000002288.1076201\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001228.1076081\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002148.1075974\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001916.1075888\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001538.1075817\n",
        "Missing fig2 for 000002482.1075920\n",
        "Missing fig1 for 000001138.1076057\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002388.1053746\n",
        "Missing fig1 for 000002688.1053918\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003085.1053772\n",
        "Missing fig2 for 000003385.1053884\n",
        "Missing fig2 for 000005430.1053818\n",
        "Missing fig2 for 000005086.1053839\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005658.1053785\n",
        "Missing fig2 for 000003166.1053779\n",
        "Missing fig2 for 000005574.1053806\n",
        "Missing fig2 for 000004873.1053915\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005842.1053815\n",
        "Missing fig1 for 000005557.1053776\n",
        "Missing fig2 for 000005558.1053742\n",
        "Missing fig2 for 000003332.1053719\n",
        "Missing fig2 for 000001986.1053795\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002320.1053946\n",
        "Missing fig2 for 000001164.1053874\n",
        "Missing fig2 for 000005220.1053905\n",
        "Missing fig2 for 000005649.1053941\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002178.1053767\n",
        "Missing fig2 for 000002940.1053851\n",
        "Missing fig2 for 000003422.1053904\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000005568.1053945\n",
        "Missing fig2 for 000004206.1053725\n",
        "Missing fig2 for 000002684.1053762\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001645.1075976\n",
        "Missing fig1 for 000001419.1075726\n",
        "Missing fig2 for 000002894.1076053\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001270.1075957\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007130.1075652\n",
        "Missing fig2 for 000007121.1075675\n",
        "Missing fig2 for 000007116.1075692\n",
        "Missing fig2 for 000007101.1075664\n",
        "Missing fig2 for 000007094.1075684\n",
        "Missing fig2 for 000007125.1075689\n",
        "Missing fig2 for 000007095.1075663\n",
        "Missing fig2 for 000007080.1075651\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007126.1075700\n",
        "Missing fig2 for 000002871.1076038\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002576.1075912\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002042.1076023\n",
        "Missing fig2 for 000001632.1075874\n",
        "Missing fig1 for 000002733.1076465\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004859.1076231\n",
        "Missing fig2 for 000002784.1076182\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000004833.1076385\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003438.1076250\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001244.1076467\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001662.1076019\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001583.1076093\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001173.1075932\n",
        "Missing fig1 for 000001888.1075983\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000005261.1053774\n",
        "Missing fig1 for 000001600.1053953\n",
        "Missing fig2 for 000005079.1053800\n",
        "Missing fig2 for 000005083.1053748\n",
        "Missing fig1 for 000005892.1053892\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002505.1053888\n",
        "Missing fig2 for 000002626.1053916\n",
        "Missing fig2 for 000005431.1053798\n",
        "Missing fig2 for 000005235.1053867\n",
        "Missing fig1 for 000002370.1053809\n",
        "Missing fig1 for 000001232.1053811\n",
        "Missing fig2 for 000005657.1053913\n",
        "Missing fig2 for 000005575.1053882\n",
        "Missing fig2 for 000005166.1053826\n",
        "Missing fig2 for 000005165.1053859\n",
        "Missing fig2 for 000005162.1053810\n",
        "Missing fig2 for 000004238.1053808\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003639.1053794\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000003025.1053740\n",
        "Missing fig2 for 000003105.1053865\n",
        "Missing fig2 for 000005219.1053934\n",
        "Missing fig2 for 000003535.1053768\n",
        "Missing fig2 for 000002413.1053764\n",
        "Missing fig2 for 000003702.1053846\n",
        "Missing fig2 for 000002642.1053735\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000005935.1053827\n",
        "Missing fig2 for 000003176.1053835\n",
        "Missing fig2 for 000003339.1053754\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002735.1075781\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001417.1075862\n",
        "Missing fig2 for 000001860.1076085\n",
        "Missing fig2 for 000001649.1075973\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000002605.1076071\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000007119.1075659\n",
        "Missing fig2 for 000007087.1075708\n",
        "Missing fig2 for 000007086.1075686\n",
        "Missing fig2 for 000007085.1075696\n",
        "Missing fig2 for 000007106.1075667\n",
        "Missing fig2 for 000007099.1075701\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001735.1076099\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig2 for 000001745.1075778\n",
        "Missing fig2 for 000001556.1076063\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001851.1076307\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000001156.1076178\n",
        "Missing fig1 for 000004782.1076257\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000002183.1076375\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missing fig1 for 000004926.1076372\n"
       ]
      }
     ],
     "prompt_number": 279
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(commands)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2397\n"
       ]
      }
     ],
     "prompt_number": 280
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(format_commands)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "686\n"
       ]
      }
     ],
     "prompt_number": 276
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(sample_ids)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "820\n"
       ]
      }
     ],
     "prompt_number": 277
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print latex_commands[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pdflatex /home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_l2E/000001367.1075806/000001367.tex\n"
       ]
      }
     ],
     "prompt_number": 291
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 281,
       "text": [
        "set"
       ]
      }
     ],
     "prompt_number": 281
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(res)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 282,
       "text": [
        "('ag_mod2_l2E', '501203')"
       ]
      }
     ],
     "prompt_number": 282
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_run_details(*list(res)[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 283,
       "text": [
        "{'exit_status': '0',\n",
        " 'mem': '29476kb',\n",
        " 'stderr_file': '/home/mcdonadt/ag_mod2_l2E.e501203',\n",
        " 'stdout_file': '/home/mcdonadt/ag_mod2_l2E.o501203',\n",
        " 'vmem': '79168kb',\n",
        " 'walltime': '00:00:00'}"
       ]
      }
     ],
     "prompt_number": 283
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat /home/mcdonadt/ag_mod2_l2E.e501203"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 284
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cythonmagic\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named Cython",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-290-044c0567378a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'load_ext cythonmagic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/mcdonadt/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2172\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/mcdonadt/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2091\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2092\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2093\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2094\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/mcdonadt/lib/python2.7/site-packages/IPython/core/magics/extension.pyc\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
        "\u001b[1;32m/home/mcdonadt/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/mcdonadt/lib/python2.7/site-packages/IPython/core/magics/extension.pyc\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/mcdonadt/lib/python2.7/site-packages/IPython/core/extensions.pyc\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                     \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_load_ipython_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/mcdonadt/lib/python2.7/site-packages/IPython/extensions/cythonmagic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_ipython_cache_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mCython\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mCython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mErrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCompileError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mCython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcythonize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named Cython"
       ]
      }
     ],
     "prompt_number": 290
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython\n",
      "return a+b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Cell magic `%%cython` not found.\n"
       ]
      }
     ],
     "prompt_number": 289
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print os.getcwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb\n"
       ]
      }
     ],
     "prompt_number": 293
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb\r\n"
       ]
      }
     ],
     "prompt_number": 294
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is used to filter the Gammaproteobacterial contaminants from American Gut samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set to True to test the notebook.  Set to False to use your own data.\n",
      "# If set to True, the user-defined settings for the following parameters (below) will be ignored:\n",
      "#      prj_name\n",
      "#      sequence_files\n",
      "#      mapping_fp\n",
      "debug = False\n",
      "\n",
      "# set the number of processors parallel tasks will use\n",
      "NUM_PROCS = 30\n",
      "\n",
      "# Fill in a project name (arbitrary, but required; must be a valid directory name)\n",
      "prj_name = \"my_project\"\n",
      "\n",
      "# If a single sequence composes at least this fraction of a gammaproteo\n",
      "# OTU that is identified to be over-representend, flat it as a contaminant.\n",
      "contamination_abundance_threshold = 0.85\n",
      "\n",
      "# The value to pass for the -l option of select_gamma.py;\n",
      "# determines what threshold to use when determining what gamma OTUs are over-represented\n",
      "select_gamma_threshold = 0.03\n",
      "\n",
      "# Fill in the paths to each sequence file (gzipped) here, if you have them. If you do not have them, leave this blank!\n",
      "sequence_files = []\n",
      "\n",
      "# The reference representative set, taxonomy file, and tree files\n",
      "reference_rep_set = '/shared/gg_13_5/gg_13_5/rep_set/97_otus.fasta' # e.g., path to 97_otus.fasta from Greengenes\n",
      "reference_taxonomy = '/shared/gg_13_5/gg_13_5/taxonomy/97_otu_taxonomy.txt' # e.g., path to 97_otu_taxonomy.txt from Grengenes\n",
      "reference_tree = '/shared/gg_13_5/gg_13_5/trees/97_otus.tree' # e.g., path to 97_otus.tree from Grengenes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From here on out, you should not have to change anything else! Simply execute the cells in order (except where otherwise noted), **waiting for one to complete before executing the next.**\n",
      "\n",
      "In the next cell, we perform some initial setup."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run cluster_utils.ipy\n",
      "import os\n",
      "from functools import partial\n",
      "from tempfile import mktemp\n",
      "from collections import defaultdict\n",
      "from cPickle import loads\n",
      "from itertools import izip\n",
      "from ftplib import FTP\n",
      "from tarfile import open as tar_open\n",
      "from gzip import open as gz_open\n",
      "\n",
      "from americangut.util import trim_fasta, concatenate_files\n",
      "from americangut.results_utils import (check_file, get_path, get_repository_dir,\n",
      "                                       stage_static_files, \n",
      "                                       parse_identifying_data,\n",
      "                                       count_unique_sequences_per_otu,\n",
      "                                       write_contaminant_fasta)\n",
      "\n",
      "# get the current absolute path\n",
      "current_dir = os.path.abspath('.')\n",
      "\n",
      "# get the path to the American Gut repository \n",
      "repo_dir = get_repository_dir()\n",
      "\n",
      "# create a place to do work\n",
      "if debug:\n",
      "    prj_name = 'debug'\n",
      "\n",
      "working_dir = os.path.join(current_dir, prj_name)\n",
      "os.makedirs(prj_name)\n",
      "\n",
      "# path wrapper\n",
      "get_relative_new_path = lambda x: os.path.join(working_dir, x)\n",
      "get_relative_existing_path = partial(get_path, working_dir)\n",
      "submit = partial(submit, prj_name)\n",
      "\n",
      "# Check the reference representative set, taxonomy file, and tree files\n",
      "check_file(reference_rep_set)\n",
      "check_file(reference_taxonomy)\n",
      "check_file(reference_tree)\n",
      "\n",
      "if debug:\n",
      "    sequence_files = [\n",
      "        os.path.join(repo_dir, 'data', 'AG_debug', 'test_seqs.fna'),\n",
      "        os.path.join(repo_dir, 'data', 'AG_debug', 'test_seqs_2.fna'),\n",
      "    ]\n",
      "\n",
      "for f in sequence_files:\n",
      "    check_file(f)\n",
      "    \n",
      "if debug:\n",
      "    mapping_fp = os.path.join(repo_dir, 'data', 'AG_debug', 'test_mapping.txt')\n",
      "else:\n",
      "    mapping_fp = os.path.join(repo_dir, 'data', 'AG', 'AG.txt')\n",
      "\n",
      "check_file(mapping_fp)\n",
      "\n",
      "# Merged sequence files\n",
      "merged_sequences_fp = get_relative_new_path('merged_sequences.fna')\n",
      "merged_sequences_trimmed_100_fp = get_relative_new_path('merged_sequences_trimmed_100.fna')\n",
      "merged_sequences_trimmed_100_fecal_only_fp = get_relative_new_path('merged_sequences_fecal_only_trimmed_100.fna')\n",
      "\n",
      "# File paths for the initial round of OTU picking, which is used to determine the most abundant proteobacterial OTUs\n",
      "pick_otus_trimmed_100_output_dir = get_relative_new_path('otus_trimmed_100')\n",
      "initial_otu_table = os.path.join(pick_otus_trimmed_100_output_dir, 'otu_table.biom')\n",
      "initial_otu_map = os.path.join(pick_otus_trimmed_100_output_dir, 'uclust_ref_picked_otus',\n",
      "                               os.path.splitext(os.path.split(merged_sequences_trimmed_100_fecal_only_fp)[-1])[0]+'_otus.txt')\n",
      "\n",
      "# Files for select_gamma.py, which calculates cumulative abundances\n",
      "gammaproteo_cumulative_abundances_fp = get_relative_new_path('gammaproteo_cumul_abundances.txt')\n",
      "\n",
      "# File paths for the second round of OTU picking, where the full set of sequences are clustered using the most\n",
      "# abundant proteobacterial OTUs as a reference\n",
      "contaminants_fp = get_relative_new_path('contaminants_%f.fna' % contamination_abundance_threshold)\n",
      "contaminant_picking_dir = get_relative_new_path('contaminant_otus_%f' % contamination_abundance_threshold)\n",
      "\n",
      "# The final set of output sequences (after contaminant filtering)\n",
      "filtered_sequences_fp = get_relative_new_path('filtered_sequences_%f.fna' % contamination_abundance_threshold)\n",
      "\n",
      "# The final round of OTU picking, using the filtered_sequences_fp above\n",
      "filtered_otus_dir = get_relative_new_path('filtered_otus_%f' % contamination_abundance_threshold)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**N.B.: Only execute this cell if you have not already done so for the current project (defined by prj_name, above)!**\n",
      "\n",
      "This cell will download the American Gut sequence files and unpack them in their proper locations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not sequence_files:\n",
      "    ftp = FTP('thebeast.colorado.edu')\n",
      "    ftp.login()\n",
      "    ftp.cwd('pub/QIIME_DB_Public_Studies')\n",
      "    \n",
      "    files = [\n",
      "        \"study_1925_split_library_seqs_and_mapping.tgz\",\n",
      "    #    \"study_1978_split_library_seqs_and_mapping.tgz\",\n",
      "    #    \"study_2006_split_library_seqs_and_mapping.tgz\",\n",
      "    #    \"study_2037_split_library_seqs_and_mapping.tgz\",\n",
      "    #    \"study_2065_split_library_seqs_and_mapping.tgz\",\n",
      "    #    \"study_2191_split_library_seqs_and_mapping.tgz\",\n",
      "    #    \"study_2186_split_library_seqs_and_mapping.tgz\",\n",
      "    #    \"study_2234_split_library_seqs_and_mapping.tgz\"\n",
      "    ]\n",
      "    \n",
      "    tgz_files = []\n",
      "    for f in files:\n",
      "        file_path = get_relative_new_path(f)\n",
      "        tgz_files.append(file_path)\n",
      "        with open(file_path, 'wb') as current_file:\n",
      "            ftp.retrbinary(\"RETR %s\" % f, current_file.write)\n",
      "    \n",
      "    for tgz_file in tgz_files:\n",
      "        study_dir_name = os.path.splitext(os.path.split(tgz_file)[-1])[0]\n",
      "        seq_file_name = study_dir_name.rsplit('_', 2)[0] + '.fna.gz'\n",
      "        seq_file_path = get_relative_new_path(seq_file_name)\n",
      "        with tar_open(tgz_file, 'r:gz') as current_file, open(seq_file_path, 'wb') as seq_file:\n",
      "            seq_file.write(current_file.extractfile(os.path.join(study_dir_name, seq_file_name)).read())\n",
      "        sequence_files.append(seq_file_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are templates for commands that will be needed later in this notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_template = 'pick_closed_reference_otus.py -i %(input)s -o %(output)s -r %(reference)s -t %(taxonomy)s -aO %(num_procs)s'\n",
      "closed_ref_no_tax_template = 'pick_closed_reference_otus.py -i %(input)s -o %(output)s -r %(reference)s -aO %(num_procs)s'\n",
      "filter_fecal_only_template = 'filter_fasta.py -f %(input)s -o %(output)s --mapping_fp=%(mapping_file)s --valid_states=\"BODY_SITE:UBERON:feces\"'\n",
      "filter_fasta_template = 'filter_fasta.py -f %(input)s -m %(otu_map)s -o %(output)s -n'\n",
      "select_gamma_template = 'select_gamma.py -i %(input)s -o %(output)s -l %(level)f'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On to the actual filtering!\n",
      "\n",
      "First, we will merge the sequence files into a single sequence file, and the mapping files into a single mapping file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# concatenate all sequence files into one merged sequence file. This can take a while!\n",
      "with open(merged_sequences_fp, 'w') as merged_seqs:\n",
      "    concatenate_files([gz_open(f, 'rb') for f in sequence_files], merged_seqs)\n",
      "\n",
      "check_file(merged_sequences_fp)\n",
      "    \n",
      "with open(merged_sequences_trimmed_100_fp, 'w') as merged_trimmed_seqs:\n",
      "    trim_fasta(open(merged_sequences_fp, 'U'), merged_trimmed_seqs, 100)\n",
      "\n",
      "check_file(merged_sequences_trimmed_100_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When finding Gammaproteobacterial contaminants, we want to consider only fecal samples, so we filter our sequences to contain only those that are associated with fecal samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filter_fecal_only_args = {\n",
      "    'input': merged_sequences_trimmed_100_fp,\n",
      "    'output': merged_sequences_trimmed_100_fecal_only_fp,\n",
      "    'mapping_file': mapping_fp\n",
      "}\n",
      "filter_fecal_only_job = submit(filter_fecal_only_template % filter_fecal_only_args)\n",
      "jobs = wait_on([filter_fecal_only_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The subsequent filtering procedure will proceed according to the following steps:\n",
      "\n",
      "1. Identify the most abundant Gammaproteobactia and select the representative sequences from the most abundant, such that the cumulative abundance of remaining Gammaproteobacteria is no greater than 3%\n",
      "2. Find all sequences in the American Gut sequences that map to these OTUs at 97PI\n",
      "3. Of these sequences, if 90% of them are a single unique sequence, treat this sequnence as a contaminant seqeunce\n",
      "4. Create reference database of these contaminant sequences\n",
      "5. Cluster (at 97PI) the full set of American Gut sequence against this reference set of contaminant sequences\n",
      "6. Remove all that map to those contaminant sequences\n",
      "7. Pick OTUs against the standard reference database (Greengenes 13_5 in this case)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_1_args = {\n",
      "    'input': merged_sequences_trimmed_100_fecal_only_fp,\n",
      "    'output': pick_otus_trimmed_100_output_dir,\n",
      "    'reference': reference_rep_set,\n",
      "    'taxonomy': reference_taxonomy,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "closed_ref_1_job = submit(closed_ref_template % closed_ref_1_args)\n",
      "jobs = wait_on([closed_ref_1_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, identify gammaproteobacterial OTUs that are over-represented in our data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "select_gamma_args = {\n",
      "    'input': initial_otu_table,\n",
      "    'output': gammaproteo_cumulative_abundances_fp,\n",
      "    'level': select_gamma_threshold,\n",
      "}\n",
      "select_gamma_job = submit(select_gamma_template % select_gamma_args)\n",
      "jobs = wait_on([select_gamma_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to grab all of our sequences that mapped to each of these OTUs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The IDs generated in the previous step by select_gamma.py\n",
      "gammaproteo_ids = {x.strip().split('\\t')[0] for x in open(gammaproteo_cumulative_abundances_fp, 'U').readlines()}\n",
      "\n",
      "print \"Counting unique sequences\"\n",
      "with open(initial_otu_map, 'U') as otu_map, open(merged_sequences_trimmed_100_fp, 'U') as input_seqs:\n",
      "    unique_counts = count_unique_sequences_per_otu(gammaproteo_ids, otu_map, input_seqs)\n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then write out the sequences that are over the abundance threshold"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Writing contaminant sequences...\"\n",
      "with open(contaminants_fp, 'w') as contaminants_f:\n",
      "    write_contaminant_fasta(unique_counts, contaminants_f, contamination_abundance_threshold)\n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now pick OTUs against this FASTA file, which represents the set of contaminant sequences, to identify all sequences in our original merged FASTA file are likely the result of contamination. We will discard these sequences before picking OTUs against our standard reference set.\n",
      "\n",
      "N.B.: If your contaminants.fna file is empty, then no contaminants were detected, and you do not need to proceed any further!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cluster (at 97PI) the full set of American Gut sequence against this reference set of contaminant sequences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_2_args = {\n",
      "    'input': merged_sequences_trimmed_100_fp,\n",
      "    'output': contaminant_picking_dir,\n",
      "    'reference': contaminants_fp,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "closed_ref_2_job = submit(closed_ref_no_tax_template % closed_ref_2_args)\n",
      "jobs = wait_on([closed_ref_2_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remove all that map to those contaminant sequences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contaminant_picking_otu_map = os.path.join(contaminant_picking_dir, 'uclust_ref_picked_otus',\n",
      "                                           os.path.splitext(os.path.split(merged_sequences_trimmed_100_fp)[-1])[0]+'_otus.txt')\n",
      "filter_fasta_args = {\n",
      "    'input': merged_sequences_trimmed_100_fp,\n",
      "    'output':filtered_sequences_fp,\n",
      "    'otu_map': contaminant_picking_otu_map\n",
      "}\n",
      "filter_fasta_job = submit(filter_fasta_template % filter_fasta_args)\n",
      "jobs = wait_on([filter_fasta_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pick OTUs against the standard reference database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_3_args = {\n",
      "    'input': filtered_sequences_fp,\n",
      "    'output': filtered_otus_dir,\n",
      "    'reference': reference_rep_set,\n",
      "    'taxonomy': reference_taxonomy,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "closed_ref_3_job = submit(closed_ref_template % closed_ref_3_args)\n",
      "jobs = wait_on([closed_ref_3_job])\n",
      "\n",
      "print \"Your filtering is complete!  Your sequences are available at\", filtered_otus_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
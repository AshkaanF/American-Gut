{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This IPython notebook assumes precomputed BIOM tables and metadata are readily available. The purpose of this notebook is to output a summary PDF, per sample, present in the American Gut dataset. This requires the full American Gut, the [Human Microbiome Project](http://www.ncbi.nlm.nih.gov/pubmed/22699609), [Global Gut](http://www.ncbi.nlm.nih.gov/pubmed/22699611) and unpublished [Personal Genome Project](http://personalgenomes.org/) microbiome samples. Please see XXX for a discussion on how these tables were created. This notebook assumes there is a PBS/Torque-based compute cluster in which to submit jobs to, and that QIIME 1.7 is available in the path. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this notebook is to produce a framable results PDF for every American Gut fecal sample. Specifically, for each sample, we will be producing the following figures:\n",
      "\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, the Human Microbiome Project, the Global Gut and the Personal Genome Project datasets.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, Global Gut project, highlighting the variation in host age has on the sample.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project.\n",
      "* Taxonomy summary of the sample, and for comparison, a few collapsed American Gut groups and Michael Pollan's pre and post antibiotics sample.\n",
      "* Significantly differentiated operational taxonomic units in the sample compared to other American Gut samples "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, lets get our environment up and running. The script utils.ipy contains some helper methods for submitting jobs to Torque-based clusters. In addition, lets create a new directory for us to work under, setup some helper functions and some paths. Please note, you'll need to specify the path to the 97% Greengenes tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run cluster_utils.ipy\n",
      "\n",
      "# get the current absolute path\n",
      "_basedir = os.path.abspath('.')\n",
      "\n",
      "    ### to toss existing saved environment\n",
      "# drop_env()\n",
      "\n",
      "if recover():\n",
      "    recover_env()\n",
      "    print \"Recovered %s\" % prj_name\n",
      "else:    \n",
      "    create_env(\"ag_mod2\")\n",
      "    \n",
      "if not working_dir.rsplit('/', 1)[0].endswith('American-Gut/ipynb'):\n",
      "    raise ValueError, \"The notebook must be run from the American-Gut/ipynb directory!\"\n",
      "\n",
      "# submission wrapper\n",
      "submit = lambda cmd: submit_qsub(cmd, job_name=prj_name, queue='memroute', extra_args='-l pvmem=8gb')\n",
      "    \n",
      "# path wrapper\n",
      "get_path = lambda x: os.path.join(working_dir, x)\n",
      "    \n",
      "# set the number of processors parallel tasks will use\n",
      "NUM_PROCS = 100\n",
      "\n",
      "# set the path to the Greengenes 13_5 97% OTU tree\n",
      "#greengenes135_97_tree_fp = os.path.expandvars('/ABSOLUTE/PATH/MUST/BE/SPECIFIED')\n",
      "greengenes135_97_tree_fp = os.path.expandvars('$HOME/ResearchWork/gg_13_5_otus/trees/97_otus.tree')\n",
      "if not os.path.exists(greengenes135_97_tree_fp):\n",
      "    raise ValueError(\"Greengenes tree not found, make sure to set the path in the variable 'greengenes135_97_tree_fp'\")\n",
      "    \n",
      "def check_file(f):\n",
      "    if not os.path.exists(f):\n",
      "        raise ValueError(\"Cannot continue! The file %s does not exist!\" % f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, lets setup the paths and some helper methods. The first PCoA plot is a combination of the American Gut, Human Microbiome Project, Personal Genome Project and Global Gut datasets. These projects all used three different sequencing technologies however, and in order to combine them, we need to use the BIOM tables derived from sequence data all trimmed to the same length. See XXX for a more detailed discussion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file paths for tables and metadata. \n",
      "# ag -> American Gut\n",
      "# pgp -> Personal Genome Project\n",
      "# hmp -> Human Microbiome Project\n",
      "# gg -> Global Gut\n",
      "#\n",
      "# _t_ -> table\n",
      "# _m_ -> mapping file\n",
      "#\n",
      "# 100nt -> trimmed to the first 100 nucleotides \n",
      "ag_100nt_t_fp = get_path('../data/AG/AG_100nt.biom')\n",
      "ag_100nt_m_fp = get_path('../data/AG/AG_100nt.txt')\n",
      "ag_t_fp = get_path('../data/AG/AG.biom')\n",
      "ag_m_fp = get_path('../data/AG/AG.txt')\n",
      "\n",
      "pgp_100nt_t_fp = get_path('../data/PGP/PGP_100nt.biom')\n",
      "pgp_100nt_m_fp = get_path('../data/PGP/PGP_100nt.txt')\n",
      "\n",
      "hmp_100nt_t_fp = get_path('../data/HMP/HMPv35_100nt.biom')\n",
      "hmp_100nt_m_fp = get_path('../data/HMP/HMPv35_100nt.txt')\n",
      "\n",
      "gg_100nt_t_fp = get_path('../data/GG/GG_100nt.biom')\n",
      "gg_100nt_m_fp = get_path('../data/GG/GG_100nt.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not all QIIME scripts support gzip'd BIOM tables, so lets uncompress if necessary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for f in [ag_100nt_t_fp, pgp_100nt_t_fp, hmp_100nt_t_fp, gg_100nt_t_fp]:\n",
      "    if not os.path.exists(f) and os.path.exists(f + '.gz'):\n",
      "        decom = f + '.gz'\n",
      "        !gunzip $decom"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to massage the metadata to improve our ability to compare samples. Specifically, were going to: \n",
      "\n",
      "* simplify body sites into their major categories (e.g., transform \"forehead\" and \"skin of hand\" to just \"skin\")\n",
      "* simplify country codes (e.g., GAZ:Venezuela to Venezuela)\n",
      "* simplify the experiment title (e.g., American Gut Project to AGP)\n",
      "* create a hybrid field combining the experiment title with the body site"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a body site mapping:\n",
      "simple_matter_map = {\n",
      "        'feces':'FECAL',\n",
      "        'sebum':'SKIN',\n",
      "        'tongue':'ORAL',\n",
      "        'skin':'SKIN',\n",
      "        'mouth':'ORAL',\n",
      "        'gingiva':'ORAL',\n",
      "        'gingival epithelium':'ORAL',\n",
      "        'nares':'SKIN',\n",
      "        'skin of hand':'SKIN',\n",
      "        'hand':'SKIN',\n",
      "        'skin of head':'SKIN',\n",
      "        'hand skin':'SKIN',\n",
      "        'throat':'ORAL',\n",
      "        'auricular region zone of skin':'SKIN',\n",
      "        'mucosa of tongue':'ORAL',\n",
      "        'palatine tonsil':'ORAL',\n",
      "        'hard palate':'ORAL',\n",
      "        'saliva':'ORAL',\n",
      "        'stool':'FECAL',\n",
      "        'vagina':'SKIN',\n",
      "        'fossa':'SKIN',\n",
      "        'buccal mucosa':'ORAL',\n",
      "        'vaginal fornix':'SKIN',\n",
      "        }\n",
      "\n",
      "def massage_mapping(in_fp, out_fp, body_site_column_name, exp_acronym):\n",
      "    \"\"\"Simplify the mapping file for use in figures\n",
      "\n",
      "    body_site_column_name : specify the column name for body\n",
      "    exp_acronym : short name for the study\n",
      "\n",
      "    Returns False on failure, True on success\n",
      "    \"\"\"\n",
      "    mapping_lines = [l.strip().split('\\t') for l in open(in_fp)]\n",
      "    \n",
      "    header = mapping_lines[0]\n",
      "    header_low = map(lambda x: x.lower(), header)\n",
      "        \n",
      "    try:\n",
      "        bodysite_idx = header_low.index(body_site_column_name.lower())\n",
      "    except IndexError:\n",
      "        print \"Could not find %s in the mapping file header!\" % body_site_column_name\n",
      "        return False\n",
      "    \n",
      "    try:\n",
      "        country_idx = header_low.index('country')\n",
      "    except IndexError:\n",
      "        print \"Could not find the country in the mapping file header!\"\n",
      "        return False\n",
      "\n",
      "    new_mapping_lines = [header[:]]\n",
      "    new_mapping_lines[0].append('SIMPLE_BODY_SITE')\n",
      "    new_mapping_lines[0].append('TITLE_ACRONYM')\n",
      "    new_mapping_lines[0].append('TITLE_BODY_SITE')\n",
      "\n",
      "    for l in mapping_lines[1:]:\n",
      "        new_line = l[:]\n",
      "        body_site = new_line[bodysite_idx]\n",
      "        country = new_line[country_idx]\n",
      "        \n",
      "        # grab the body site\n",
      "        if body_site.startswith('UBERON_'):\n",
      "            body_site = body_site.split('_',1)[-1].replace(\"_\",\" \")\n",
      "        elif body_site.startswith('UBERON:'):\n",
      "            body_site = body_site.split(':',1)[-1]\n",
      "        elif body_site in ['NA', 'unknown']:\n",
      "            # controls, environmental, etc\n",
      "            continue\n",
      "        else:\n",
      "            print \"Unknown body site value: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            return False\n",
      "        \n",
      "        # remap the body site\n",
      "        if body_site.lower() not in simple_matter_map:\n",
      "            print \"Could not remap body site: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            return False\n",
      "        else:\n",
      "            body_site = simple_matter_map[body_site.lower()]\n",
      "         \n",
      "        # simplify the country    \n",
      "        if country.startswith('GAZ:'):\n",
      "            country = country.split(':',1)[-1]\n",
      "        else:\n",
      "            print \"Could not parse country value: %s\" % country\n",
      "            return False\n",
      "        \n",
      "        new_line.append(body_site)\n",
      "        new_line.append(exp_acronym)\n",
      "        new_line.append(\"%s-%s\" % (exp_acronym, body_site))\n",
      "        new_line[country_idx] = country\n",
      "        new_mapping_lines.append(new_line)\n",
      "    \n",
      "    output = open(out_fp,'w')\n",
      "    output.write('\\n'.join(['\\t'.join(l) for l in new_mapping_lines]))\n",
      "    output.write('\\n')\n",
      "    output.close()\n",
      "    \n",
      "    return True\n",
      "\n",
      "# test massage_mapping\n",
      "#ag_100nt_m_TEST_fp = get_path('AG_100nt_TEST.txt')\n",
      "#assert not massage_mapping(ag_100nt_m_fp, ag_100nt_m_TEST_fp, 'does not exist', 'AGP')\n",
      "#assert massage_mapping(ag_100nt_m_fp, ag_100nt_m_TEST_fp, 'body_site', 'AGP')\n",
      "#test_mapping = [l.strip().split('\\t') for l in open(ag_100nt_m_TEST_fp)]\n",
      "#test_header = test_mapping[0]\n",
      "#test_header_length = len(test_header)\n",
      "#assert test_header[-3:] = ['SIMPLE_BODY_SITE', 'TITLE_ACRONYM', 'TITLE_BODY_SITE']\n",
      "#for l in test_mapping[1:]:\n",
      "#    assert l[-3] in ['FECAL','SKIN','ORAL']\n",
      "#    assert l[-2] == 'AGP'\n",
      "#    acro, site = l[-1]\n",
      "#    assert acro == 'AGP'\n",
      "#    assert site in ['FECAL','SKIN','ORAL']\n",
      "#    assert len(l) == test_header_length\n",
      "\n",
      "# new file paths\n",
      "ag_100nt_m_massaged_fp = get_path('AG_100nt_massaged.txt')\n",
      "gg_100nt_m_massaged_fp = get_path('GG_100nt_massaged.txt')\n",
      "pgp_100nt_m_massaged_fp = get_path('PGP_100nt_massaged.txt')\n",
      "hmp_100nt_m_massaged_fp = get_path('HMP_100nt_massaged.txt')\n",
      "\n",
      "# massage\n",
      "assert massage_mapping(ag_100nt_m_fp, ag_100nt_m_massaged_fp, 'body_site', 'AGP')\n",
      "assert massage_mapping(gg_100nt_m_fp, gg_100nt_m_massaged_fp, 'body_site', 'GG')\n",
      "assert massage_mapping(pgp_100nt_m_fp, pgp_100nt_m_massaged_fp, 'body_site', 'PGP')\n",
      "assert massage_mapping(hmp_100nt_m_fp, hmp_100nt_m_massaged_fp, 'bodysite', 'HMP')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've massaged the metadata, we need to merge the mapping files from all the analyses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup output paths, (mm -> massaged mapping)\n",
      "hmp_gg_mm_fp = get_path('HMP_GG_100nt_massaged.txt')\n",
      "ag_pgp_mm_fp = get_path('AG_PGP_100nt_massaged.txt')\n",
      "hmp_gg_ag_pgp_mm_fp = get_path('HMP_GG_AG_PGP_100nt_massaged.txt')\n",
      "\n",
      "hmp_gg_cmd_args = {'input_a':hmp_100nt_m_massaged_fp,\n",
      "                  'input_b':gg_100nt_m_massaged_fp,\n",
      "                  'output':hmp_gg_mm_fp}\n",
      "\n",
      "ag_pgp_cmd_args = {'input_a':ag_100nt_m_massaged_fp,\n",
      "                  'input_b':pgp_100nt_m_massaged_fp,\n",
      "                  'output':ag_pgp_mm_fp}\n",
      "\n",
      "hmp_gg_ag_pgp_cmd_args = {'input_a':hmp_gg_mm_fp,\n",
      "                          'input_b':ag_pgp_mm_fp,\n",
      "                          'output':hmp_gg_ag_pgp_mm_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_job = submit(qiime_scripts['Merge Mapping Files'] % hmp_gg_cmd_args)\n",
      "ag_pgp_job = submit(qiime_scripts['Merge Mapping Files'] % ag_pgp_cmd_args)\n",
      "jobs = wait_on([hmp_gg_job, ag_pgp_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_ag_pgp_job = submit(qiime_scripts['Merge Mapping Files'] % hmp_gg_ag_pgp_cmd_args)\n",
      "jobs = wait_on(hmp_gg_ag_pgp_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to simplify compute on the downstream processes, we're going to filter out the metadata columns that we aren't interested in."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_mapping_file(in_fp, out_fp, columns_to_keep):\n",
      "    \"\"\"Filter out columns in a mapping file\n",
      "\n",
      "    in_fp : the input file path\n",
      "    out_fp : the output file path\n",
      "    columns_to_keep : a dict of the columns to keep, valued by specific category value\n",
      "        if desired to filter out samples that don't meet a given criteria\n",
      "    \"\"\"\n",
      "    lines = [l.strip().split('\\t') for l in open(in_fp)]\n",
      "    header = lines[0][:]\n",
      "    header_lower = map(lambda x: x.lower(), header)\n",
      "\n",
      "    columns_to_keep['#SampleID'] = None\n",
      "    new_header = [\"#SampleID\"]\n",
      "    indices = []\n",
      "    for c in columns_to_keep:\n",
      "        if c.lower() not in header_lower:\n",
      "            raise ValueError(\"Cannot find %s!\" % c)\n",
      "        \n",
      "        indices.append(header_lower.index(c))\n",
      "        new_header.append(c)\n",
      "    \n",
      "    new_lines = [new_header]\n",
      "    for l in lines[1:]:\n",
      "        new_line = []\n",
      "        \n",
      "        keep = True\n",
      "        # fetch values from specific columns\n",
      "        for column, index in zip(new_header, indices):\n",
      "            value = l[index] # implicit, but should never be None\n",
      "            \n",
      "            if columns_to_keep[column] is None:\n",
      "                new_line.append(value)  \n",
      "            elif columns_to_keep[column] != value:\n",
      "                keep = False\n",
      "                break\n",
      "        \n",
      "        if keep:\n",
      "            new_lines.append(new_line)\n",
      "        \n",
      "    out = open(out_fp, 'w')\n",
      "    out.write('\\n'.join(['\\t'(l) for l in new_lines]))\n",
      "    out.write('\\n')\n",
      "    out.close()\n",
      "\n",
      "# test filter_mapping_file\n",
      "#fmf_TEST_fp = get_path('GG_AG_fmf_TEST.txt')\n",
      "#filter_mapping_file(gg_ag_mm_fp, fmf_TEST_fp, {'SIMPLE_BODY_SITE':'FECAL', 'AGE':None, 'TITLE_ACRONYM':None})\n",
      "#test_mapping = [l.strip().split('\\t') for l in open(fmf_test_fp)]\n",
      "#test_header = test_mapping[0]\n",
      "#assert len(test_header, 4)\n",
      "#assert sorted(test_header) == sorted(['#SampleID','SIMPLE_BODY_SITE','AGE','TITLE_ACRONYM']))\n",
      "#test_sbs = test_header.index('SIMPLE_BODY_SITE')\n",
      "#test_acro = test_header.index('TITLE_ACRONYM')\n",
      "#for l in mapping[1:]:\n",
      "#    assert len(l) == 4\n",
      "#    assert l[test_sbs] == 'FECAL'\n",
      "#    assert l[test_acro] in ['AGP','GG']   \n",
      "    \n",
      "fig1_m_fp = get_path('HMP_GG_AG_PGP_figure1.txt')\n",
      "fig2_m_fp = get_path('GG_AG_fecal_figure2.txt')\n",
      "fig3_m_fp = get_path('AG_fecal_figure3.txt')\n",
      "fig4_m_fp = ...\n",
      "fig5_m_fp = get_path('AG_fecal_figure5.txt')\n",
      "\n",
      "filter_mapping_file(hmp_gg_ag_pgp_mm_fp, fig1_m_fp, {'SIMPLE_BODY_SITE':None})\n",
      "filter_mapping_file(gg_ag_mm_fp, fig2_m_fp, {'AGE':None})\n",
      "\n",
      "# what columns here?\n",
      "###filter_mapping_file(ag_m_fp, fig3_m_fp, {'SIMPLE_BODY_SITE':'FECAL', 'TITLE_ACRONYM':None, 'TITLE_BODY_SITE':None})\n",
      "raise ValueError, \"NEED TO RESOLVE METADATA FOR FIG 3\"\n",
      "\n",
      "filter_mapping_file(ag_m_fp, fig5_m_fp, {'AGE':None, 'SIMPLE_BODY_SITE':'FECAL', 'BMI':None, 'DIET_TYPE':None, 'SEX':None})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To improve the readability of the code here, let's also define what the QIIME commands we're going to use will look like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qiime_scripts = {\n",
      "    'Merge OTU Tables':'merge_otu_tables.py -i %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Single Rarifaction':'single_rarifaction.py -i %(input)s -o %(output)s -d %(depth)s',\n",
      "    'Parallel Beta Diversity':'parallel_beta_diversity.py -i %(input)s -o %(output)s -X %(job_prefix)s -O %(num_jobs)s -m unweighted_unifrac -t %(gg97_tree)s',\n",
      "    'Principal Coordinates':'principal_coordinates.py -i %(input)s -o %(output)s',\n",
      "    'Merge Mapping Files':'merge_mapping_files.py -m %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Filter Samples':'filter_samples_from_otu_table.py -i %(input)s -o %(output)s --sample_id_fp=%(sample_id_fp)s',\n",
      "    'Summarize OTU by Category':'summarize_otu_by_cat.py -i %(mapping)s -o %(output)s -n -c %(otu_table)s -m %(category)s'\n",
      "}\n",
      "other_scripts = {\n",
      "    'Signifigant OTU Table':'otu_significance.py -i %(input)s -o %(output)s -l %(level)s -t adasdsdasd' # input -> biomtable, output->directory,\n",
      "    'Taxonomy Comparison':'taxonomy_comparison.py -i %(input)s -m %(mapping)s -l %(level)s -o %(output)s -c %(list_of_categories)s'\n",
      "    'Make Emperor':'make_emperor.py -i %(input)s -o %(output)s -m %(mapping)s',\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the figures, we'll need to merge and filter the tables in a few ways. For figure 1, we want to place the American Gut population in the context of other large studies. To do so, we need to first merge the tables together. Since there are 4 tables to merge, we need to use two merge calls. (It is also feasible to use QIIME's parallel_merge_otu_tables.py here as well). Figure 2 is a combination of Global Gut and the American Gut, but only fecal samples, as is figure 3. Note that for figure 3, we're using the full American Gut table and not the 100 nucleotide version. Since this table is not being combined with the HiSeq data in the Global Gut, we can get away with retaining the full read for added specificity. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting paths\n",
      "hmp_pgp_t_fp = get_path(\"HMP_PGP_100nt.biom\")\n",
      "ag_gg_t_fp = get_path(\"AG_GG_100nt.biom\")\n",
      "hmp_gg_ag_pgp_t_fp = get_path(\"HMP_GG_AG_PGP_100nt.biom\")\n",
      "ag_gg_fecal_t_fp = get_path(\"AG_GG_100nt_fecal.biom\")\n",
      "ag_fecal_t_fp = get_path(\"AG_fecal.biom\")\n",
      "\n",
      "# setup the command arguments for each call\n",
      "hmp_pgp_cmd_args = {'input_a':hmp_100nt_t_fp, \n",
      "                   'input_b':pgp_100nt_t_fp,\n",
      "                   'output':hmp_pgp_t_fp}\n",
      "ag_gg_cmd_args = {'input_a':ag_100nt_t_fp,\n",
      "                   'input_b':gg_100nt_t_fp,\n",
      "                   'output':ag_gg_t_fp}\n",
      "hmp_gg_ag_pgp_cmd_args = {'input_a':ag_gg_t_fp,\n",
      "                          'input_b':hmp_pgp_t_fp,\n",
      "                          'output':hmp_gg_ag_pgp_t_fp}\n",
      "\n",
      "# filter AG-GG table, and AG table\n",
      "ag_gg_filter_fecal_cmd_args = {'input':ag_gg_t_fp,\n",
      "                        'output':ag_gg_fecal_t_fp,\n",
      "                        'sample_id_fp':fig2_m_fp}\n",
      "ag_filter_fecal_cmd_args = {'input':ag_t_fp,\n",
      "                     'output':ag_fecal_t_fp,\n",
      "                     'sample_id_fp':fig3_m_fp}\n",
      "\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_pgp_cmd_args)\n",
      "ag_gg_job = submit(qiime_scripts['Merge OTU Tables'] % ag_g_cmd_args)\n",
      "jobs = wait_on([hmp_pgp_job, ag_gg_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_ag_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_gg_ag_pgp_cmd_args)\n",
      "jobs = wait_on(hmp_gg_ag_pgp_job)\n",
      "\n",
      "# and finally, filter for figures 2 and 3\n",
      "ag_gg_filter_fecal_job = submit(qiime_scripts['Filter Samples'] % ag_gg_filter_fecal_cmd_args)\n",
      "ag_filter_fecal_job = submit(qiime_scripts['Filter Samples'] % ag_filter_fecal_cmd_args)\n",
      "jobs = wait_on([ag_gg_filter_fecal_job, ag_filter_fecal_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 / 2 jobs still running, approximately  260 seconds elapsed\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-27-d724aad846be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mhmp_gg_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqiime_scripts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Merge OTU Tables'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mhmp_gg_cmd_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mag_pgp_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqiime_scripts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Merge OTU Tables'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mag_pgp_cmd_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mwait_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhmp_gg_job\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_pgp_job\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# merge and block until completion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-18-ccc6f0cc57e7>\u001b[0m in \u001b[0;36mwait_on\u001b[1;34m(jobs_to_monitor, additional_prefix)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mrunning_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_qstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mjobs_to_monitor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPOLL_INTERVAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0melapsed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mPOLL_INTERVAL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the data are combined, we need to rarify them in order to normalize for sequencing effort. The Human Microbiome Project has the shallowest coverage per sample, and when operating with the HMP dataset, a rarifaction depth of 1,000 is a reasonable balance between effort and retaining sufficient numbers of samples. The Global Gut and American Gut Project have much deeper coverage per sample (particularly in the case of the Global Gut) and as such, we will rarify at 10,000 sequences per sample for those tables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting path\n",
      "hmp_gg_ag_pgp_t_1k_fp = get_path(\"HMP_GG_AG_PGP_100nt_even1k.biom\")\n",
      "ag_gg_fecal_t_1k_fp   = get_path(\"AG_GG_100nt_fecal_even10k.biom\")\n",
      "hmp_gg_ag_pgp_t_1k_fp = get_path(\"AG_fecal_even10k.biom\")\n",
      "\n",
      "# setup the command arguments\n",
      "hmp_gg_ag_pgp_t_1k_cmd_args = {'input':hmp_gg_ag_pgp_t_fp,\n",
      "                               'output':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                               'depth':'1000'}\n",
      "ag_gg_fecal_t_1k_cmd_args = {'input':ag_gg_fecal_t_fp,\n",
      "                              'output':ag_gg_fecal_t_1k_fp,\n",
      "                              'depth':'1000'}\n",
      "ag_fecal_t_1k_cmd_args = {'input':ag_fecal_t_fp,\n",
      "                          'output':ag_fecal_t_1k_fp,\n",
      "                          'depth':'1000'}\n",
      "\n",
      "# rarifiy and block until completion\n",
      "hmp_gg_ag_pgp_t_1k_job = submit(qiime_scripts['Single Rarifaction'] % hmp_gg_ag_pgp_t_1k_cmd_args)\n",
      "ag_gg_fecal_t_1k_job   = submit(qiime_scripts['Single Rarifaction'] % ag_gg_fecal_t_1k_cmd_args)\n",
      "ag_fecal_t_1k_job      = submit(qiime_scripts['Single Rarifaction'] % ag_fecal_t_1k_cmd_args)\n",
      "jobs = wait_on([hmp_gg_ag_pgp_t_1k_job, ag_gg_fecal_t_1k_job, ag_fecal_t_1k_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unhashable type: 'dict'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-be3b878152aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[('module2', '491121'), ('module2', '491122'), ('module2', '491123'), ('module2', '491124'), ('module2', '491125'), ('module2', '491126'), ('module2', '491127'), ('module2', '491128'), ('module2', '491129'), ('module2', '491130'), ('module2', '491131'), ('module2', '491132'), ('module2', '491133'), ('module2', '491134'), ('module2', '491135'), ('module2', '491136'), ('module2', '491137'), ('module2', '491138'), ('module2', '491139'), ('module2', '491140'), ('module2', '491141'), ('module2', '491142'), ('module2', '491143'), ('module2', '491144'), ('module2', '491145'), ('module2', '491146'), ('module2', '491147'), ('module2', '491148'), ('module2', '491149'), ('module2', '491150')]]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our rarified table, that combines the AG, PGP, HMP, and GG datasets, and a merged mapping file, we can now compute the beta diversity of this OTU table. This step is computationally demanding, and will run for a few hours on a 100 processors. Since its going to run for a while, let's not block (e.g., issue a call to wait_on()) and instead get a few more things queued up to process."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# verify we have the files we need to operate on\n",
      "check_file(hmp_gg_ag_pgp_t_1k_fp); check_file(fig1_m_fp)\n",
      "check_file(ag_gg_fecal_t_1k_fp);   check_file(fig2_m_fp)\n",
      "check_file(ag_fecal_t_1k_fp);      check_file(fig3_m_fp)\n",
      "    \n",
      "# setup output directory\n",
      "hmp_gg_ag_pgp_1k_unweighted_unifrac_d = get_path('bdiv_hmp_gg_ag_pgp_1k_unweighted_unifrac')\n",
      "ag_gg_fecal_1k_unweighted_unifrac_d   = get_path('bdiv_ag_gg_fecal_1k_unweighted_unifrac')\n",
      "ag_fecal_1k_unweighted_unifrac_d      = get_path('bdiv_ag_fecal_1k_unweighted_unifrac')\n",
      " \n",
      "# setup beta diversity arguments\n",
      "hmp_gg_ag_pgp_cmd_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                          'output':hmp_gg_ag_pgp_1k_unweighted_unifrac_d,\n",
      "                          'job_prefix':prj_name + '_f1b_',\n",
      "                          'num_jobs':NUM_PROCS,\n",
      "                          'gg97_tree':greengenes135_97_tree_fp}\n",
      "ag_gg_fecal_cmd_args = {'input':ag_gg_fecal_t_1k_fp,\n",
      "                        'output':ag_gg_fecal_1k_unweighted_unifrac_d,\n",
      "                        'job_prefix':prj_name + '_f2b_',\n",
      "                        'num_jobs':NUM_PROCS,\n",
      "                        'gg97_tree':greengenes135_97_tree_fp}\n",
      "ag_fecal_cmd_args = {'input':ag_fecal_t_1k_fp,\n",
      "                     'output':ag_fecal_1k_unweighted_unifrac_d,\n",
      "                     'job_prefix':prj_name + '_f3b_',\n",
      "                     'num_jobs':NUM_PROCS,\n",
      "                     'gg97_tree':greengenes135_97_tree_fp}\n",
      "\n",
      "# submit and wait\n",
      "hmp_gg_ag_pgp_bdiv_job = submit(qiime_scripts['Parallel Beta Diversity'] % hmp_gg_ag_pgp_cmd_args)\n",
      "ag_gg_bdiv_job         = submit(qiime_scripts['Parallel Beta Diversity'] % ag_gg_fecal_cmd_args)\n",
      "ag_bdiv_job            = submit(qiime_scripts['Parallel Beta Diversity'] % ag_fecal_cmd_args)\n",
      "jobs = wait_on([hmp_gg_ag_pgp_bdiv_job, ag_gg_bdiv_job, ag_bdiv_job], additional_prefix=prj_name + '_f')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our Unifrac distance matrices, need to transform them into principal coordinates space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create two helper methods for the paths\n",
      "bdiv_path = lambda x: os.path.join(x, \"unweighted_unifrac_%s.txt\" % os.path.basename(x)))\n",
      "pc_path = lambda x: os.path.join(x, \"unweighted_unifrac_%s_pc.txt\" % os.path.basename(x)))\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(bdiv_path(hmp_gg_ag_pgp_1k_unweighted_unifrac_d))\n",
      "check_file(bdiv_path(ag_gg_fecal_1k_unweighted_unifrac_d))\n",
      "check_file(bdiv_path(ag_fecal_1k_unweighted_unifrac_d))\n",
      "\n",
      "# setup our arguments\n",
      "hmp_gg_ag_pgp_cmd_args = {'input':bdiv_path(hmp_gg_ag_pgp_1k_unweighted_unifrac_d),\n",
      "                          'output':pc_path(hmp_gg_ag_pgp_1k_unweighted_unifrac_d)}\n",
      "ag_gg_fecal_cmd_args = {'input':bdiv_path(ag_gg_fecal_1k_unweighted_unifrac_d),\n",
      "                        'output':pc_path(ag_gg_fecal_1k_unweighted_unifrac_d)}\n",
      "ag_fecal_cmd_args = {'input':bdiv_path(ag_fecal_1k_unweighted_unifrac_d),\n",
      "                     'output':pc_path(ag_fecal_1k_unweighted_unifrac_d)}\n",
      "\n",
      "# submit the jobs\n",
      "hmp_gg_ag_pgp_pc_job = submit(qiime_scripts['Principal Coordinates'] % hmp_gg_ag_pgp_cmd_args)\n",
      "ag_gg_pc_job         = submit(qiime_scripts['Principal Coordinates'] % ag_gg_fecal_cmd_args)\n",
      "ag_pc_job            = submit(qiime_scripts['Principal Coordinates'] % ag_fecal_cmd_args)\n",
      "jobs = wait_on([hmp_gg_ag_pgp_pc_job, ag_gg_pc_job, ag_pc_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### do it all at 1k..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now to actually make figures 1, 2, and 3 using [Emperor](http://qiime.org/emperor/), a WebGL-based Principal Coordinates viewer. While Emperor is the only tool that we're aware of that can effectively scale to these size datasets for 3D visualization and painting of arbitrary metadata, the tie to WebGL makes its use here a little bit of a challenge. Specifically, we'll be able to generate the plots, but we cannot automatically generate the images from the notebook. First, lets get Emperor up and running, in the following cell, we'll describe how what needs to happen to produce the images."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# quick little helper method\n",
      "emp_path = lambda x,y: os.path.join(x,y)\n",
      "\n",
      "# verify expected files are present\n",
      "check_file(pc_path(hmp_gg_ag_pgp_1k_unweighted_unifrac_d))\n",
      "check_file(pc_path(ag_gg_fecal_1k_unweighted_unifrac_d))\n",
      "check_file(pc_path(ag_fecal_1k_unweighted_unifrac_d))\n",
      "\n",
      "# setup output paths\n",
      "fig1_emp_path = emp_path(hmp_gg_ag_pgp_1k_unweighted_unifrac_d, 'figure1')\n",
      "fig2_emp_path = emp_path(ag_gg_fecal_1k_unweighted_unifrac_d, 'figure2')\n",
      "fig3_emp_path = emp_path(ag_fecal_1k_unweighted_unifrac_d, 'figure3')\n",
      "\n",
      "# setup arguments\n",
      "hmp_gg_ag_pgp_cmd_args = {'input':pc_path(hmp_gg_ag_pgp_1k_unweighted_unifrac_d), \n",
      "                          'output':fig1_emp_path, \n",
      "                          'mapping':fig1_m_fp}\n",
      "ag_gg_fecal_cmd_args = {'input':pc_path(ag_gg_fecal_1k_unweighted_unifrac_d),\n",
      "                        'ouput':fig2_emp_path,\n",
      "                        'mapping':fig2_m_fp}\n",
      "ag_fecal_cmd_args = {'input':pc_path(ag_fecal_1k_unweighted_unifrac_d),\n",
      "                     'output':fig3_emp_path,\n",
      "                     'mapping':fig3_m_fp}\n",
      "\n",
      "hmp_gg_ag_pgp_emp_job = submit(other_scripts['Make Emperor'] % hmp_gg_ag_pgp_cmd_args)\n",
      "ag_gg_emp_job         = submit(other_scripts['Make Emperor'] % ag_gg_fecal_cmd_args)\n",
      "ag_emp_job            = submit(other_scripts['Make Emperor'] % ag_fecal_cmd_args)\n",
      "jobs = wait_on([hmp_gg_ag_pgp_emp_job, ag_gg_emp_job, ag_emp_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's the required manual intervention:\n",
      "\n",
      "1. Click on the first URL link in the resulting pane\n",
      "2. Rotate the view to your a nice perspective\n",
      "3. Click the \"Options\" tab on the right side, and then click on the \"Multishot\" button. **NOTE: this will produce 10s of gigabytes of output!**\n",
      "4. Wait until all of the images have downloaded\n",
      "5. ........... #### possibly need figure specific directions if diff metadata, gradient, etc. unclear how to best copy files right now\n",
      "6. repeat for each figure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.lib.display import FileLink\n",
      "\n",
      "# form the expected paths for Emperor\n",
      "fig1 = get_path(fig1_emp_path, 'index.html')\n",
      "fig2 = get_path(fig2_emp_path, 'index.html')\n",
      "fig3 = get_path(fig3_emp_path, 'index.html')\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(fig1)\n",
      "check_file(fig2)\n",
      "check_file(fig3)\n",
      "\n",
      "# create new HTML links\n",
      "FileLink(fig1, result_html_suffix='Figure 1')\n",
      "FileLink(fig2, result_html_suffix='Figure 2')\n",
      "FileLink(fig3, result_html_suffix='Figure 3')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "cannot import name FileLink",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-0d63255c4f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileLink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# form the expected paths for Emperor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig1_emp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig2_emp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: cannot import name FileLink"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the images are created, we need to copy them to our working area. It is possible file paths will be different, in which case, you may need to change the DOWNLOAD_DIRECTORY variable in the next cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DOWNLOAD_DIRECTORY = '~/Downloads/'\n",
      "\n",
      "# helper function for creating wildcard paths\n",
      "source_path = lambda x,y: os.path.join(x,y)\n",
      "\n",
      "# setup the destination paths\n",
      "figure1_images = get_path('figure1_images')\n",
      "figure2_images = get_path('figure2_images')\n",
      "figure3_images = get_path('figure3_images')\n",
      "\n",
      "# setup the source paths\n",
      "figure1_wildcard = source_path(DOWNLOAD_DIRECTORY, 'figure1_*.svg')\n",
      "figure2_wildcard = source_path(DOWNLOAD_DIRECTORY, 'figure2_*.svg')\n",
      "figure3_wildcard = source_path(DOWNLOAD_DIRECTORY, 'figure3_*.svg')\n",
      "\n",
      "# move the images\n",
      "!mv $figure1_wildcard $figure1_images\n",
      "!mv $figure2_wildcard $figure2_images\n",
      "!mv $figure3_wildcard $figure3_images"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### summarize over the categories provided by justine\n",
      "fig_5_sex_fp = get_path('fig5_sex_path')\n",
      "fig_5_age_fp = get_path('fig5_age_path')\n",
      "fig_5_diet_fp = get_path('fig5_diet_path')\n",
      "fig_5_bmi_fp = get_path('fig5_bmi_path')\n",
      "\n",
      "check_file(ag_fecal_t_1k_fp)\n",
      "check_file(fig5_m_fp)\n",
      "\n",
      "otu_by_cat_sex_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig5_sex_fp,\n",
      "                       'mapping':fig5_m_fp\n",
      "                       'category':'SEX'}\n",
      "otu_by_cat_age_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig5_age_fp,\n",
      "                       'mapping':fig5_m_fp\n",
      "                       'category':'AGE'}\n",
      "otu_by_cat_diet_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig5_sex_fp,\n",
      "                       'mapping':fig5_m_fp\n",
      "                       'category':'DIET_TYPE'}\n",
      "otu_by_cat_bmi_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig5_bmi_fp,\n",
      "                       'mapping':fig5_m_fp\n",
      "                       'category':'BMI'}\n",
      "\n",
      "otu_by_cat_sex_job = submit(qiime_commands['Summarize OTU by Category'] % otu_by_cat_sex_args)\n",
      "otu_by_cat_age_job = submit(qiime_commands['Summarize OTU by Category'] % otu_by_cat_age_args)\n",
      "otu_by_cat_diet_job = submit(qiime_commands['Summarize OTU by Category'] % otu_by_cat_diet_args)\n",
      "otu_by_cat_bmi_job = submit(qiime_commands['Summarize OTU by Category'] % otu_by_cat_bmi_args)\n",
      "\n",
      "res = wait_on([otu_by_cat_sex_job, otu_by_cat_age_job, otu_by_cat_diet_job, otu_by_cat_bmi_job])\n",
      "# submit(summarize_otu_by_cat.py... -n... -m SEX\n",
      "# submit(summarize_otu_by_cat.py... -n... -m AGE\n",
      "# submit(summarize_otu_by_cat.py... -n... -m DIET\n",
      "\n",
      "### merge tables\n",
      "# submit(merge_otu_tables...\n",
      "\n",
      "### perform the taxonomy comparison, pass the merged table, the full (rarified) table, mapping, output directory\n",
      "# submit(taxonomy_comparison.py -i merged -f full, -m mapping... -o directory      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
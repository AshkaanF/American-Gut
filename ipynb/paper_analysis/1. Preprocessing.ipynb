{
 "metadata": {
  "name": "",
  "signature": "sha256:07f8e6580c70d5345c410254fc1d84fe1ac035ecffc6225c1792efa472de3613"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook will preform pre-processing necessary for American Gut Analysis. It is recommended this notebook be used before subsequent, individual analysis notebooks (<a href=\"\">i.e. 2.Rarefaction</a>). This notebook will generate tables necessary to run the subsequent notebooks. It does not need to be run again, unless a new OTU tables have been acquired.\n",
      "\n",
      "If you choose not to run this notebook, all the OTU tables generated here can be downloaded in a tar file from link. Individual data sets can be found\n",
      "<ul><li>All OTUs, all samples\n",
      "</li><li>Filtered OTUs, all samples\n",
      "</li><li>All OTUs, single sample per individual\n",
      "</li><li>Filtered OTUs, single sample per individual\n",
      "</li><li>All OTUs, \n",
      "\n",
      "This notebook assumes the following system requirements.\n",
      "<ul><li><a href=\"https://www.python.org/download/releases/2.7/\">Python 2.7</a>\n",
      "</li><li><a href=\"https://pypi.python.org/pypi/numpy\">Numpy $\\geq$ 1.7</a>\n",
      "</li><li><a href=\"http://qiime.org/install/install.html#latest-development-version\">Qiime 1.8 (development version), commit </a>\n",
      "</li><li><a href=\"http://biom-format.org\">Biom 2.0.1</a>\n",
      "</li><li><a href=\"http://scikit-bio.org\">Scikit Bio 0.1.4</a>\n",
      "</li><li><a href=\"http://pandas.pydata.org\">Pandas 0.14.1</a>\n",
      "</li><li><a href=\"http://ipython.org\">iPython</a>\n",
      "</li></ul>\n",
      "\n",
      "<a id=\"top\"></a>\n",
      "###Table of Contents\n",
      "<ul><li><a href=\"#parameters\">Sets up parameters for data analysis</a>\n",
      "</li><li><a href=\"#files\">Set up files and directories for analysis</a>\n",
      "</li><li><a href=\"#github_download\">Download raw OTU tables from GitHub</a>. This will require an internet connection.\n",
      "</li><li><a href=\"#splitsite\">Split the OTU tables by bodysite.</a>\n",
      "</li><li><a href=\"#clean_map\">Cleans the mapping columns file.</a>\n",
      "</li><li>The mapping file is updated to include a set of derived columns.\n",
      "<ul><li><a href=\"#age_bin\">Age</a>\n",
      "</li><li><a href=\"#etoh_bin\">Alcohol Consumption</a>\n",
      "</li><li><a href=\"#bmi_bin\">BMI</a>\n",
      "</li><li><a href=\"#month_bin\">Collection Date</a>\n",
      "</li><li><a href=\"#state_bin\">Collection Location</a>\n",
      "</li></ul>\n",
      "</li><li><a href=\"#id_subset\">Identifies the subetset of the data</a>\n",
      "</li><li><a href=\"#filter\">Filters the data to remove uncommon OTUs</a>\n",
      "</li><li><a href=\"#rarefaction\">Rarefies the data</a>\n",
      "</li><li><a href=\"#alpha\">Calculates the alpha diversity</a>\n",
      "</li><li><a href=\"#single\">Identifies single sample OTU table and mapping file</a>\n",
      "</li><li><a href=\"#beta\">Calculates beta diversity for the tables</a>\n",
      "</li></ul>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports necessary functions\n",
      "from os import mkdir, remove\n",
      "from os.path import abspath, join as pjoin, exists, split\n",
      "from shutil import copy2, move\n",
      "from numpy import (nan, isnan, arange, ones, sqrt, \n",
      "                   square, nanmin, hstack, array, ceil)\n",
      "from numpy.random import choice\n",
      "from pandas import read_csv, Series, DataFrame\n",
      "from biom import load_table\n",
      "from skbio import DistanceMatrix\n",
      "from jwd_code.pandas_fun import check_dir, pad_index, check_strs\n",
      "from jwd_code.geography_lib import (regions_by_state,\n",
      "                                    us_state_map,\n",
      "                                    canadian_map_english)\n",
      "\n",
      "# Writes a file to save the json string tables\n",
      "def write_biom(table, fp):\n",
      "    \"\"\"Writes a biom table as a json string\"\"\"\n",
      "    file_ = open(fp, 'w')\n",
      "    file_.write(table.to_json(''))\n",
      "    file_.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "<a id=\"parameters\"></a>\n",
      "### Sets Analysis Parameters\n",
      "\n",
      "We need to consider several parameters. These will handle the file reading, rarefaction, alpha_diversity and beta_diversity handling.\n",
      "\n",
      "We can start by setting an important intial parameter: <strong><code>overwrite</code></strong>. This allows us to determin if files files should be downloaded and processed or not. To minimize computational time, <code>overwrite</code> should be <code><font color=\"green\">False</font></code> unless downloading new tables.\n",
      "\n",
      "##### Pandas filehandling parameters\n",
      "<ul><li>The <strong><code>txt_delim</code></strong> specifies the way columns are seperated in the files. Qiime standards typically use text (<code>.txt</code>) files, which are seperated by a tab-character (<code><font color=\"FireBrick\">'\\t'</font></code>).\n",
      "</li><li><strong><code>map_index</code></strong> specifies the name of the file containing the sample names. In Qiime, this is named #SampleID.\n",
      "</li><li>Possible empty values, <strong><code>map_nas</code></strong> provides a list of values which specify data has not been supplied. American Gut participants are free to skip any survey question they do not wish to answer. As a result, the mapping file can contain  empty or missing-data fields.\n",
      "</li><li><strong><code>write_na</code></strong> gives a value used when the files are written. Using an empty space, <code><font color=\"FireBrick\">''</font></code>, scripts like <code>group_signifigance.py</code> will ignore the group.\n",
      "</li></ul>\n",
      "\n",
      "##### Rarefaction parameters\n",
      "<ul><li>The <strong><code>rarefaction_depth</code></strong> specifies the number of sequence per samples to be used for analysis. A depth of 10,000 seqs/sample was selected  because it balances a better picture of diversity with retaining samples. Rarefaction begins by removing samples from the table which do not have the minimum number of counts. Sequences are then drawn randomly out of a weighted pool until we reach the appropriate number. Our unpublished data shows rarefaction is conservative and is appropriate for UniFrac measurements.\n",
      "</li><li><strong><code>num_rarifactions</code></strong> gives the number of rarefaction instances which should be drawn. This controls for bias due to single rarefaction instances. We selected 10 rarefactions to achieve a balance between computational effeciency and appropriate depth.\n",
      "</li></ul>\n",
      "\n",
      "##### Filtering parameters\n",
      "The filtering parameters, <strong><code>filt_frac</code></strong> and <strong><code>filt_num</code></strong> refer to the minimum number of samples, or the minimum fraction of samples in which a particular OTU must be present to be retained after filtering. This final filtering fraction is calculated as the minimum of these two numbers.\n",
      "\n",
      "##### Aphha Diversity Metrics\n",
      "<ul><li><strong><code>alpha_metrics</code></strong> is a comma-seperated string, listing the desired metrics. <br>The current notebook is set to calculate four alpha diversity metrics, <em>PD Whole Tree</em>, <em>Observed Species</em>, <em>choa1</em> and <em>shannon</em> diveristy. A list of avaliable metrics can be found through the <a href=\"http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html#module-skbio.diversity.alpha\">Scikit-Bio website</a>.\n",
      "</li><li><strong><code>div_metric</code></strong> is the diversity metric to be used during analysis.<br>While we can calculate a variety of diversity metrics, is advantegous to focus only on one metric during analysis. Here, we use the mean of the 10 PD whole tree diversity calculations, performed one each rarified table. PD whole tree diversity was selected for thsi analysis becuase it takes into account not only the number of different organisms present, but also their similarity and dissimilarity.\n",
      "</li></ul>\n",
      "\n",
      "##### Beta Diversity Metrics\n",
      "<ul><li><strong><code>beta_metrics</code></strong> are a list of beta diversity metrics. The default is to use weighted and unweighted UniFrac distance.\n",
      "</li></ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "overwrite = False\n",
      "\n",
      "# Sets up parameters for when pandas reads and writes data.\n",
      "txt_delim = '\\t'\n",
      "map_index = '#SampleID'\n",
      "map_nas = ['NA', 'no_data', 'unknown', '']\n",
      "write_na = ''\n",
      "\n",
      "# Sets up rarifaction parameter for 10,000 sequences/sample and \n",
      "# 10 rounds of rarifaction.\n",
      "rarifaction_depth = 10000\n",
      "num_rarifactions = 10\n",
      "\n",
      "# Sets the filtering parameters.\n",
      "filt_frac = 0.01\n",
      "filt_num = 50\n",
      "\n",
      "# Handles the alpha diversity metrics\n",
      "alpha_metrics = 'PD_whole_tree,observed_species,chao1,shannon'\n",
      "div_metric = 'PD_whole_tree'\n",
      "\n",
      "# Handles the beta diversity metrics\n",
      "beta_metrics = \"unweighted_unifrac,weighted_unifrac\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"files\"></a>\n",
      "#### Sets up files\n",
      "\n",
      "We can begin by setting up directories to save the files which will be downloaded and handled here. The default setting is to perform the analysis in a new directory, <code>AGPanalysis</code> in the parent directory of the current directory (assumed to be the notebook directory).\n",
      "\n",
      "To change where data is saved, the <code>base_dir</code> should be set.\n",
      "\n",
      "We can also set the filenames for saving data in the directory. We will produce eight sets of files:\n",
      "<ul><li>The unfiltered table with all samples\n",
      "</li><li>The filtered table, with all samples\n",
      "</li><li>The unfiltered table with only a subset of healhty individuals\n",
      "</li><li>The filtered table with only a subset of healhty individuals\n",
      "</li><li>The unfiltered table with a single sample per indidivual\n",
      "</li><li>The filtered table, with a single sample per indidivual\n",
      "</li><li>The unfiltered table with a single sample for each healthy indiviual\n",
      "</li><li>The filtered table with only a single sample for each healthy indiviual\n",
      "</li></ul>\n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets the base directory for the analysis\n",
      "base_dir = pjoin(abspath('..'), 'agp_analysis')\n",
      "check_dir(base_dir)\n",
      "\n",
      "# Sets up a directory for the OTU information\n",
      "otu_dir = pjoin(base_dir, \"otu_tables\")\n",
      "check_dir(otu_dir)\n",
      "\n",
      "# Creates a new directory where the raw OTU table and raw mapping file should be saved\n",
      "raw_dir = pjoin(otu_dir, 'raw_otu_tables')\n",
      "check_dir(raw_dir)\n",
      "\n",
      "# Creates a directory where split tables should be saved\n",
      "split_dir = pjoin(otu_dir, \"split_by_bodysite\")\n",
      "check_dir(split_dir)\n",
      "\n",
      "# Creates a directory for the the filtered data\n",
      "fecal_dir = pjoin(otu_dir, 'fecal_samples')\n",
      "check_dir(fecal_dir)\n",
      "\n",
      "# Sets up a directory for rarefaction\n",
      "raw_rare_dir = pjoin(otu_dir, \"raw_rarefaction\")\n",
      "check_dir(raw_rare_dir)\n",
      "filt_rare_dir = pjoin(otu_dir, \"filtered_rarefaction\")\n",
      "check_dir(filt_rare_dir)\n",
      "alpha_dir = pjoin(otu_dir, 'alpha')\n",
      "check_dir(alpha_dir)\n",
      "\n",
      "# Sets up directories for saving the tables, maps and distance matrices\n",
      "awt_dir = pjoin(base_dir, 'all_otus_all_samples')\n",
      "check_dir(awt_dir)\n",
      "ast_dir = pjoin(base_dir, 'all_otus_subset_samples')\n",
      "check_dir(ast_dir)\n",
      "aws_dir = pjoin(base_dir, 'all_otus_single_samples')\n",
      "check_dir(aws_dir)\n",
      "ass_dir = pjoin(base_dir, 'all_otus_subset_single_samples')\n",
      "check_dir(ass_dir)\n",
      "fwt_dir = pjoin(base_dir, 'filtered_otus_all_samples')\n",
      "check_dir(fwt_dir)\n",
      "fst_dir = pjoin(base_dir, 'filtered_otus_subset_samples')\n",
      "check_dir(fst_dir)\n",
      "fws_dir = pjoin(base_dir, 'filtered_otus_single_samples')\n",
      "check_dir(fws_dir)\n",
      "fss_dir = pjoin(base_dir, 'filtered_otus_subset_single_samples')\n",
      "check_dir(fss_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also set the file names for the dowloaded files.\n",
      "\n",
      "The <code>tree_fp</code> specifies the location of the phylogenetic tree used for the dataset. American Gut data was picked closed reference using 97% similarity in hte the greengenes 13_5 reference set. The reference set can be downloaded <a href=\"\">here</a>. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets the reference tree filepath. This is required when using phylogentic\n",
      "# metrics such as PD_whole_tree alpha diversity and UniFrac beta diversity.\n",
      "tree_fp = '~/lib/Greengenes/gg_13_5_otus/trees/97_otus.tree'\n",
      "\n",
      "# Sets up the location filepaths for the raw data\n",
      "raw_otu_zip = pjoin(raw_dir, 'AG_100nt.biom.gz')\n",
      "raw_otu_fp = pjoin(raw_dir, 'AG_100nt.biom')\n",
      "raw_map_fp = pjoin(raw_dir, 'AG_100nt.txt')\n",
      "\n",
      "# Handles renaming the split files\n",
      "ori_feces_otu_fp = pjoin(split_dir, 'AG_100nt_UBERON:feces.biom')\n",
      "ori_feces_map_fp = pjoin(split_dir, 'mapping_UBERON:feces.txt')\n",
      "\n",
      "# Sets the filename pattern which will appear for the rarified table\n",
      "rare_filepattern = 'rarefaction_%i_%i.biom'\n",
      "\n",
      "# Sets up the alpha diversity filepattern\n",
      "alpha_filepattern = 'alpha_%s_even%i_%i.txt'\n",
      "\n",
      "# Sets the filepaths for the unrarified tables\n",
      "awtu_otu_fp = pjoin(awt_dir, 'AGP_100nt_fecal.biom')\n",
      "awtu_map_fp = pjoin(awt_dir, 'AGP_100nt_fecal.txt')\n",
      "astu_otu_fp = pjoin(ast_dir, 'AGP_100nt_fecal.biom')\n",
      "astu_map_fp = pjoin(ast_dir, 'AGP_100nt_fecal.txt')\n",
      "awsu_otu_fp = pjoin(aws_dir, 'AGP_100nt_fecal.biom')\n",
      "awsu_map_fp = pjoin(aws_dir, 'AGP_100nt_fecal.txt')\n",
      "assu_otu_fp = pjoin(ass_dir, 'AGP_100nt_fecal.biom')\n",
      "assu_map_fp = pjoin(ass_dir, 'AGP_100nt_fecal.txt')\n",
      "fwtu_otu_fp = pjoin(fwt_dir, 'AGP_100nt_fecal.txt')\n",
      "fwtu_map_fp = pjoin(fwt_dir, 'AGP_100nt_fecal.biom')\n",
      "fstu_otu_fp = pjoin(fst_dir, 'AGP_100nt_fecal.txt')\n",
      "fstu_map_fp = pjoin(fst_dir, 'AGP_100nt_fecal.biom')\n",
      "fwsu_otu_fp = pjoin(fws_dir, 'AGP_100nt_fecal.txt')\n",
      "fwsu_map_fp = pjoin(fws_dir, 'AGP_100nt_fecal.biom')\n",
      "fssu_otu_fp = pjoin(fss_dir, 'AGP_100nt_fecal.txt')\n",
      "fssu_map_fp = pjoin(fss_dir, 'AGP_100nt_fecal.biom')\n",
      "\n",
      "# Sets the filepath for the rarefied table\n",
      "awtr_otu_fp = pjoin(awt_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "awtr_map_fp = pjoin(awt_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "astr_otu_fp = pjoin(ast_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "astr_map_fp = pjoin(ast_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "awsr_otu_fp = pjoin(aws_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "awsr_map_fp = pjoin(aws_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "assr_otu_fp = pjoin(ass_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "assr_map_fp = pjoin(ass_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "fwtr_otu_fp = pjoin(fwt_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fwtr_map_fp = pjoin(fwt_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "fstr_otu_fp = pjoin(fst_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fstr_map_fp = pjoin(fst_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "fwsr_otu_fp = pjoin(fws_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fwsr_map_fp = pjoin(fws_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "fssr_otu_fp = pjoin(fss_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fssr_map_fp = pjoin(fss_dir, 'AGP_100nt_fecal_even10k.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"github_download\"></a>\n",
      "#### Downloads the data from GitHub\n",
      "\n",
      "We now use cURL to download the data. If the data has already been downloaded, it will not be downloaded again unless the notebook is explicitly set to <code>Overwrite = <font color=\"green\">True</font></code>, which will always create or overwrite files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets the biom file\n",
      "if not exists(raw_otu_fp) or overwrite:\n",
      "    # Downloads the compressed biom file\n",
      "    !curl -OL https://github.com/biocore/American-Gut/raw/master/data/AG/AG_100nt.biom.gz\n",
      "    # Unzips the biom file\n",
      "    !gunzip AG_100nt.biom.gz\n",
      "    # Moves the biom file to its final location\n",
      "    move(pjoin('.', 'AG_100nt.biom'), raw_otu_fp)\n",
      "\n",
      "if not exists(raw_map_fp) or overwrite:\n",
      "    # Downloads the mapping file\n",
      "    !wget https://github.com/biocore/American-Gut/raw/master/data/AG/AG_100nt.txt\n",
      "    move(pjoin('.', 'AG_100nt.txt'), raw_map_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "<a id=\"splitsite\"></a>\n",
      "#### Splits the OTU table\n",
      "\n",
      "For more information on splitting out tables using a metadata field, see the <a href=\"http://qiime.org/scripts/split_otu_table.html\">Qiime documentation</a>. <br><em>Note: due to the size of the table, this may take a while to run.</em>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Splits the otu table by body site\n",
      "if not exists(ori_feces_otu_fp) or overwrite:\n",
      "    !split_otu_table.py -i $raw_otu_fp -m $raw_map_fp -f BODY_SITE -o $split_dir\n",
      "\n",
      "# Renames the fecal files to something tidier\n",
      "copy2(ori_feces_otu_fp, awtu_otu_fp)\n",
      "copy2(ori_feces_map_fp, awtu_map_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"clean_map\"></a>\n",
      "#### Cleans up mapping columns\n",
      "There are a set of mapping columns which are currently altered in the EBI submission. These three columns are <code>SEX</code>, <code>AGE_UNIT</code>, and <code>DOMINANT_HAND</code>. So, we will fix the values in these columns for the mapping file we plan to use in our analyses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reads in the raw otu table\n",
      "awtu_otu = load_table(awtu_otu_fp)\n",
      "awtu_map = pad_index(read_csv(awtu_map_fp,\n",
      "                             sep=txt_delim, \n",
      "                             na_values=map_nas,\n",
      "                             parse_dates = [42, 158, 134]),\n",
      "                    index_col=map_index)\n",
      "\n",
      "# Checks the samples in the mapping file are in the otu table, and the\n",
      "# samples in the otu table are in the map\n",
      "awtu_map = awtu_map.loc[awtu_otu.ids()]\n",
      "\n",
      "# Cleans up the sex information based on a miscoding\n",
      "awtu_map.loc[awtu_map.SEX == '47', 'SEX'] = 'male'\n",
      "awtu_map.loc[awtu_map.SEX == '48', 'SEX'] = 'female'\n",
      "\n",
      "# Cleans up age information based on miscodeing\n",
      "awtu_map.loc[awtu_map.AGE_UNIT == '78', 'AGE_UNIT'] = 'years'\n",
      "\n",
      "# Cleans up handedness based on miscoding\n",
      "awtu_map.loc[awtu_map.DOMINANT_HAND == '151', 'DOMINANT_HAND'] = 'left'\n",
      "awtu_map.loc[awtu_map.DOMINANT_HAND == '152', 'DOMINANT_HAND'] = 'right'\n",
      "awtu_map.loc[awtu_map.DOMINANT_HAND == '153', 'DOMINANT_HAND'] = 'ambidextrous'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"age_bin\"></a>\n",
      "######Age\n",
      "\n",
      "There are also a set of columns which are not included in the map, but may be useful for downstream anlyses. These include age binned by decade (<code>AGE_CAT</code>). While there are Qiime analyese which can handle simple, linear, regression, binning can help identify more complex patterns and simplify some of the noise.\n",
      "\n",
      "Here, we bin age by decade, with the exception of people under the age of 20. The gut develops in the first two years of life, and the guts of young children are signifigantly different than older children or adults [citation needed]."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Bins age by decade (with the exception of young children)\n",
      "def categorize_age(x):\n",
      "    if isnan(x):\n",
      "        return x\n",
      "    elif x < 3:\n",
      "        return \"baby\"\n",
      "    elif x < 13:\n",
      "        return \"child\"\n",
      "    elif x < 20:\n",
      "        return \"teen\"\n",
      "    elif x < 30:\n",
      "        return \"20s\"\n",
      "    elif x < 40:\n",
      "        return \"30s\"\n",
      "    elif x < 50:\n",
      "        return \"40s\"\n",
      "    elif x < 60:\n",
      "        return \"50s\"\n",
      "    elif x < 70:\n",
      "        return \"60s\"\n",
      "    else:\n",
      "        return \"70+\"\n",
      "awtu_map['AGE_CAT'] = awtu_map.AGE.apply(categorize_age)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"etoh_bin\"></a>\n",
      "######Alcohol Consumption\n",
      "\n",
      "Maps alcohol frequency into consumption."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def categorize_etoh(x):\n",
      "    if x == 'Never':\n",
      "        return \"No\"\n",
      "    elif isinstance(x, str):\n",
      "        return \"Yes\"\n",
      "    elif isnan(x):\n",
      "        return x\n",
      "    \n",
      "awtu_map['ALCOHOL_CONSUMPTION'] = awtu_map.ALCOHOL_FREQUENCY.apply(categorize_etoh)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"bmi_bin\"></a>\n",
      "######Body Mass Index\n",
      "\n",
      "Maps BMI into categories... more on this..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Categorizes the BMI into groups\n",
      "def categorize_bmi(x):\n",
      "    if isnan(x):\n",
      "        return x\n",
      "    elif x < 18.5:\n",
      "        return \"Underweight\"\n",
      "    elif x < 25:\n",
      "        return \"Normal\"\n",
      "    elif x < 30:\n",
      "        return \"Overweight\"\n",
      "    else:\n",
      "        return \"Obese\"\n",
      "awtu_map['BMI_CAT'] = awtu_map.BMI.apply(categorize_bmi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"month_bin\"></a>\n",
      "######Collection Date\n",
      "\n",
      "Maps the collection date to the month and season, agnostic of year."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Categorizes data by collection month and collection season\n",
      "month_map = {1: ['January', 'Winter'],\n",
      "             2: ['February', 'Winter'],\n",
      "             3: ['March', 'Spring'],\n",
      "             4: ['April', 'Spring'],\n",
      "             5: ['May', 'Spring'],\n",
      "             6: ['June', 'Summer'],\n",
      "             7: ['July', 'Summer'],\n",
      "             8: ['August', 'Summer'],\n",
      "             9: ['September', 'Fall'],\n",
      "             10: ['October', 'Fall'],\n",
      "             11: ['November', 'Fall'],\n",
      "             12: ['December', 'Winter']}\n",
      "\n",
      "# Maps the data as a month\n",
      "awtu_map['COLLECTION_MONTH'] = \\\n",
      "    awtu_map.COLLECTION_DATE.apply(lambda x:month_map[x.month][0])\n",
      "\n",
      "# Maps the data as a season\n",
      "awtu_map['COLLECTION_SEASON'] = \\\n",
      "    awtu_map.COLLECTION_DATE.apply(lambda x:month_map[x.month][1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"state_bin\"></a>\n",
      "######Collection Location\n",
      "\n",
      "We also check the collection state, to insure uniformity in analyses and remove any states which cannot clearly be identified. US States or Canadian providences can be encoded as two letter names (i.e. TX for Texas, BC for British Columbia), or by their full names. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Removes state information for any state not in the US \n",
      "# (This may change as additional countries are added.)\n",
      "countries = awtu_map.groupby('COUNTRY').count().STATE.index.values\n",
      "for country in countries:\n",
      "    if not country == 'GAZ:United States of America' and not country == 'GAZ:Canada':\n",
      "        awtu_map.loc[awtu_map.COUNTRY == country, 'STATE'] = nan\n",
      "\n",
      "# Handles regional mapping, cleaning up states so that only American and\n",
      "# Canadian states are included \n",
      "def check_state(x):\n",
      "    if isinstance(x, str) and x in us_state_map:\n",
      "        return us_state_map[x.upper()]\n",
      "    elif  isinstance(x, str) and x in canadian_map_english:\n",
      "        return canadian_map_english[x.upper()]\n",
      "    else:\n",
      "        return nan\n",
      "def census_f(x):\n",
      "    if  isinstance(x, str) and x in regions_by_state:\n",
      "        return regions_by_state[x]['Census_1']\n",
      "    else:\n",
      "        return nan\n",
      "\n",
      "def economic_f(x):\n",
      "    if isinstance(x, str) and  x in regions_by_state:\n",
      "        return regions_by_state[x]['Economic']\n",
      "    else:\n",
      "        return nan\n",
      "\n",
      "# Applies the functions\n",
      "awtu_map['STATE'] = awtu_map.STATE.apply(check_state)\n",
      "\n",
      "# Adds the census and economic regions\n",
      "awtu_map['CENSUS_REGION'] = awtu_map.STATE.apply(census_f)\n",
      "awtu_map['ECONOMIC_REGION'] = awtu_map.STATE.apply(economic_f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "<a id=\"id_subset\"></a>\n",
      "#### Splits the data into \"healthy\" subset\n",
      "\n",
      "<p>Certain health aspects of participants may have known influences on alpha diversity which overwhelms other potential influences. As a result, we chose to filter out individuals who may belong to a group which confounds the data.</p>\n",
      "<p><a href=\"http://www.pnas.org/content/108/Supplement_1/4554.long\">Recent antibiotic use</a> has been shown to affect alpha diversity; only participants who reported not using antibiotics in the last year were considered in this analysis.</p>\n",
      "<p><a href=\"\">Inflammatory Bowel Disease</a>, both <a href=\"http://www.nature.com/srep/2014/140122/srep03814/full/srep03814.html?message-global=remove\">Type I</a> and <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0009085\">Type II</a> diabetes are associated with lower alpha diversity than age-matched controls, so individuals with these conditions were also excluded.</p>\n",
      "<p><p>A relationship between <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677729/\">Obesity</a> and lower alpha diversity has also been previously observed. In addition, we found that individuals who were considered underweight also had lower diversity than people in the \"normal\" or \"overweight\" BMI categories. As a result, we selected only to include subjects who had a BMI between 18.5 and 25.</p>\n",
      "<p>We start by removing anyone under the age of 20 for three reasons: <a href=\"http://www.nature.com/nature/journal/v486/n7402/full/nature11053.html\">young children</a> have low diversity compared to adults. <a href=\"http://win.niddk.nih.gov/statistics/\">BMI qualifications in children under 18</a> into categories such as \"underweight\", \"normal\", and \"overweight\" depend on the child's age and gender. Finally, we considered alcohol consumption as one of the variables which might affect alpha diversity in this study. Since the overwhelming majority of American Gut participants are American, we chose to set the lower limit of the age range near the legal drinking age to remove potential age-related biases among people who report never drinking.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reads in the mapping file\n",
      "if 'SUBSET' not in awtu_map.columns:\n",
      "    subset_f = {'AGE': lambda x: 19 < x and x < 70 and not isnan(x),\n",
      "                'DIABETES': lambda x: x == 'I do not have diabetes',\n",
      "                'IBD': lambda x: x == 'I do not have IBD',\n",
      "                'ANTIBIOTIC_SELECT': lambda x: x == 'Not in the last year',\n",
      "                'BMI': lambda x: 18.5 <= x < 30 and not isnan(x)}\n",
      "    \n",
      "    # Determines which samples meet the requirements of the categories\n",
      "    new_bin = {}\n",
      "    for cat, f in subset_f.iteritems():\n",
      "        new_bin[cat] = awtu_map[cat].apply(f)\n",
      "    \n",
      "    # Builds up the new binary dataframe\n",
      "    bin_frame = DataFrame(new_bin)\n",
      "    \n",
      "    # Adds a column to the current dataframe to look at the subset\n",
      "    bin_series = DataFrame(new_bin).all(1)\n",
      "    bin_series.name = 'SUBSET'\n",
      "    \n",
      "    awtu_map = awtu_map.join(bin_series)\n",
      "\n",
      "# Saves the updated mapping file\n",
      "awtu_map.loc[awtu_otu.ids()]\n",
      "awtu_map.to_csv(awtu_map_fp,\n",
      "                sep=txt_delim, \n",
      "                na_rep=write_na, \n",
      "                index_label=map_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"filter\"></a>\n",
      "#### Filters the data to remove uncommon OTUs\n",
      "\n",
      "For some analyses, it may be useful to have a set of OTU tables with a single sample per individual. So, we'll read in the mapping file and OTU table, and use these to generate a set of OTUs which are a single sample per individiual. To maximize the number of samples which are retained, samples will be weighted based on the number of sequences in the samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not exists(fwtu_otu_fp) or overwrite:\n",
      "    # Determines the number of samples in the OTU table\n",
      "    num_samps = awtu_otu.ids().shape[0]\n",
      "\n",
      "    # Calculates the number of samples in which an OTU must appear to not be filtered out\n",
      "    filt_count = int(min(ceil(num_samps*filt_frac), filt_num))\n",
      "\n",
      "    # Filters out the low abundance samples\n",
      "    obs_counts = Series(awtu_otu.nonzero_counts('observation'), \n",
      "                        index=awtu_otu.ids('observation'))\n",
      "    obs_keep = obs_counts[obs_counts > filt_count].index\n",
      "\n",
      "    # Gets the filtered table\n",
      "    fwtu_otu = awtu_otu.filter(obs_keep, axis='observation')\n",
      "    \n",
      "    # Gets the mapping file associated with the OTU table\n",
      "    fwtu_map = awtu_map.copy()\n",
      "\n",
      "    # Saves a copy of the updated mapping file in the raw and filtered directory\n",
      "    fwtu_map = awtu_map.loc[fwtu_otu.ids()]\n",
      "    fwtu_map.to_csv(fwtu_map_fp,\n",
      "                    sep=txt_delim, \n",
      "                    na_rep=write_na, \n",
      "                    index_label=map_index)\n",
      "    # Saves an updated copy of the mapping file\n",
      "    awtu_map.to_csv(awtu_map_fp,\n",
      "                    sep=txt_delim, \n",
      "                    na_rep=write_na, \n",
      "                    index_label=map_index)\n",
      "    \n",
      "    # Saves a copy of the filtered table\n",
      "    write_biom(fwtu_otu, fwtu_otu_fp)\n",
      "\n",
      "else:\n",
      "    fwtu_otu = load_table(fwtu_otu_fp)\n",
      "    fwtu_map = pad_index(read_csv(fwtu_map_fp,\n",
      "                             sep=txt_delim, \n",
      "                             na_values=map_nas,\n",
      "                             parse_dates = [42, 158, 134]),\n",
      "                         index_col=map_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"rarefaction\"></a>\n",
      "#### Rarefies the data\n",
      "\n",
      "<p>Next, we rarify the data to 10,000 sequences per sample. This depth was chosen since it balances a better picture of diversity with retaining samples. Rarifaction begins by removing samples from the table which do not have the minimum number of counts. Sequences are then drawn randomly out of a weighted pool until we reach the appropriate number. We can repeat the process multiple times to increase the probability that uncommon taxa are selected.</p>\n",
      "\n",
      "<p>We will use this random subsampling technique ten times, which when averaged, is more likely to recapitulate the original diversity.<br><em>Note: due to the size of the table, this may take a while to run.</em></p>\n",
      "\n",
      "<p>We will begin by rarifying the unfiltered OTU table.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Preforms multiple rarifactions at an even depth\n",
      "if not exists(pjoin(raw_rare_dir, rare_filepattern % (rarifaction_depth, 0))) or overwrite:\n",
      "    !multiple_rarefactions_even_depth.py -i $awtu_otu_fp -o $raw_rare_dir -d $rarifaction_depth -n $num_rarifactions  --lineages_included\n",
      "\n",
      "if not exists(pjoin(filt_rare_dir, rare_filepattern % (rarifaction_depth, 0))) or overwrite:\n",
      "    !multiple_rarefactions_even_depth.py -i $fwtu_otu_fp -o $filt_rare_dir -d $rarifaction_depth -n $num_rarifactions --lineages_included"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"alpha\"></a>\n",
      "#### Calculates the alpha diversity for the filtered and unfiltered table\n",
      "\n",
      "Alpha diversity is a measure of intra sample diversity, or how much variability we find in each sample. There are a variety of ways to calculate alpha diversity. This notebook will calculate four metrics: \n",
      "<a href=\"http://scikit-bio.org/docs/latest/generated/generated/skbio.diversity.alpha.observed_otus.html#skbio.diversity.alpha.observed_otus\">\n",
      "Observed Species Diversity</a>, \n",
      "<a href=\"http://scikit-bio.org/docs/latest/generated/generated/skbio.diversity.alpha.chao1.html#skbio.diversity.alpha.chao1\">\n",
      "Chao1 diversity,</a>\n",
      "<a href=\"http://scikit-bio.org/docs/latest/generated/generated/skbio.diversity.alpha.shannon.html#skbio.diversity.alpha.shannon\">\n",
      "Shannon entropy</a>, and \n",
      "<a href=\"http://www.sciencemag.org/content/308/5728/1635.full\">PD Whole Tree Diversity</a>. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Checks if the alpha rarefaction is necessary\n",
      "if not exists(awtr_map_fp) or not exists(fwtr_map_fp) or overwrite:\n",
      "    # Sets up a holding object for the alpha diversity\n",
      "    alpha_rounds_f = {'%s' % m:{} for m in alpha_metrics.split(',')}\n",
      "    alpha_rounds_r = {'%s' % m:{} for m in alpha_metrics.split(',')}\n",
      "\n",
      "    # Calculates alpha diversity for each rarefaction instance\n",
      "    for i in range(num_rarifactions):\n",
      "        # Sets up the filenames of the rarefaction instances\n",
      "        f_rare_fp = pjoin(filt_rare_dir, rare_filepattern % (rarifaction_depth, i))\n",
      "        r_rare_fp = pjoin(raw_rare_dir, rare_filepattern % (rarifaction_depth, i))\n",
      "\n",
      "        # Sets up the filenames for the alpha diversity tables\n",
      "        f_alpha_fp = pjoin(alpha_dir, alpha_filepattern % ('filt', rarifaction_depth, i))\n",
      "        r_alpha_fp = pjoin(alpha_dir, alpha_filepattern % ('raw', rarifaction_depth, i))\n",
      "\n",
      "        # Calculates the alpha diversity\n",
      "        if not exists(f_alpha_fp) or overwrite:\n",
      "            !alpha_diversity.py -i $f_rare_fp -o $f_alpha_fp -m $alpha_metrics -t $tree_fp\n",
      "        if not exists(r_alpha_fp) or overwrite:\n",
      "            !alpha_diversity.py -i $r_rare_fp -o $r_alpha_fp -m $alpha_metrics -t $tree_fp\n",
      "\n",
      "        # Reads in the alpha diversity\n",
      "        f_alpha = pad_index(read_csv(f_alpha_fp, sep=txt_delim), 'Unnamed: 0')\n",
      "        r_alpha = pad_index(read_csv(r_alpha_fp, sep=txt_delim), 'Unnamed: 0')\n",
      "\n",
      "        # Extracts the data\n",
      "        for col in alpha_rounds_f:\n",
      "            alpha_rounds_f['%s' % col]['%i' % i] = f_alpha[col]\n",
      "            alpha_rounds_f['%s' % col]['%i' % i].name = '%i' % i\n",
      "            alpha_rounds_r['%s' % col]['%i' % i] = r_alpha[col]\n",
      "            alpha_rounds_r['%s' % col]['%i' % i].name = '%i' % i\n",
      "\n",
      "        # Calculates metrics for each set of data\n",
      "        f_metrics = {}\n",
      "        r_metrics = {}\n",
      "        for metric in alpha_rounds_f:\n",
      "            f_metrics['%s_mean' % metric] = DataFrame(alpha_rounds_f[metric]).mean(1)\n",
      "            r_metrics['%s_mean' % metric] = DataFrame(alpha_rounds_r[metric]).mean(1)\n",
      "            f_metrics['%s_stdv' % metric] = DataFrame(alpha_rounds_f[metric]).std(1)\n",
      "            r_metrics['%s_stdv' % metric] = DataFrame(alpha_rounds_r[metric]).std(1)\n",
      "            f_metrics['%s_med' % metric] = DataFrame(alpha_rounds_f[metric]).median(1)\n",
      "            r_metrics['%s_med' % metric] = DataFrame(alpha_rounds_r[metric]).median(1)\n",
      "\n",
      "    # Converts the metrics to a single dataframe\n",
      "    r_alpha_df = DataFrame(r_metrics)\n",
      "    f_alpha_df = DataFrame(f_metrics)\n",
      "\n",
      "    # Appends the data to the mapping file\n",
      "    awtr_map = awtu_map.copy()\n",
      "    fwtr_map = fwtu_map.copy()\n",
      "    awtr_map = awtr_map.join(r_alpha_df)\n",
      "    fwtr_map = fwtr_map.join(f_alpha_df)\n",
      "\n",
      "    # Selects the appropriate metric\n",
      "    r_all_rounds = DataFrame(alpha_rounds_r[div_metric])\n",
      "    r_all_rounds = r_all_rounds.sort_index()\n",
      "    f_all_rounds = DataFrame(alpha_rounds_f[div_metric])\n",
      "    f_all_rounds = f_all_rounds.sort_index()\n",
      "\n",
      "    # Selects the means\n",
      "    r_alpha_df = r_alpha_df.sort_index()\n",
      "    f_alpha_df = f_alpha_df.sort_index()\n",
      "\n",
      "    # Calculates the distance between the two\n",
      "    r_mean_rounds = ([r_alpha_df['%s_mean' % div_metric].values]*ones((num_rarifactions, 1))).transpose()\n",
      "    r_diff = sqrt(square(r_all_rounds.values - r_mean_rounds))/r_mean_rounds\n",
      "    f_mean_rounds = ([f_alpha_df['%s_mean' % div_metric].values]*ones((num_rarifactions, 1))).transpose()\n",
      "    f_diff = sqrt(square(f_all_rounds.values - f_mean_rounds))/f_mean_rounds\n",
      "\n",
      "    # Determines the minimum distance\n",
      "    round_labels = arange(0, 10)\n",
      "    r_round_avg = r_diff.mean(0)\n",
      "    f_round_avg = f_diff.mean(0)\n",
      "\n",
      "    # Identifies the best round\n",
      "    r_best_rarifaction = round_labels[r_round_avg == min(r_round_avg)][0]\n",
      "    f_best_rarifaction = round_labels[f_round_avg == min(f_round_avg)][0]\n",
      "\n",
      "    # Copies the best round into a new folder to be used as the whole table file\n",
      "    copy2(pjoin(raw_rare_dir, rare_filepattern % (rarifaction_depth, r_best_rarifaction)), awtr_otu_fp)\n",
      "    copy2(pjoin(filt_rare_dir, rare_filepattern % (rarifaction_depth, f_best_rarifaction)), fwtr_otu_fp)\n",
      "\n",
      "    # Loads the OTU table\n",
      "    awtr_otu = load_table(awtr_otu_fp)\n",
      "    fwtr_otu = load_table(fwtr_otu_fp)\n",
      "\n",
      "    # Loads the rarefied OTU tables\n",
      "    awtr_map = awtr_map.loc[awtr_otu.ids()]\n",
      "    fwtr_map = fwtr_map.loc[fwtr_otu.ids()]\n",
      "    # Saves the updated file\n",
      "    awtr_map.to_csv(awtr_map_fp, \n",
      "                    sep=txt_delim, \n",
      "                    na_rep=write_na, \n",
      "                    index_label=map_index)\n",
      "    fwtr_map.to_csv(fwtr_map_fp, \n",
      "                    sep=txt_delim, \n",
      "                    na_rep=write_na, \n",
      "                    index_label=map_index)\n",
      "else:\n",
      "    # Loads the OTU table\n",
      "    awtr_otu = load_table(awtr_otu_fp)\n",
      "    fwtr_otu = load_table(fwtr_otu_fp)\n",
      "    awtr_map = pad_index(read_csv(awtr_map_fp,\n",
      "                                  sep=txt_delim, \n",
      "                                  na_values=map_nas,\n",
      "                                  parse_dates = [42, 158, 134]),\n",
      "                         index_col=map_index)\n",
      "\n",
      "    fwtr_map = pad_index(read_csv(fwtr_map_fp,\n",
      "                                  sep=txt_delim, \n",
      "                                  na_values=map_nas,\n",
      "                                  parse_dates = [42, 158, 134]),\n",
      "                         index_col=map_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"single\"></a>\n",
      "#### Identifies single samples in the OTU tables and mapping files.\n",
      "\n",
      "Single samples will be selected from the rarefied set, so the comparison can be made for samples which we know will can be used at at least the specified rarefaction depth. The samples will also be selected from the filtered table, to maximize the possibility they will appear in both the raw and the filtered tables.\n",
      "\n",
      "Individuals can be identified by the <code>HOST_SUBJECT_ID</code>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not exists(awsu_otu_fp) or not exists(fwsr_map_fp) or overwrite:\n",
      "    # Prealocates a place for the ids\n",
      "    single_ids = array([])\n",
      "\n",
      "    # Groups the filtered samples by the HOST SUBJECT ID\n",
      "    host_grouped = fwtr_map.groupby('HOST_SUBJECT_ID')\n",
      "\n",
      "    # Loops through the host subject IDS\n",
      "    for indv, ids in host_grouped.groups.iteritems():\n",
      "        # Checks if a single sample has been submitted or passed rarefaction\n",
      "        # for the indiviudal\n",
      "        if len(ids) == 1:\n",
      "            single_ids = hstack((single_ids, ids))\n",
      "        # Otherwise, a random sample is selected for the indiviudal\n",
      "        else:\n",
      "            single_ids = hstack((single_ids, choice(ids, 1)))\n",
      "\n",
      "    # Filters the OTU tables down to single samples\n",
      "    awsu_otu = awtu_otu.filter(single_ids)\n",
      "    fwsu_otu = fwtu_otu.filter(single_ids)\n",
      "    awsr_otu = awtr_otu.filter(single_ids)\n",
      "    fwsr_otu = fwtr_otu.filter(single_ids)\n",
      "\n",
      "    # Filters the mapping files down to single samples\n",
      "    awsu_map = awtu_map.loc[awsu_otu.ids()]\n",
      "    fwsu_map = fwtu_map.loc[fwsu_otu.ids()]\n",
      "    awsr_map = awtr_map.loc[awsr_otu.ids()]\n",
      "    fwsr_map = fwtr_map.loc[fwsr_otu.ids()]\n",
      "\n",
      "    # Saves the mapping files\n",
      "    awsu_map.to_csv(awsu_map_fp, \n",
      "                   sep=txt_delim, \n",
      "                   na_rep=write_na, \n",
      "                   index_label=map_index)\n",
      "    fwsu_map.to_csv(fwsu_map_fp, \n",
      "                   sep=txt_delim, \n",
      "                   na_rep=write_na, \n",
      "                   index_label=map_index)\n",
      "    awsr_map.to_csv(awsr_map_fp, \n",
      "                   sep=txt_delim, \n",
      "                   na_rep=write_na, \n",
      "                   index_label=map_index)\n",
      "    fwsr_map.to_csv(fwsr_map_fp, \n",
      "                    sep=txt_delim, \n",
      "                    na_rep=write_na, \n",
      "                    index_label=map_index)\n",
      "\n",
      "    # Saves the OTU tables\n",
      "    write_biom(awsu_otu, awsu_otu_fp)\n",
      "    write_biom(fwsu_otu, fwsu_otu_fp)\n",
      "    write_biom(awsr_otu, awsr_otu_fp)\n",
      "    write_biom(fwsr_otu, fwsr_otu_fp)\n",
      "\n",
      "else:\n",
      "    # Loads the mapping files\n",
      "    awsu_map = pad_index(read_csv(awsu_map_fp,\n",
      "                                  sep=txt_delim, \n",
      "                                  na_values=map_nas,\n",
      "                                  parse_dates = [42, 158, 134]),\n",
      "                         index_col=map_index)\n",
      "    fwsu_map = pad_index(read_csv(fwsu_map_fp,\n",
      "                                  sep=txt_delim, \n",
      "                                  na_values=map_nas,\n",
      "                                  parse_dates = [42, 158, 134]),\n",
      "                         index_col=map_index)\n",
      "    awsr_map = pad_index(read_csv(awsr_map_fp,\n",
      "                                  sep=txt_delim, \n",
      "                                  na_values=map_nas,\n",
      "                                  parse_dates = [42, 158, 134]),\n",
      "                         index_col=map_index)\n",
      "    fwsr_map = pad_index(read_csv(fwsr_map_fp,\n",
      "                                  sep=txt_delim, \n",
      "                                  na_values=map_nas,\n",
      "                                  parse_dates = [42, 158, 134]),\n",
      "                         index_col=map_index)\n",
      "    # Loads the OTU tables\n",
      "    awsu_map = load_table(awsu_map_fp)\n",
      "    fwsu_map = load_table(fwsu_map_fp)\n",
      "    awsr_map = load_table(awsr_map_fp)\n",
      "    fwsr_map = load_table(fwsr_map_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"subset\"></a>\n",
      "#### Splits the table into the healthy subsets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_subset_table(all_map, all_otu, sub_map_fp, sub_otu_fp):\n",
      "    \"\"\"Saves the subset data\"\"\"\n",
      "    # Writes the file if it does not exist\n",
      "    if not exists(sub_map_fp) or not exists(sub_otu_fp) or overwrite:\n",
      "        # Filters the mapping table to remove any samples not in the OTU table\n",
      "        all_map = all_map.loc[all_otu.ids()]\n",
      "\n",
      "        # Gets the subset table and otu table\n",
      "        sub_map = all_map.groupby('SUBSET').get_group(True)\n",
      "        sub_otu = all_otu.filter(sub_map.index.values)\n",
      "\n",
      "        # Saves the map and otu table\n",
      "        sub_map.to_csv(sub_map_fp, \n",
      "                       sep=txt_delim, \n",
      "                       na_rep=write_na, \n",
      "                       index_label=map_index)\n",
      "        write_biom(sub_otu, sub_otu_fp)\n",
      "\n",
      "# Sets up a set of maps, otus, and filepaths\n",
      "all_data = [[awtu_map, awtu_otu, astu_map_fp, astu_otu_fp],\n",
      "            [awtr_map, awtr_otu, astr_map_fp, astr_otu_fp],\n",
      "            [awsu_map, awsu_otu, assu_map_fp, assu_otu_fp],\n",
      "            [awsr_map, awsr_otu, assr_map_fp, assr_otu_fp],\n",
      "            [fwtu_map, fwtu_otu, fstu_map_fp, fstu_otu_fp],\n",
      "            [fwtr_map, fwtr_otu, fstr_map_fp, fstr_otu_fp],\n",
      "            [fwsu_map, fwsu_otu, fssu_map_fp, fssu_otu_fp],\n",
      "            [fwsr_map, fwsr_otu, fssr_map_fp, fssr_otu_fp]]\n",
      "\n",
      "# Loops through the data, and writes the subset data\n",
      "for save_set in all_data:\n",
      "    save_subset_table(*save_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"beta\"></a>\n",
      "#### Gets the beta diversity for the tables\n",
      "\n",
      "We calculate weighted and unweighted \n",
      "<a href=\"http://aem.asm.org/content/71/12/8228.short\"> UniFrac</a> distance. Thi s is a phylogenetic metric..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets up the list of file directories\n",
      "dirs = [(awt_dir, awtr_otu_fp), (fwt_dir, fwtr_otu_fp),\n",
      "        (ast_dir, astr_otu_fp), (fst_dir, fstr_otu_fp),\n",
      "        (aws_dir, awsr_otu_fp), (fws_dir, fwsr_otu_fp), \n",
      "        (ass_dir, assr_otu_fp), (fss_dir, fssr_otu_fp)]\n",
      "\n",
      "# Calculates the beta diverity for the tables\n",
      "for (out_dir, otu_fp) in dirs:\n",
      "    un_beta_fp = pjoin(out_dir, 'unweighted_unifrac_%s.txt' % split(otu_fp)[1].replace('.biom', ''))\n",
      "    we_beta_fp = pjoin(out_dir, 'weighted_unifrac_%s.txt' % split(otu_fp)[1].replace('.biom', ''))\n",
      "    if not exists(un_beta_fp) or not exists(we_beta_fp) or overwrite:\n",
      "        !beta_diversity.py -i $otu_fp -t $tree_fp -m $beta_metrics -o $out_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've now generated all the files needed for initial analysis. Other files which are needed will can be generated in the notebooks.\n",
      "\n",
      "<a href=\"#top\">Return to the top</a>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
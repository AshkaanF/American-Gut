{
 "metadata": {
  "name": "",
  "signature": "sha256:e57384f519fc1ccc55938ce4e63bf26066da2646693031c74203c224ef22c13a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Rarefaction\n",
      "[Text describing the import.]\n",
      "\n",
      "####Notebook Requirements\n",
      "<ul><li>cURL\n",
      "</li><li>Python 2.7\n",
      "</li><li>Numpy $\\geq$ 1.7\n",
      "</li><li>Biom format 2.0.1\n",
      "</li><li>Pandas 0.14.1\n",
      "</li><li>StatsModel 0.5.0\n",
      "</li><li>Scikit bio 0.1.4\n",
      "</li><li>Qiime 1.8.0-dev\n",
      "</li><li>Custom code libraries, <a href=\"\">pandas_fun.py</a> and <a href=\"\">power.py</a>.\n",
      "</li></ul>\n",
      "\n",
      "<a id=\"top\"></a>\n",
      "####Table of contents\n",
      "<ul><li><a href=\"#parameters\">Sets analysis parameters</a>\n",
      "</li><li><a href=\"#ifilepath\">Imports necessary files</a>\n",
      "</li><li><a href=\"#multiple_rare\">Multiple Rarefaction</a>\n",
      "</li></ul>\n",
      "\n",
      "We start by importing necessary functions, and determining if files should be overwritten."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# File handling\n",
      "from os import remove, rename\n",
      "from os.path import abspath, isfile, exists, join as pjoin\n",
      "from shutil import move\n",
      "import numpy as np\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.pylab as plt\n",
      "import pandas as pd\n",
      "from biom import load_table\n",
      "from jwd_code.pandas_fun import (check_dir, \n",
      "                                 pad_index)\n",
      "\n",
      "# Writes a file to save the json string tables\n",
      "def write_biom(table, fp):\n",
      "    \"\"\"Writes a biom table as a json string\"\"\"\n",
      "    file_ = open(fp, 'w')\n",
      "    file_.write(table.to_json(''))\n",
      "    file_.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jwdebelius/.virtualenvs/qiime-development/lib/python2.7/site-packages/matplotlib/__init__.py:1312: UserWarning:  This call to matplotlib.use() has no effect\n",
        "because the backend has already been chosen;\n",
        "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\n",
        "  warnings.warn(_use_error_msg)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will also set up some plotting parameters so the generated figures use Helvetica or Arial as their default font."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets up plotting parameters so that the default setting is use to Helvetica\n",
      "# in plots\n",
      "rcParams['font.family'] = 'sans-serif'\n",
      "rcParams['font.sans-serif'] = ['Helvetica', 'Arial']\n",
      "rcParams['text.usetex'] = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=#top>Return to the top</a>\n",
      "\n",
      "<a id=\"parameters\"></a>\n",
      "###### Sets analysis parameters\n",
      "We can also set some necessary parameters for handling files and this analysis.\n",
      "\n",
      "<strong><code>overwrite</code></strong> indicates whether or not new files should be saved and downloaded if they are not otherwise avalaible. For a notebook which runs faster, <strong><code>overwrite</code></strong> should be set to <code><font color=\"green\">False</font></code>.\n",
      "\n",
      "###### Pandas file handling parameters\n",
      "* The <strong><code>txt_delim</code></strong> specifies the way columns are separated in the files. Qiime standards typically use text (.txt) files, which are separated by a tab-character (<code><font color=\"firebrick\">'\\t'</font></code>).\n",
      "* <strong><code>map_index</code></strong> specifies the name of the file containing the sample names. In Qiime, this is named <code>\u201c#SampleID\u201d</code>.\n",
      "* It is possible the mapping file may be missing values, as participants are free to skip any question, so possible missing values are given by <strong><code>map_nas</code></strong>.\n",
      "* <strong><code>write_na</code></strong> gives a value used when the files are written. Using an empty space, (<code><font color=\"firebrick\">''</font></code>), will cause certain Qiime scripts like <code>group_signifigance.py</code> will ignore the missing group.\n",
      "\n",
      "###### Rarefaction Parameters\n",
      "* <code><strong>rare_depths</strong></code> specifies a vector of rarefaction depths to be sampled. We'd like to sample between 100 and 50,000 seqs/sample.\n",
      "* To determine how many samples are to be selected at each depth, specify a vector values in <strong><code>samps_drawn</code></strong>. We will look at between 2 and 2500 samples.\n",
      "* The <strong><code>num_iterations</code></strong> determines how many times a rarefactions should be performed for a given rarefaction depth and number of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "overwrite = False\n",
      "\n",
      "# Sets parameters for reading tables into pandas\n",
      "txt_delim = '\\t'\n",
      "map_index = '#SampleID'\n",
      "map_nas = ['NA', 'no_data', 'unknown']\n",
      "write_na = ''\n",
      "\n",
      "# Sets the rarefaction and sample number parameters\n",
      "rare_depths = np.hstack((np.array([100, 500, 1000]), np.arange(2000, 52000, 2000)))\n",
      "samps_drawn = np.hstack((np.arange(2, 9, 2), np.arange(10, 100, 20), \n",
      "                         np.arange(100, 500, 50), np.arange(500, 1000, 100),\n",
      "                         np.arange(1000, 2500, 250)))\n",
      "num_iterations = 5\n",
      "num_rarefactions = rare_depths.shape[0]\n",
      "num_samples = samps_drawn.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"ifilepath\"></a>\n",
      "######Imports the files\n",
      "\n",
      "This points to the directories and files where analysis files are located. If the directories are not avaliable, they can be downloaded without processing. This notebook focuses on samples from the <a href=\"https://www.dropbox.com/s/c54ggzikun6rjcm/AG_1_10_fecal_subset.tgz\">American Gut rounds 1-10 healthy subset</a> and then <a href=\"https://www.dropbox.com/s/5ufd2v92i633q1i/AG_1_10_filtered_fecal_subset.tgz\">filtered American Gut rounds 1-10 healthy subset</a>. The data will be downloaded using cURL if the files cannot be found.\n",
      "\n",
      "The data is assumed to be saved in a directory called <font color=\"darkblue\">AGPanalysis</font> in the parent directory of the current directory. (It is assumed the filepath is located in the current, notebook directory). To change the location of these files, the <code>base_dir</code> can be changed to an alternative lcoation.\n",
      "\n",
      "We can begin by setting up directories to save the files which will be downloaded and handled here. The default setting is to perform the analysis in a new directory, <code>AGPanalysis</code> in the parent directory of the current directory (assumed to be the notebook directory).\n",
      "\n",
      "To change where data is saved, the <code>base_dir</code> should be set.\n",
      "\n",
      "The downloaded directory will contain all the necessary files, and filenames.\n",
      "\n",
      "This notebook will use files with the ending description <font color=\"firebrick\">\"_single\"</font>, denoting they have been filtered down to have only a single sample per indiviudal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets up the base directory\n",
      "base_dir = pjoin(abspath('..'), 'agp_analysis')\n",
      "check_dir(base_dir)\n",
      "otu_dir = pjoin(base_dir, 'otu_tables')\n",
      "\n",
      "# Sets the directory where the files should be located\n",
      "fecal_dir = pjoin(otu_dir, 'fecal_samples')\n",
      "check_dir(fecal_dir)\n",
      "hmp_dir = pjoin(fecal_dir, 'hmp')\n",
      "check_dir(hmp_dir)\n",
      "aws_dir = pjoin(fecal_dir, 'all_otus_single_samples')\n",
      "check_dir(aws_dir)\n",
      "ass_dir = pjoin(fecal_dir, 'all_otus_subset_single_samples')\n",
      "check_dir(ass_dir)\n",
      "\n",
      "# Sets the subset filepath for all samples\n",
      "awsu_otu_fp = pjoin(aws_dir, 'AGP_100nt_fecal.biom')\n",
      "awsu_map_fp = pjoin(aws_dir, 'AGP_100nt_fecal.txt')\n",
      "\n",
      "assu_otu_fp = pjoin(ass_dir, 'AGP_100nt_fecal.biom')\n",
      "assu_map_fp = pjoin(ass_dir, 'AGP_100nt_fecal.txt')\n",
      "\n",
      "hmpu_otu_fp = pjoin(hmp_dir, 'HMPv35_100nt.biom')\n",
      "hmpu_map_fp = pjoin(hmp_dir, 'HMPv35_100nt.txt')\n",
      "\n",
      "# Sets up a directory to save the rarefaction results\n",
      "rare_dir = pjoin(base_dir, 'rarefaction')\n",
      "check_dir(rare_dir)\n",
      "aws_bar_fp = pjoin(rare_dir, 'agp_all_otus_single_fecal_sample_mean_otus.txt')\n",
      "aws_std_fp = pjoin(rare_dir, 'agp_all_otus_single_fecal_sample_std_otus.txt')\n",
      "\n",
      "ass_bar_fp = pjoin(rare_dir, 'agp_all_otus_single_fecal_subset_sample_mean_otus.txt')\n",
      "ass_std_fp = pjoin(rare_dir, 'agp_all_otus_single_fecal_subset_sample_std_otus.txt')\n",
      "\n",
      "hmp_bar_fp = pjoin(rare_dir, 'hmp_all_otus_single_fecal_sample_mean_otus.txt')\n",
      "hmp_std_fp = pjoin(rare_dir, 'hmp_all_otus_single_fecal_sample_std_otus.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Downloads the files if they are not already avaliable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets data for single samples with all the OTUs\n",
      "if not exists(aws_dir) or overwrite:\n",
      "    # Downloads the files\n",
      "    !curl -OL https://www.dropbox.com/s/w85221gh1tmr4u0/all_otus_single_samples.tgz\n",
      "    # Extracts the data\n",
      "    !tar -xzf all_otus_single_samples.tgz\n",
      "    # Moves the data\n",
      "    remove(pjoin('.', 'all_otus_single_samples.tgz'))\n",
      "    move(pjoin('.', 'all_otus_single_samples'), fecal_dir)\n",
      "\n",
      "# Gets data for single samples with all the OTUs\n",
      "if not exists(ass_dir) or overwrite:\n",
      "    # Downloads the files\n",
      "    !curl -OL https://www.dropbox.com/s/0jnxaubnocer9f8/all_otus_subset_single_samples.tgz\n",
      "    # Extracts the data\n",
      "    !tar -xzf all_otus_subset_single_samples.tgz\n",
      "    # Moves the data\n",
      "    remove(pjoin('.', 'all_otus_subset_single_samples.tgz'))\n",
      "    move(pjoin('.', 'all_otus_subset_single_samples'), fecal_dir)\n",
      "    \n",
      "if not exists(hmpu_otu_fp) or overwrite:\n",
      "    # Downloads the OTU table\n",
      "    !curl -OL https://github.com/biocore/American-Gut/raw/master/data/HMP/HMPv35_100nt.biom.gz\n",
      "    !curl -OL https://github.com/biocore/American-Gut/raw/master/data/HMP/HMPv35_100nt.txt\n",
      "    # Unzips the biom file\n",
      "    !gunzip HMPv35_100nt.biom.gz\n",
      "    # Moves the data to the correct location\n",
      "    move(pjoin(abspath('.'), 'HMPv35_100nt.biom'), hmpu_otu_fp)\n",
      "    move(pjoin(abspath('.'), 'HMPv35_100nt.txt'), hmpu_map_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: github.com\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: github.com\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gunzip: can't stat: HMPv35_100nt.biom.gz (HMPv35_100nt.biom.gz.gz): No such file or directory\r\n"
       ]
      },
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: '/Users/jwdebelius/Repositories/American-Gut/ipynb/paper_analysis/HMPv35_100nt.biom'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-21-3d05dcb67241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'gunzip HMPv35_100nt.biom.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Moves the data to the correct location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HMPv35_100nt.biom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmpu_otu_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HMPv35_100nt.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmpu_map_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSpecialFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`%s` is a named pipe\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/jwdebelius/Repositories/American-Gut/ipynb/paper_analysis/HMPv35_100nt.biom'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loads the data into the notebook for analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loads the unfiltered, unrarified files into python\n",
      "aws_otu = load_table(awsu_otu_fp)\n",
      "aws_map = pad_index(pd.read_csv(awsu_map_fp,\n",
      "                                sep=txt_delim, \n",
      "                                na_values=map_nas),\n",
      "                    index_col=map_index)\n",
      "    \n",
      "ass_otu = load_table(assu_otu_fp)\n",
      "ass_map = pad_index(pd.read_csv(assu_map_fp,\n",
      "                                sep=txt_delim, \n",
      "                                na_values=map_nas),\n",
      "                    index_col=map_index)\n",
      "\n",
      "hmp_otu = load_table(hmpu_otu_fp)\n",
      "hmp_map = pd.read_csv(hmpu_map_fp,\n",
      "                      sep=txt_delim,\n",
      "                      na_values=map_nas,\n",
      "                      index_col='#SampleID')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: '/Users/jwdebelius/Repositories/American-Gut/ipynb/agp_analysis/otu_tables/fecal_samples/hmp/HMPv35_100nt.biom'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-f950b7af940b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     index_col=map_index)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhmp_otu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmpu_otu_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m hmp_map = pd.read_csv(hmpu_map_fp,\n\u001b[1;32m     16\u001b[0m                       \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtxt_delim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jwdebelius/.virtualenvs/qiime-development/lib/python2.7/site-packages/biom/parse.pyc\u001b[0m in \u001b[0;36mload_table\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mbiom_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_biom_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jwdebelius/.virtualenvs/qiime-development/lib/python2.7/site-packages/biom/util.pyc\u001b[0m in \u001b[0;36mbiom_open\u001b[0;34m(fp, permission)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_gzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rb'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpermission\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpermission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jwdebelius/.virtualenvs/qiime-development/lib/python2.7/site-packages/biom/util.pyc\u001b[0m in \u001b[0;36mis_gzip\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mport\u001b[0m \u001b[0mit\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mBIOM\u001b[0m \u001b[0mFormat\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mit\u001b[0m \u001b[0munder\u001b[0m \u001b[0mBIOM\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mBSD\u001b[0m \u001b[0mlicense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \"\"\"\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\x1f\\x8b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/jwdebelius/Repositories/American-Gut/ipynb/agp_analysis/otu_tables/fecal_samples/hmp/HMPv35_100nt.biom'"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We next filter the HMP OTU table down to a single fecal sample per individual, to allow direct comparisons between the studies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Filters the HMP table down to fecal samples only\n",
      "hmpf_map = hmp_map.groupby('bodysite').get_group('UBERON_stool')\n",
      "hmpf_otu = hmp_otu.filter(hmpf_map.index.values)\n",
      "# Filters the HMP table down to a single sample per individual\n",
      "single_ids = np.array([0])\n",
      "for indv, ids in hmpf_map.groupby('hostsubjectid').groups.iteritems():\n",
      "    if len(ids) == 1:\n",
      "        single_ids = np.hstack((single_ids, ids))\n",
      "    else:\n",
      "        single_ids = np.hstack((single_ids, np.random.choice(ids, 1)))\n",
      "hmpf_map = hmpf_map.loc[single_ids[1:]]\n",
      "hmpf_otu = hmpf_otu.filter(single_ids[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the Top</a>\n",
      "\n",
      "<a id=\"multiple_rare\"></a>\n",
      "#####Performs multiple rarefactions on the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not exists(aws_bar_fp) or not exists(ass_bar_fp) or not exists(hmp_bar_fp) or overwrite:\n",
      "    aws_counts = []\n",
      "    ass_counts = []\n",
      "    hmp_counts = []\n",
      "\n",
      "    # Draws the samples\n",
      "    for id1, depth in enumerate(rare_depths):\n",
      "        aws_r = []\n",
      "        ass_r = []\n",
      "        hmp_r = []\n",
      "        for id2, samp in enumerate(samps_drawn):\n",
      "            aws_vec = np.zeros(num_iterations)\n",
      "            ass_vec = np.zeros(num_iterations)\n",
      "            hmp_vec = np.zeros(num_iterations)\n",
      "            for id3 in range(num_iterations):\n",
      "                # Gets the number of OTUs for all samples from a single individual\n",
      "                try:\n",
      "                    sub1 = aws_otu.copy()\n",
      "                    sub1 = sub1.subsample(depth, axis='sample')\n",
      "                    sub2 = sub1.subsample(samp, axis='sample', by_id=True)\n",
      "                    aws_vec[id3] = sum(sub2.nonzero_counts(axis='observation') > 0)\n",
      "                except:\n",
      "                    aws_vec[id3] = np.nan\n",
      "                # Gets the number of OTUs for all samples from the healthy subset\n",
      "                try:\n",
      "                    sub1 = ass_otu.copy()\n",
      "                    sub1 = sub1.subsample(depth, axis='sample')\n",
      "                    sub2 = sub1.subsample(samp, axis='sample', by_id=True)\n",
      "                    ass_vec[id3] = sum(sub2.nonzero_counts(axis='observation') > 0)\n",
      "                except:\n",
      "                    ass_vec[id3] = np.nan\n",
      "                # Gets the number of OTUs for the samples in the HMP\n",
      "                try:\n",
      "                    sub1 = hmpf_otu.copy()\n",
      "                    sub1 = sub1.subsample(depth, axis='sample')\n",
      "                    sub2 = sub1.subsample(samp, axis='sample', by_id=True)\n",
      "                    hmp_vec[id3] = sum(sub2.nonzero_counts(axis='observation') > 0)\n",
      "                except:\n",
      "                    hmp_vec[id3] = np.nan\n",
      "            aws_r.append(aws_vec)\n",
      "            ass_r.append(ass_vec)\n",
      "            hmp_r.append(hmp_vec)\n",
      "        aws_counts.append(aws_r)\n",
      "        ass_counts.append(ass_r)\n",
      "        hmp_counts.append(hmp_r)\n",
      "    # Prealocates the objects for averaging\n",
      "    aws_bar = np.zeros((num_rarefactions, num_samples))\n",
      "    aws_std = np.zeros((num_rarefactions, num_samples))\n",
      "\n",
      "    ass_bar = np.zeros((num_rarefactions, num_samples))\n",
      "    ass_std = np.zeros((num_rarefactions, num_samples))\n",
      "\n",
      "    hmp_bar = np.zeros((num_rarefactions, num_samples))\n",
      "    hmp_std = np.zeros((num_rarefactions, num_samples))\n",
      "\n",
      "    for i in range(num_rarefactions):\n",
      "        for j in range(num_samples):\n",
      "            aws_bar[i, j] = aws_counts[i][j].mean()\n",
      "            aws_std[i, j] = aws_counts[i][j].std()\n",
      "\n",
      "            ass_bar[i, j] = aws_counts[i][j].mean()\n",
      "            ass_std[i, j] = aws_counts[i][j].std()\n",
      "\n",
      "            hmp_bar[i, j] = aws_counts[i][j].mean()\n",
      "            hmp_std[i, j] = aws_counts[i][j].std()\n",
      "    # Saves the tables\n",
      "    aws_bar_df = pd.DataFrame(data=aws_bar, index=rare_depths, columns=samps_drawn)\n",
      "    aws_std_df = pd.DataFrame(data=aws_std, index=rare_depths, columns=samps_drawn)\n",
      "\n",
      "    ass_bar_df = pd.DataFrame(data=ass_bar, index=rare_depths, columns=samps_drawn)\n",
      "    ass_std_df = pd.DataFrame(data=ass_std, index=rare_depths, columns=samps_drawn)\n",
      "\n",
      "    hmp_bar_df = pd.DataFrame(data=hmp_bar, index=rare_depths, columns=samps_drawn)\n",
      "    hmp_std_df = pd.DataFrame(data=hmp_std, index=rare_depths, columns=samps_drawn)\n",
      "    aws_bar_df.to_csv(aws_bar_fp, sep=txt_delim, index_col='counts')\n",
      "    aws_std_df.to_csv(aws_std_fp, sep=txt_delim, index_col='counts')\n",
      "    ass_bar_df.to_csv(ass_bar_fp, sep=txt_delim, index_col='counts')\n",
      "    ass_std_df.to_csv(ass_std_fp, sep=txt_delim, index_col='counts')\n",
      "    hmp_bar_df.to_csv(hmp_bar_fp, sep=txt_delim, index_col='counts')\n",
      "    hmp_std_df.to_csv(hmp_std_fp, sep=txt_delim, index_col='counts')\n",
      "else:\n",
      "    # Loads the table\n",
      "    aws_bar_df = pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loads the table\n",
      "aws_bar_df = pd.read_csv(aws_bar_fp, sep='\\t', index_label='counts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
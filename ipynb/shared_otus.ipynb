{
 "metadata": {
  "name": "",
  "signature": "sha256:ec88a3e35cc5c6700a80700c1380acae9452ba25fccfcfc22d04a1789903da6b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* American Gut Project\n",
      "* License: BSD\n",
      "* Author of this notebook: Daniel McDonald\n",
      "\n",
      "For this notebook, we're going to explore how the number of shared OTUs differs based on metadata. What these plots will show are the number of OTUs that are shared in an increasing number of samples. For instance, we expect to see a very large number of OTUs shared in at least 1% of the samples, fewer that are shared in at least 10%, even fewer at 50%, and almost none in 100% of the samples. This makes sense as we harbor unique microbial communities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This Notebook assumes the following dependencies are in your environment:\n",
      "\n",
      "* [BIOM](http://biom-format.org) == 2.0.1\n",
      "* [matplotlib](http://matplotlib.org/) >= 1.1.0\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from biom import load_table\n",
      "from biom.parse import MetadataMap\n",
      "from collections import Counter\n",
      "\n",
      "!wget https://github.com/biocore/American-Gut/raw/master/data/AG/AG_100nt.txt --no-check-certificate\n",
      "!wget https://github.com/biocore/American-Gut/raw/master/data/AG/AG_100nt.biom.gz --no-check-certificate\n",
      "\n",
      "metadata = MetadataMap.from_file('AG_100nt.txt')\n",
      "table = load_table('AG_100nt.biom.gz')\n",
      "table.add_metadata(metadata)\n",
      "\n",
      "# Filter samples that have fewer than 10,000 sequences\n",
      "more_than_10k_seqs = lambda v, id_, md: sum(v) >= 10000\n",
      "table.filter(more_than_10k_seqs);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first function that we're going to define will compute the percentage of shared OTUs over the samples for a specific metadata category and value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_shared_otus(table_full, category, values):\n",
      "    \"\"\"Compute shared OTUs for a given category and values\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    table_full : Table\n",
      "        A BIOM table. It is assumed that this table has sample metadata associated with it.\n",
      "    category : str\n",
      "        The metadata category\n",
      "    values : list of str\n",
      "        The values of interest within the category\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    np.array\n",
      "        Observed percentages\n",
      "    np.array\n",
      "        Observed top OTU IDs\n",
      "    list\n",
      "        Observed top taxa\n",
      "    int\n",
      "        Number of samples in the filtered table\n",
      "    \n",
      "    \"\"\"\n",
      "    def _get_taxa(t, id_):\n",
      "        md = t.metadata(id_, axis='observation')\n",
      "        if md is None:\n",
      "            taxa = ''\n",
      "        else:\n",
      "            taxa = '; '.join(md['taxonomy'])\n",
      "        return taxa\n",
      "    \n",
      "    table = table_full.pa(inplace=False)\n",
      "    \n",
      "    # filter to just those samples that meet our metadata category and values\n",
      "    cat_fn = lambda v, i, md: md[category] in values\n",
      "    site = table.filter(cat_fn, inplace=False)\n",
      "\n",
      "    # normalize\n",
      "    norm_fn = lambda v, i, md: v / len(site.sample_ids)\n",
      "    site.transform(norm_fn)\n",
      "    \n",
      "    # compute percents\n",
      "    obs_full = site.sum('observation')\n",
      "    obs_percents = np.array([(obs_full > i).sum() for i in np.arange(0.0, 1.0, 0.01)], dtype=float)\n",
      "    \n",
      "    # determine the taxa that appear to be highly shared\n",
      "    top_ids = site.observation_ids[np.argwhere(obs_full > 0.95)].squeeze()\n",
      "    top_taxa = []\n",
      "    if len(np.atleast_1d(top_ids)) > 0:\n",
      "        if not top_ids.shape: # array of scalar value\n",
      "            top_ids = np.array(str(top_ids))\n",
      "            top_taxa.append(_get_taxa(site, str(top_ids)))\n",
      "        else:\n",
      "            for id_ in top_ids:\n",
      "                top_taxa.append(_get_taxa(site, id_))\n",
      "    \n",
      "    return obs_percents, top_ids, top_taxa, len(site.sample_ids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next cell, we're going to first define a helper function for filtering to just fecal samples, and then define a more complex method that will allow us to compute shared OTUs over multiple category values within a metadata category. This method will additionally subsample the number of samples being examined to normalize for the number of samples used in the compute within each category value. Since we're subsampling the category values that have a larger number of samples associated, we're going to perform the subsample multiple times and return the mean shared OTUs observed. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fecal_filter = lambda v, i, md: md['BODY_SITE'] == 'UBERON:feces'\n",
      "def compute_multiple_values(table_full, category, min_count=25, prefilter=fecal_filter, iterations=10, \n",
      "                            discretize=None):\n",
      "    \"\"\"Compute shared OTUs over multiple category values\n",
      "    \n",
      "    This method will print summary details about the category being examined which can\n",
      "    be used to inform a good min_count.\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    table_full : Table\n",
      "        The full table to operate on.\n",
      "    category : str\n",
      "        The metadata category to examine.\n",
      "    min_count : int, optional\n",
      "        The minimum number of samples that must be associated with a category value.\n",
      "        Defaults to 25.\n",
      "    prefilter : func\n",
      "        Prefilter, e.g., down to just fecal samples\n",
      "    iterations : int, optional\n",
      "        Number of rarefactions to perform\n",
      "    discretize : list of tuple, optional\n",
      "        Bins to discretize the data. Expected form: [(min, max)]\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    np.array\n",
      "        The mean observed percents in row order with observed values\n",
      "    np.array\n",
      "        The stderr observed percents in row order with observed values\n",
      "    list of str\n",
      "        The observed category values\n",
      "    int\n",
      "        Sample depth used\n",
      "        \n",
      "    Notes\n",
      "    -----\n",
      "    This method ignores NA and no_data category values\n",
      "    \n",
      "    \"\"\"\n",
      "    def make_filter_f(md_field, md_val):\n",
      "        \"\"\"Construct a filter function based on a metadata category and value\n",
      "        \n",
      "        If md_val is a tuple, a discretization is attempted.\n",
      "        \"\"\"\n",
      "        if isinstance(md_val, tuple):\n",
      "            min_, max_ = md_val\n",
      "            def f(v, i, samp_md):\n",
      "                \"\"\"Return True if the sample is value is min_ <= foo < max_\"\"\"\n",
      "                try:\n",
      "                    return min_ <= float(samp_md[md_field]) < max_\n",
      "                except ValueError:\n",
      "                    return False\n",
      "        else:\n",
      "            def f(v, i, samp_md):\n",
      "                \"\"\"Return true if the sample is associated with our desired value\"\"\"\n",
      "                return samp_md[md_field] == md_val\n",
      "        return f\n",
      "    \n",
      "    pa = table_full.filter(prefilter, inplace=False).pa()\n",
      "    \n",
      "    # get the counts within each category value\n",
      "    metadata_counts = Counter()\n",
      "    for md in pa.sample_metadata:\n",
      "        # ignore missing data\n",
      "        if md[category] in ['NA', 'no_data']:\n",
      "            continue\n",
      "            \n",
      "        if discretize is not None:\n",
      "            for min_, max_ in discretize:\n",
      "                if min_ <= float(md[category]) < max_:\n",
      "                    metadata_counts[(min_, max_)] += 1\n",
      "                    break\n",
      "        else:\n",
      "            metadata_counts[md[category]] += 1\n",
      "        \n",
      "    print \"Category counts for %s:\" % category\n",
      "    for k, v in sorted(metadata_counts.items(), key=lambda item: item[1], reverse=True):\n",
      "        print \"  %s: %d\" % (k, v)\n",
      "        \n",
      "    # determine the minimum sampling depth\n",
      "    sample_depth = min(filter(lambda v: v > min_count, metadata_counts.values()))        \n",
      "    metadata_values = [k for k, v in metadata_counts.items() if v >= sample_depth]\n",
      "    \n",
      "    observed_means = []\n",
      "    observed_stderrs = []\n",
      "    observed_values = []\n",
      "    for obs_v in sorted(metadata_values):\n",
      "        if discretize is not None:\n",
      "            observed_values.append(\"%d <= x < %d\" % obs_v)\n",
      "        else:\n",
      "            observed_values.append(obs_v)\n",
      "    \n",
      "        # filter to the specific category value\n",
      "        md_specific = pa.filter(make_filter_f(category, obs_v), inplace=False)\n",
      "        ids = md_specific.sample_ids.copy() \n",
      "\n",
      "        results = np.zeros((iterations, 100), dtype=float)\n",
      "        for idx in range(iterations):\n",
      "            # subsample the table\n",
      "            np.random.shuffle(ids)\n",
      "            subsampled_ids = set(ids[:sample_depth])\n",
      "            filter_f = lambda v, i, md: i in subsampled_ids\n",
      "            ss = md_specific.filter(filter_f, inplace=False)\n",
      "            ss.filter(lambda v, i, md: sum(v) > 0, axis='observation')\n",
      "\n",
      "            # normalize\n",
      "            norm_f = lambda v, i, md: v / len(ss.sample_ids)\n",
      "            ss.transform(norm_f)\n",
      "            \n",
      "            # compute percents\n",
      "            obs_full = ss.sum('observation')\n",
      "            results[idx] = np.array([(obs_full > j).sum() for j in np.arange(0.0, 1.0, 0.01)], dtype=float)        \n",
      "        \n",
      "        observed_means.append(results.mean(axis=0))\n",
      "        observed_stderrs.append(results.std(axis=0) / np.sqrt(iterations))\n",
      "    \n",
      "    return np.asarray(observed_means), np.asarray(observed_stderrs), observed_values, sample_depth"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we're going to construct a method to plot the results of out shared OTUs functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_shared_otus(n_samples, percents, labels, title=None, depth=None, stderrs=None, start=0):\n",
      "    \"\"\"Plot the shared OTUs\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_samples : int\n",
      "        The number of samples reflected\n",
      "    percents : np.array\n",
      "        The percents to plot\n",
      "    title : str, optional\n",
      "        Additional title name\n",
      "    depth : int, optional\n",
      "        The subsample depth\n",
      "    \"\"\" \n",
      "    if len(percents.shape) == 1:\n",
      "        percents = np.array([percents])\n",
      "    \n",
      "    fig = plt.figure()\n",
      "    ax_count = plt.gca()\n",
      "    \n",
      "    title_str = 'Shared OTUs'\n",
      "    \n",
      "    if title is not None:\n",
      "        title_str += ': %s' % title\n",
      "    if depth is not None:\n",
      "        title_str += ', n=%d' % depth\n",
      "    \n",
      "    ax_count.set_title(title_str)\n",
      "    ax_count.set_yscale('log')\n",
      "    ax_count.set_ylabel('Number of OTUs')\n",
      "    ax_count.set_xlabel('Number of samples')\n",
      "    \n",
      "    x_ticks = np.arange(0.0, 1.0, 0.01) * n_samples\n",
      "    \n",
      "    for idx, row in enumerate(percents):\n",
      "        if stderrs is not None:\n",
      "            ax_count.errorbar(x_ticks[start:], row[start:], yerr=stderrs[idx][start:], fmt='o', \n",
      "                              markersize=5, markeredgewidth=0.25)\n",
      "        else:\n",
      "            ax_count.plot(x_ticks[start:], row[start:])\n",
      "\n",
      "    plt.legend(labels, numpoints=1)\n",
      "            \n",
      "    plt.tight_layout()\n",
      "    ax_count.grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we're going to define a few helper methods just to make the subsequent calls very easy. The compute methods above return other interesting bits of information though which you may like to explore."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def helper_single(category, values, label):\n",
      "    percents, top_ids, top_taxa, n_samples = compute_shared_otus(table, category, values)\n",
      "    plot_shared_otus(n_samples, percents, [label])\n",
      "    print \"Top observed shared taxa:\"\n",
      "    for t in top_taxa:\n",
      "        print '    %s' % t\n",
      "\n",
      "def helper_multiple(category, title, min_count=None, discretize=None):\n",
      "    means, stderrs, values, depth = compute_multiple_values(table, category, min_count, discretize=discretize)\n",
      "    plot_shared_otus(depth, means, values, title=title, depth=depth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Plots**\n",
      "\n",
      "Now lets go and make some plots using the functions we just created. The first three plots will be high level, and just of the body sites. What you can see is that there are a lot of shared OTUs between people, but the number that are shared begin to drop off quickly as you increase the number of samples (_note that the y-axis is in a log-scale_). It does appear that a handful of OTUs are shared over all samples within the oral and skin sites but the over all sample size is much lower than the fecal site."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "helper_single('BODY_SITE', ['UBERON:feces'], 'Fecal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "helper_single('BODY_SITE', ['UBERON:skin', 'UBERON:hand'], 'Skin')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "helper_single('BODY_SITE', ['UBERON:tongue'], 'Oral')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, lets slice and dice the data further. The next set of plots will look just within the fecal samples, and then within a given metadata category. Since we have uneven numbers of samples associated with each category (e.g., the number of people who have taken antibiotics recently is much less than the number of people who have not taken antibiotics in the last year), we're going to randomly select an even number of samples within each group and then compute shared OTUs from there. Since we're randomly subsampling, we're going to do this procedure 10 times and plot the mean number of shared OTUs.\n",
      "\n",
      "In the first plot, we're taking a look at the number of shared OTUs stratified by how recently antibiotics were used.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "helper_multiple('ANTIBIOTIC_SELECT', 'Antibiotic use')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next plot, we're looking at whether seasonal allergies appear to impact the number of shared OTUs. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "helper_multiple('SEASONAL_ALLERGIES', 'Seasonal allergies')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Diet interestingly appears to show a difference, with vegans having the most shared OTUs. This could potentially be explained by the number of plants consumed (see below), which appear to be associated with the number of shared OTUs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "helper_multiple('DIET_TYPE', 'Diet type')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The number of plants consumed in a given week looks like it may have an affect as well, where if you consume less than 5 different types of plants per week, you appear to have fewer shared OTUs with the rest of the population."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "helper_multiple('TYPES_OF_PLANTS', 'Number of different plants')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets take a look at BMI. Interestingly, both the severely underweight and severely obese groups show a reduced number of shared OTUs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BMI_ranges = [(0, 18),\n",
      "              (18, 25),\n",
      "              (25, 30),\n",
      "              (30, 35),\n",
      "              (35, 40)]\n",
      "helper_multiple('BMI', 'BMI', discretize=BMI_ranges)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
# <nbformat>3.0</nbformat>

# <codecell>

import os
from IPython.core.display import clear_output
import sys
from time import sleep

def submit_qsub(cmd, job_name='ipy_ftw', queue=None, extra_args=''):
    """Submit a job and return the full job name"""
    job_data = {'workdir':working_dir, 
                'cmd':cmd, 
                'queue':'-q %s' % queue if queue is not None else '', 
                'extra_args':extra_args, 
                'jobname':job_name}
    job_template = 'echo "cd %(workdir)s; %(cmd)s" | qsub -k oe -N %(jobname)s %(queue)s %(extra_args)s'
    job = job_template % job_data
    job_id = !$job
    return (job_name, job_id[0].split('.')[0])

def parse_qstat():
    user = os.environ['USER']
    lines = !qstat -u $user
    
    jobs = {}
    for id_, name, state in lines.grep(user).fields(0,3,9).fields():
        job_id = id_.split('.')[0]
        jobs[job_id] = {}
        jobs[job_id]['name'] = name
        jobs[job_id]['state'] = state
        
    return jobs

def still_running(monitoring, running_jobs, additional_prefix=None):
    """Check if our jobs to be monitored are running

    additional_prefix can be specified to track derivative worker processes
    (e.g., from a parallel QIIME workflow)
    """        
    new_monitoring = set([])
    for name, id_ in monitoring:
        # stop monitoring if not present anymore
        if id_ not in running_jobs:
            continue
            
        # stop monitoring if complete
        if running_jobs[id_]['state'] == 'C':
            continue
            
        new_monitoring.add((name, id_))
    
    if additional_prefix is not None:
        # see if we have any new jobs with a prefix we're interested in
        for id_,md in running_jobs.items():
            if md['name'].startswith(additional_prefix):
                if md['state'] in ['R','Q']: # running or queued
                    new_monitoring.add((md['name'], id_))
                    
    return new_monitoring

def job_run_details(name, id_):
    """Run tracejob and parse out the useful bits"""
    # go back 2 days. This may need to be smarter.
    job_details = !tracejob -a -m -l -f job -n 2 $id_
    if not job_details.grep('dest='):
        raise ValueError("Cannot find job %s!" % id_)
    
    dest = job_details.grep('dest=').fields(-2)[0].strip('(),').split('=')[1]
    tmp = job_details.grep('Exit_status=').fields(3,4,5,6).fields()[0]
    exit_status, walltime, mem, vmem = map(lambda x: x.split('=')[-1], tmp)

    stderr_file = os.path.expandvars("$HOME/%s.e%s" % (name, id_))
    if not os.path.exists(stderr_file):
        raise ValueError, "Could not find expected standard error output: %s" % stderr_file
    
    stdout_file = os.path.expandvars("$HOME/%s.o%s" % (name, id_))
    if not os.path.exists(stdout_file):
        raise ValueError, "Could not find expected standard output: %s" % stdout_file

    return {'exit_status':exit_status,
            'walltime':walltime,
            'mem':mem, 
            'vmem':vmem,
            'stderr_file':stderr_file,
            'stdout_file':stdout_file}
 
def wait_on(jobs_to_monitor, additional_prefix=None):
    """Block while jobs to monitor are running, and provide a status update"""
    POLL_INTERVAL = 5
    elapsed = 0
    
    # fragile.
    if isinstance(jobs_to_monitor, tuple) and len(jobs_to_monitor) == 2:
        jobs_to_monitor = [jobs_to_monitor]
    
    all_jobs = set(jobs_to_monitor)
    n_jobs = len(jobs_to_monitor)
    
    print "monitoring %d jobs..." % n_jobs
    sys.stdout.flush()

    running_jobs = parse_qstat()
    while jobs_to_monitor:
        sleep(POLL_INTERVAL)
        elapsed += POLL_INTERVAL
        
        running_jobs = parse_qstat()
        jobs_to_monitor = still_running(jobs_to_monitor, running_jobs, additional_prefix)

        all_jobs.update(set(jobs_to_monitor))
        n_running = len(jobs_to_monitor)
        n_total = len(all_jobs)

        clear_output()
        print "%d / %d jobs still running, approximately  %d seconds elapsed" % (n_running, n_total, elapsed)
        sys.stdout.flush()

    n_running = len(jobs_to_monitor)
    n_total = len(all_jobs)

    clear_output()
    print "%d / %d jobs still running, approximately  %d seconds elapsed" % (n_running, n_total, elapsed)
    print "All jobs completed!"
    sys.stdout.flush()

    # check if any jobs errored out
    for name, id_ in all_jobs:
        deets = job_run_details(name, id_)
        if deets['exit_status'] != '0':
            stderr_fp = deets['stderr_file']
            last_stderr_line = !tail -n 1 $stderr_fp
            print "ERROR! Job %s did not exit cleanly." % id_
            print "Here is the last line of standard error (%s)" % stderr_fp
            print last_stderr_line[0]
            print 
    return all_jobs
